{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bachngo/anaconda3/envs/gemm_assistant/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Define the paper search endpoint URL\n",
    "url = 'https://api.semanticscholar.org/graph/v1/paper/search'\n",
    "\n",
    "# Define the required query parameter and its value (in this case, the keyword we want to search for)\n",
    "query_params = {\n",
    "    'query': 'semantic scholar platform',\n",
    "    'limit': 1\n",
    "}\n",
    "\n",
    "# Make the GET request with the URL and query parameters\n",
    "search_response = requests.get(url, params=query_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a separate function to make a request to the paper details endpoint using a paper_id. This function will be used later on (after we call the paper search endpoint).\n",
    "def get_paper_data(paper_ids):\n",
    "  url = 'https://api.semanticscholar.org/graph/v1/paper/batch'\n",
    "\n",
    "  # Define which details about the paper you would like to receive in the response\n",
    "  paper_data_query_params = {'fields': 'references'}\n",
    "\n",
    "  # Send the API request and store the response in a variable\n",
    "  response = requests.post(url, params=paper_data_query_params, json={\"ids\": paper_ids})\n",
    "  if response.status_code == 200:\n",
    "    return response.json()\n",
    "  else:\n",
    "    return None\n",
    "\n",
    "# Retrieve the paper id corresponding to the 1st result in the list\n",
    "paper_id = [\"arxiv:2204.03458\", \"arxiv:2405.04133\"]\n",
    "\n",
    "# Retrieve the paper details corresponding to this paper id using the function we defined earlier.\n",
    "paper_details = get_paper_data(paper_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'paperId': '4247f45a5730e3bda5836e2bc7941e30f5b91cb7', 'title': 'Board'},\n",
       " {'paperId': 'af9f365ed86614c800f082bd8eb14be76072ad16',\n",
       "  'title': 'Classifier-Free Diffusion Guidance'},\n",
       " {'paperId': '4e65a4f248344c0a17e20d712d38d91085f608a2',\n",
       "  'title': 'Transframer: Arbitrary Frame Prediction with Generative Models'},\n",
       " {'paperId': '1641774b55a471a23eb31b722ee05c2e032fec7a',\n",
       "  'title': 'Diffusion Probabilistic Modeling for Video Generation'},\n",
       " {'paperId': '7002ae048e4b8c9133a55428441e8066070995cb',\n",
       "  'title': 'GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models'},\n",
       " {'paperId': '9eeb5f7c36654dff1dc93adb7150e7bab52cd3e2',\n",
       "  'title': 'Deblurring via Stochastic Refinement'},\n",
       " {'paperId': '97bee918b08c244eb2e54d41e8ea6da00a3e5dbf',\n",
       "  'title': 'NÃœWA: Visual Synthesis Pre-training for Neural visUal World creAtion'},\n",
       " {'paperId': '37c9c4e7648f639c0b36f150fc6c6c90b3682f4a',\n",
       "  'title': 'Palette: Image-to-Image Diffusion Models'},\n",
       " {'paperId': '5e51ebd5507cf6810b3b5833885115ca91bca356',\n",
       "  'title': 'CCVS: Context-aware Controllable Video Synthesis'},\n",
       " {'paperId': '94bcd712aed610b8eaeccc57136d65ec988356f2',\n",
       "  'title': 'Variational Diffusion Models'},\n",
       " {'paperId': 'aa5e7fffefb7f53fb6f7e937b5976584d001c906',\n",
       "  'title': 'FitVid: Overfitting in Pixel-Level Video Prediction'},\n",
       " {'paperId': '0f183bcfe65781c06b1a48a6f56e0f3c63e8e4a4',\n",
       "  'title': 'Cascaded Diffusion Models for High Fidelity Image Generation'},\n",
       " {'paperId': '64ea8f180d0682e6c18d1eb688afdb2027c02794',\n",
       "  'title': 'Diffusion Models Beat GANs on Image Synthesis'},\n",
       " {'paperId': '2d9ae4c167510ed78803735fc57ea67c3cc55a35',\n",
       "  'title': 'VideoGPT: Video Generation using VQ-VAE and Transformers'},\n",
       " {'paperId': '8a1ea7b6e7e834d146ad782be5d63f57f806a9cc',\n",
       "  'title': 'Image Super-Resolution via Iterative Refinement'},\n",
       " {'paperId': 'b6382a7351c0c595f91472ac71d3b2d87b3c4844',\n",
       "  'title': 'ViViT: A Video Vision Transformer'},\n",
       " {'paperId': 'e8b540f92ba947d6814e5abe3afda4da4d9d78f3',\n",
       "  'title': 'Predicting Video with VQVAE'},\n",
       " {'paperId': 'de18baa4964804cf471d85a5a090498242d2e79f',\n",
       "  'title': 'Improved Denoising Diffusion Probabilistic Models'},\n",
       " {'paperId': 'fa08b41ccdfc5d8771adfbc34c176fa237d4646c',\n",
       "  'title': 'Is Space-Time Attention All You Need for Video Understanding?'},\n",
       " {'paperId': '633e2fbfc0b21e959a244100937c5853afca4853',\n",
       "  'title': 'Score-Based Generative Modeling through Stochastic Differential Equations'},\n",
       " {'paperId': 'ac4e977aba1731ce1bd5ddeb9ea466fa86c10a5f',\n",
       "  'title': 'Latent Neural Differential Equations for Video Generation'},\n",
       " {'paperId': '79e1854115a29fd7bac54d553ce50404969b6275',\n",
       "  'title': 'Image Representations Learned With Unsupervised Pre-Training Contain Human-like Biases'},\n",
       " {'paperId': '014576b866078524286802b1d0e18628520aa886',\n",
       "  'title': 'Denoising Diffusion Implicit Models'},\n",
       " {'paperId': '34bf13e58c7226d615afead0c0f679432502940e',\n",
       "  'title': 'DiffWave: A Versatile Diffusion Model for Audio Synthesis'},\n",
       " {'paperId': '685af6d2bcdff7170574643b2c5ab4fbcc36f597',\n",
       "  'title': 'WaveGrad: Estimating Gradients for Waveform Generation'},\n",
       " {'paperId': '579ad7dc135cb6957707581b0dfbdd5f412ff848',\n",
       "  'title': 'Solving Linear Inverse Problems Using the Prior Implicit in a Denoiser'},\n",
       " {'paperId': '5c126ae3421f05768d8edd97ecd44b1364e2c99a',\n",
       "  'title': 'Denoising Diffusion Probabilistic Models'},\n",
       " {'paperId': '7891b414b353a88b96a4a0e14ccfbea8b16c8d39',\n",
       "  'title': 'Transformation-based Adversarial Video Prediction on Large-Scale Data'},\n",
       " {'paperId': 'b5b01749791a5cc753b5f260963c5f79e1d32609',\n",
       "  'title': 'Lower Dimensional Kernels for Video Discriminators'},\n",
       " {'paperId': 'aa6bb4f4ad12ca1ff14faff0c2147de3be059e79',\n",
       "  'title': 'Markov Decision Process for Video Generation'},\n",
       " {'paperId': '12c37cb419121cdb43f2c6620303932f43e2e1b7',\n",
       "  'title': 'Adversarial Video Generation on Complex Datasets'},\n",
       " {'paperId': '366244acdd930e488ae224ab6e2a92dc24aa7e06',\n",
       "  'title': 'Axial Attention in Multidimensional Transformers'},\n",
       " {'paperId': '965359b3008ab50dd04e171551220ec0e7f83aba',\n",
       "  'title': 'Generative Modeling by Estimating Gradients of the Data Distribution'},\n",
       " {'paperId': 'e763fdc9ae56826ff799163ea035b29bffd8ea6f',\n",
       "  'title': 'Scaling Autoregressive Video Models'},\n",
       " {'paperId': 'c73211167d621446593f0859f12b6f0679f06b22',\n",
       "  'title': 'Neural Stochastic Differential Equations: Deep Latent Gaussian Models in the Diffusion Limit'},\n",
       " {'paperId': 'a78bc54068384d9ea6f07db89a1ce902ef56c2c6',\n",
       "  'title': 'VideoFlow: A Flow-Based Generative Model for Video'},\n",
       " {'paperId': 'b59233aab8364186603967bc12d88af48cc0992d',\n",
       "  'title': 'Towards Accurate Generative Models of Video: A New Metric & Challenges'},\n",
       " {'paperId': '0c5f6d07b2a355312ba50132bab30832d1a4d883',\n",
       "  'title': 'Train Sparsely, Generate Densely: Memory-Efficient Unsupervised Training of High-Resolution Temporal GAN'},\n",
       " {'paperId': '36d3a18a1519e27f7c9c8479b19fc00d4d805a00',\n",
       "  'title': 'Generating High Fidelity Images with Subscale Pixel Networks and Multidimensional Upscaling'},\n",
       " {'paperId': '62dccab9ab715f33761a5315746ed02e48eed2a0',\n",
       "  'title': 'A Short Note about Kinetics-600'},\n",
       " {'paperId': '9f67271a1edea3bf80c64c2d54e2a0a57612a567',\n",
       "  'title': 'Stochastic Adversarial Video Prediction'},\n",
       " {'paperId': 'b0c5dc3fa19a2bc97606ccb6f55226b913984395',\n",
       "  'title': 'Women also Snowboard: Overcoming Bias in Captioning Models'},\n",
       " {'paperId': 'c8efcc854d97dfc2a42b83316a2109f9d166e43f',\n",
       "  'title': 'Self-Attention with Relative Position Representations'},\n",
       " {'paperId': '18858cc936947fc96b5c06bbe3c6c2faa5614540',\n",
       "  'title': 'Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification'},\n",
       " {'paperId': 'd1c424c261c577958917055f72fb9e2ad0348865',\n",
       "  'title': 'PixelSNAIL: An Improved Autoregressive Generative Model'},\n",
       " {'paperId': '8899094797e82c5c185a0893896320ef77f60e64',\n",
       "  'title': 'Non-local Neural Networks'},\n",
       " {'paperId': '59d86da5c5936e7a236678bf5eaaa7753c226fb1',\n",
       "  'title': 'Stochastic Variational Video Prediction'},\n",
       " {'paperId': 'cf18287e79b1fd73cd333fc914bb24c00a537f4c',\n",
       "  'title': 'Self-Supervised Visual Planning with Temporal Skip Connections'},\n",
       " {'paperId': 'e76edb86f270c3a77ed9f5a1e1b305461f36f96f',\n",
       "  'title': 'MoCoGAN: Decomposing Motion and Content for Video Generation'},\n",
       " {'paperId': '231af7dc01a166cac3b5b01ca05778238f796e41',\n",
       "  'title': 'GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium'},\n",
       " {'paperId': '204e3073870fae3d05bcbc2f6a8e263d9b72e776',\n",
       "  'title': 'Attention is All you Need'},\n",
       " {'paperId': 'b61a3f8b80bbd44f24544dc915f52fd30bbdf485',\n",
       "  'title': 'Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset'},\n",
       " {'paperId': '86e1bdbfd13b9ed137e4c4b8b459a3980eb257f6',\n",
       "  'title': 'The Kinetics Human Action Video Dataset'},\n",
       " {'paperId': '2e77b99e8bd10b9e4551a780c0bde9dd10fdbe9b',\n",
       "  'title': 'PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications'},\n",
       " {'paperId': '7fc464470b441c691d10e7331b14a525bc79b8bb',\n",
       "  'title': '3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation'},\n",
       " {'paperId': '571b0750085ae3d939525e62af510ee2cee9d5ea',\n",
       "  'title': 'Improved Techniques for Training GANs'},\n",
       " {'paperId': '1c4e9156ca07705531e45960b7a919dc473abb51',\n",
       "  'title': 'Wide Residual Networks'},\n",
       " {'paperId': '6364fdaa0a0eccd823a779fcdd489173f938e91a',\n",
       "  'title': 'U-Net: Convolutional Networks for Biomedical Image Segmentation'},\n",
       " {'paperId': '2dcef55a07f8607a819c21fe84131ea269cc2e3c',\n",
       "  'title': 'Deep Unsupervised Learning using Nonequilibrium Thermodynamics'},\n",
       " {'paperId': 'd25c65d261ea0e6a458be4c50c40ffe5bc508f77',\n",
       "  'title': 'Learning Spatiotemporal Features with 3D Convolutional Networks'},\n",
       " {'paperId': 'da9e411fcf740569b6b356f330a1d0fc077c8d7c',\n",
       "  'title': 'UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild'},\n",
       " {'paperId': '872bae24c109f7c30e052ac218b17a8b028d08a0',\n",
       "  'title': 'A Connection Between Score Matching and Denoising Autoencoders'},\n",
       " {'paperId': '804b27dc02becf7bbbd89ba949e1e07e8677c459',\n",
       "  'title': 'DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generative Transformers'},\n",
       " {'paperId': None,\n",
       "  'title': 'resizes the input data to 112x112 resolution, so perceptual scores are approximately comparable even when the data is sampled at a different resolution originally'},\n",
       " {'paperId': 'c434a618949c83c6b6423e655e0c4cbdd49481a8',\n",
       "  'title': 'Stochastic Solutions for Linear Inverse Problems using the Prior Implicit in a Denoiser'},\n",
       " {'paperId': 'df2b0e26d0599ce3e70df8a9da02e51594e0e992',\n",
       "  'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'},\n",
       " {'paperId': None,\n",
       "  'title': 'Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?'},\n",
       " {'paperId': None, 'title': 'Attention head dimension'},\n",
       " {'paperId': None,\n",
       "  'title': 'Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation'},\n",
       " {'paperId': None,\n",
       "  'title': 'Did you report error bars (e.g., with respect to the random seed after running experiments multiple times'},\n",
       " {'paperId': None,\n",
       "  'title': 'Did you include any new assets either in the supplemental material or as a URL?'},\n",
       " {'paperId': None,\n",
       "  'title': 'code, data, models) or curating/releasing new assets... (a) If your work uses existing assets'},\n",
       " {'paperId': None,\n",
       "  'title': 'If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots'},\n",
       " {'paperId': None,\n",
       "  'title': 'Training steps: 700000 Diffusion noise schedule: cosine Joint training independent images per video: 8 Noise schedule log SNR range'},\n",
       " {'paperId': None,\n",
       "  'title': 'Here, we list the hyperparameters, training details, and compute resources used for each model'},\n",
       " {'paperId': None,\n",
       "  'title': 'A Details and hyperparameters Figure 5: More samples accompanying Fig'},\n",
       " {'paperId': None,\n",
       "  'title': 'Blocks per resolution: 3 Batch size: 128 Attention resolutions: 8'},\n",
       " {'paperId': None,\n",
       "  'title': 'b) Did you describe any potential participant risks, with links to Institutional Review'},\n",
       " {'paperId': None,\n",
       "  'title': 'Conditioning embedding dimension: 1024 Training hardware: 64 TPU-v4 chips Conditioning embedding MLP layers: 4 Training steps'},\n",
       " {'paperId': None,\n",
       "  'title': 'Kinetics Base channels: 256 Optimizer: Adam (Î²1 = 0.9, Î²2 = 0.99) Channel multipliers: 1, 2, 4, 8 Learning'},\n",
       " {'paperId': None, 'title': 'Datasets, a collection of ready-to-use datasets'}]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "class PaperNode:\n",
    "    title: str\n",
    "    arxiv_id: str\n",
    "    \n",
    "    def __init__(self, title, arxiv_id):\n",
    "        self.title = title\n",
    "        self.arxiv_id = arxiv_id\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"Title: {self.title},\\n Arxiv ID: {self.arxiv_id}\"\n",
    "\n",
    "class PaperEdge:\n",
    "    category: str\n",
    "    explanation: str\n",
    "    verbose = True\n",
    "\n",
    "    def __init__(self, category, explanation):\n",
    "        self.category = category\n",
    "        self.explanation = explanation\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        if self.verbose:\n",
    "            return f\"Category: {self.category},\\n Explanation: {self.explanation}\"\n",
    "        else:\n",
    "            return f\"Category: {self.category}\"\n",
    "\n",
    "def create_ego_graph(retriever_response, service: str = \"ss\", graph: nx.DiGraph = None) -> nx.DiGraph:\n",
    "\n",
    "    if service == \"ss\":\n",
    "        paper_ids = [\"arxiv:\"+ n.metadata[\"paper_id\"] for n in retriever_response]\n",
    "        url = 'https://api.semanticscholar.org/graph/v1/paper/batch'\n",
    "\n",
    "        # Define which details about the paper you would like to receive in the response\n",
    "        paper_data_query_params = {'fields': 'references'}\n",
    "        G = nx.DiGraph()\n",
    "\n",
    "        # Send the API request and store the response in a variable\n",
    "        response = requests.post(url, params=paper_data_query_params, json={\"ids\": paper_ids})\n",
    "        source_nodes = []\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            for idx, item in enumerate(data):\n",
    "                source_node = PaperNode(\n",
    "                    title=retriever_response[idx].metadata[\"title\"],\n",
    "                    arxiv_id=retriever_response[idx].metadata[\"paper_id\"]\n",
    "                )\n",
    "                source_nodes.append(source_node)\n",
    "                for reference in item[\"references\"]:\n",
    "                    target_node = PaperNode(\n",
    "                        title=reference[\"title\"],\n",
    "                        arxiv_id=reference[\"paperId\"]\n",
    "                    )\n",
    "                    \n",
    "                    G.add_edge(source_node.title, target_node.title)\n",
    "            \n",
    "            nodes_to_remove = [node for node in G if G.degree(node) < 2]\n",
    "\n",
    "            # Remove the nodes from the ego graph\n",
    "            G.remove_nodes_from(nodes_to_remove)\n",
    "            # Assign colors: highlighted nodes in red, others in blue\n",
    "            highlight_color = \"orange\"\n",
    "            for node in G.nodes():\n",
    "                if node in source_nodes:\n",
    "                    G.nodes[node]['color'] = highlight_color\n",
    "                    \n",
    "            return G\n",
    "        else:\n",
    "            raise ValueError(f\"Request failed with status code {response.status_code}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat:cs.AI+OR+cat:cs.CV+OR+cat:cs.IR+OR+cat:cs.LG+OR+cat:cs.CL\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import feedparser\n",
    "\n",
    "def get_latest_arxiv_papers(categories=['cs.AI', 'cs.CV', 'cs.IR', 'cs.LG', 'cs.CL'], max_results=50):\n",
    "    base_url = 'http://export.arxiv.org/api/query?'\n",
    "    all_categories=[f'cat:{category}' for category in categories]\n",
    "    search_query = '+OR+'.join(all_categories)\n",
    "    query = f'search_query={search_query}&start=0&max_results={max_results}&sortBy=submittedDate&sortOrder=descending'\n",
    "    response = requests.get(base_url + query)\n",
    "    feed = feedparser.parse(response.content)\n",
    "    return feed.entries\n",
    "\n",
    "response = get_latest_arxiv_papers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "I am interested in LLM, Generative AI and trendy stuffs in AI, picks out the most notable papers published today:\n",
    "\n",
    "==============\n",
    "{paper_list}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "paper_list = []\n",
    "\n",
    "for r in response:\n",
    "    paper_list.append(f\"\"\"\n",
    "Title: {r['title']}\n",
    "Link: {r['link']}\n",
    "Summary: {r['summary']}\n",
    "                  \n",
    "    \"\"\")\n",
    "\n",
    "prompt = prompt = template.format(paper_list=\"\\n==============\\n\".join(paper_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv(override=True)\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4o\", api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "res = llm.complete(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are some of the most notable papers published today in the fields of Large Language Models (LLMs), Generative AI, and other trendy AI topics:\n",
      "\n",
      "1. **MambaOut: Do We Really Need Mamba for Vision?**\n",
      "   - **Link:** [arxiv.org/abs/2405.07992v1](http://arxiv.org/abs/2405.07992v1)\n",
      "   - **Summary:** This paper explores the necessity of the Mamba architecture for vision tasks, concluding that Mamba is not essential for image classification but shows potential for long-sequence visual tasks like detection and segmentation.\n",
      "\n",
      "2. **SPIN: Simultaneous Perception, Interaction and Navigation**\n",
      "   - **Link:** [arxiv.org/abs/2405.07991v1](http://arxiv.org/abs/2405.07991v1)\n",
      "   - **Summary:** Introduces a reactive mobile manipulation framework that integrates perception, interaction, and navigation, allowing a mobile manipulator to navigate and interact with its environment using an active visual system.\n",
      "\n",
      "3. **Plot2Code: A Comprehensive Benchmark for Evaluating Multi-modal Large Language Models in Code Generation from Scientific Plots**\n",
      "   - **Link:** [arxiv.org/abs/2405.07990v1](http://arxiv.org/abs/2405.07990v1)\n",
      "   - **Summary:** Proposes a benchmark for evaluating the ability of Multi-modal Large Language Models (MLLMs) to generate code from scientific plots, highlighting the challenges and performance of various MLLMs.\n",
      "\n",
      "4. **A Generalist Learner for Multifaceted Medical Image Interpretation**\n",
      "   - **Link:** [arxiv.org/abs/2405.07988v1](http://arxiv.org/abs/2405.07988v1)\n",
      "   - **Summary:** Introduces MedVersa, a generalist learner for medical image interpretation that leverages a large language model to support multimodal inputs and perform various medical image analysis tasks.\n",
      "\n",
      "5. **The Platonic Representation Hypothesis**\n",
      "   - **Link:** [arxiv.org/abs/2405.07987v1](http://arxiv.org/abs/2405.07987v1)\n",
      "   - **Summary:** Discusses the convergence of representations in AI models towards a shared statistical model of reality, akin to Plato's concept of an ideal reality, and explores the implications of this trend.\n",
      "\n",
      "6. **AgentClinic: a multimodal agent benchmark to evaluate AI in simulated clinical environments**\n",
      "   - **Link:** [arxiv.org/abs/2405.07960v1](http://arxiv.org/abs/2405.07960v1)\n",
      "   - **Summary:** Presents AgentClinic, a benchmark for evaluating AI agents in simulated clinical environments, focusing on the ability to diagnose and manage patients through dialogue and active data collection.\n",
      "\n",
      "7. **Hierarchical Decision Mamba**\n",
      "   - **Link:** [arxiv.org/abs/2405.07943v1](http://arxiv.org/abs/2405.07943v1)\n",
      "   - **Summary:** Introduces Decision Mamba and Hierarchical Decision Mamba, which enhance the performance of Transformer models in imitation learning tasks by leveraging the Mamba architecture.\n",
      "\n",
      "8. **RAID: A Shared Benchmark for Robust Evaluation of Machine-Generated Text Detectors**\n",
      "   - **Link:** [arxiv.org/abs/2405.07940v1](http://arxiv.org/abs/2405.07940v1)\n",
      "   - **Summary:** Proposes RAID, a benchmark dataset for evaluating the robustness of machine-generated text detectors, including variations in sampling strategy, adversarial attacks, and generative models.\n",
      "\n",
      "9. **CTRLorALTer: Conditional LoRAdapter for Efficient 0-Shot Control & Altering of T2I Models**\n",
      "   - **Link:** [arxiv.org/abs/2405.07913v1](http://arxiv.org/abs/2405.07913v1)\n",
      "   - **Summary:** Presents LoRAdapter, a method for zero-shot control of text-to-image diffusion models, enabling fine-grained control conditioning during generation.\n",
      "\n",
      "10. **PLUTO: Pathology-Universal Transformer**\n",
      "    - **Link:** [arxiv.org/abs/2405.07905v1](http://arxiv.org/abs/2405.07905v1)\n",
      "    - **Summary:** Introduces PLUTO, a light-weight pathology foundation model pre-trained on a diverse dataset, designed to extract meaningful representations across multiple WSI scales for various pathology tasks.\n",
      "\n",
      "These papers cover a range of innovative approaches and benchmarks in AI, from improving model architectures and benchmarks to exploring new applications in medical imaging and autonomous systems.\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemm_assistant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
