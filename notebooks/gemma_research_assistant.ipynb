{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Building Gemma Research Assistant**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../figures/GemmaAIO-main-image.webp\" alt=\"main-image\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey everyone, welcome to this notebook where we're diving into something super cool: building an all-in-one research chatbot using the power of the Gemma Large Language Model! ðŸš€\n",
    "\n",
    "Here's the game plan:\n",
    "\n",
    "- First up, Section 1: We're kicking things off with a research paper query engine. Imagine being able to find any research paper with just a simple chat. Sounds handy, right?\n",
    "\n",
    "- Moving on to Section 2: We'll spice things up with a graph paper relationship engine. This is all about connecting the dots between different papers and seeing the bigger picture.\n",
    "\n",
    "- Section 3: We'll add a basic data science assistant to our toolkit. This chatbot will help with all those tricky data questions, from stats to machine learning.\n",
    "\n",
    "- Section 4 is for the coders: We're building an AI code assistant that's going to be like your coding sidekick, helping you solve problems and understand complex codes.\n",
    "\n",
    "- And for the grand finale, Section 5: We're bringing it all together with a combination module. This is where we make sure everything works in harmony, giving you a powerhouse tool for any research or coding project.\n",
    "\n",
    "So, let's roll up our sleeves and jump into this exciting project. An overview of this project is below: ðŸŒŸ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../figures/RAG%20-%20Scientific%20Assistant%20-%20Frame%201.jpg\" alt=\"pipeline\" width=800/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Scientific Research Assistant**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we're focusing on creating the first part of our chatbot: a tool that can search through a huge number of research papers on arXiv. The key to this tool is using embeddings, taken from paper abstracts. Think of these as unique IDs that sum up what each paper is about.\n",
    "\n",
    "When you ask the chatbot something, it uses these embeddings to look through the abstracts and find papers that really match what you're looking for, not just by keywords, but by the actual ideas and concepts you're interested in. This is more about understanding the meaning of your question and finding papers that really match.\n",
    "\n",
    "We'll go through everything: picking the right papers from arXiv, getting the abstracts ready, and choosing a way to turn these abstracts into embeddings. Then, we'll set up a smart search that can quickly find the best matches when you ask a question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../figures/Science-Paper-Search.jpg\" alt=\"science paper search\" width=800/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.1 Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2455227, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0704.0001</td>\n",
       "      <td>Calculation of prompt diphoton production cros...</td>\n",
       "      <td>A fully differential calculation in perturba...</td>\n",
       "      <td>hep-ph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0704.0002</td>\n",
       "      <td>Sparsity-certifying Graph Decompositions</td>\n",
       "      <td>We describe a new algorithm, the $(k,\\ell)$-...</td>\n",
       "      <td>math.CO cs.CG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0704.0003</td>\n",
       "      <td>The evolution of the Earth-Moon system based o...</td>\n",
       "      <td>The evolution of Earth-Moon system is descri...</td>\n",
       "      <td>physics.gen-ph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0704.0004</td>\n",
       "      <td>A determinant of Stirling cycle numbers counts...</td>\n",
       "      <td>We show that a determinant of Stirling cycle...</td>\n",
       "      <td>math.CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0704.0005</td>\n",
       "      <td>From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...</td>\n",
       "      <td>In this paper we show how to compute the $\\L...</td>\n",
       "      <td>math.CA math.FA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              title  \\\n",
       "0  0704.0001  Calculation of prompt diphoton production cros...   \n",
       "1  0704.0002           Sparsity-certifying Graph Decompositions   \n",
       "2  0704.0003  The evolution of the Earth-Moon system based o...   \n",
       "3  0704.0004  A determinant of Stirling cycle numbers counts...   \n",
       "4  0704.0005  From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...   \n",
       "\n",
       "                                            abstract       categories  \n",
       "0    A fully differential calculation in perturba...           hep-ph  \n",
       "1    We describe a new algorithm, the $(k,\\ell)$-...    math.CO cs.CG  \n",
       "2    The evolution of Earth-Moon system is descri...   physics.gen-ph  \n",
       "3    We show that a determinant of Stirling cycle...          math.CO  \n",
       "4    In this paper we show how to compute the $\\L...  math.CA math.FA  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.kaggle.com/code/matthewmaddock/nlp-arxiv-dataset-transformers-and-umap\n",
    "\n",
    "# This takes about 1 minute.\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "cols = ['id', 'title', 'abstract', 'categories']\n",
    "data = []\n",
    "file_name = '../data/arxiv-metadata-oai-snapshot.json'\n",
    "\n",
    "\n",
    "with open(file_name, encoding='latin-1') as f:\n",
    "    for line in f:\n",
    "        doc = json.loads(line)\n",
    "        lst = [doc['id'], doc['title'], doc['abstract'], doc['categories']]\n",
    "        data.append(lst)\n",
    "\n",
    "df_data = pd.DataFrame(data=data, columns=cols)\n",
    "\n",
    "print(df_data.shape)\n",
    "\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a total of almost 2,5M papers on arxiv, that's too much! However, not all of them are about AI, so let's narrow down to the topics we're interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0704.0047</td>\n",
       "      <td>Intelligent location of simultaneously active ...</td>\n",
       "      <td>The intelligent acoustic emission locator is...</td>\n",
       "      <td>cs.NE cs.AI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0704.0050</td>\n",
       "      <td>Intelligent location of simultaneously active ...</td>\n",
       "      <td>Part I describes an intelligent acoustic emi...</td>\n",
       "      <td>cs.NE cs.AI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>0704.0304</td>\n",
       "      <td>The World as Evolving Information</td>\n",
       "      <td>This paper discusses the benefits of describ...</td>\n",
       "      <td>cs.IT cs.AI math.IT q-bio.PE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>0704.0671</td>\n",
       "      <td>Learning from compressed observations</td>\n",
       "      <td>The problem of statistical learning is to co...</td>\n",
       "      <td>cs.IT cs.LG math.IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>0704.0954</td>\n",
       "      <td>Sensor Networks with Random Links: Topology De...</td>\n",
       "      <td>In a sensor network, in practice, the commun...</td>\n",
       "      <td>cs.IT cs.LG math.IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2443613</th>\n",
       "      <td>quant-ph/0411140</td>\n",
       "      <td>Improved Bounds on Quantum Learning Algorithms</td>\n",
       "      <td>In this article we give several new results ...</td>\n",
       "      <td>quant-ph cs.LG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2445483</th>\n",
       "      <td>quant-ph/0507231</td>\n",
       "      <td>Algebras of Measurements: the logical structur...</td>\n",
       "      <td>In Quantum Physics, a measurement is represe...</td>\n",
       "      <td>quant-ph cs.AI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2448330</th>\n",
       "      <td>quant-ph/0607111</td>\n",
       "      <td>`Plausibilities of plausibilities': an approac...</td>\n",
       "      <td>Probability-like parameters appearing in som...</td>\n",
       "      <td>quant-ph cs.AI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2450042</th>\n",
       "      <td>quant-ph/0702072</td>\n",
       "      <td>Markovian Entanglement Networks</td>\n",
       "      <td>Graphical models of probabilistic dependenci...</td>\n",
       "      <td>quant-ph cs.AI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2452122</th>\n",
       "      <td>quant-ph/9802028</td>\n",
       "      <td>Analogue Quantum Computers for Data Analysis</td>\n",
       "      <td>Analogue computers use continuous properties...</td>\n",
       "      <td>quant-ph cs.CV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>336892 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id                                              title  \\\n",
       "46              0704.0047  Intelligent location of simultaneously active ...   \n",
       "49              0704.0050  Intelligent location of simultaneously active ...   \n",
       "303             0704.0304                  The World as Evolving Information   \n",
       "670             0704.0671              Learning from compressed observations   \n",
       "953             0704.0954  Sensor Networks with Random Links: Topology De...   \n",
       "...                   ...                                                ...   \n",
       "2443613  quant-ph/0411140     Improved Bounds on Quantum Learning Algorithms   \n",
       "2445483  quant-ph/0507231  Algebras of Measurements: the logical structur...   \n",
       "2448330  quant-ph/0607111  `Plausibilities of plausibilities': an approac...   \n",
       "2450042  quant-ph/0702072                    Markovian Entanglement Networks   \n",
       "2452122  quant-ph/9802028       Analogue Quantum Computers for Data Analysis   \n",
       "\n",
       "                                                  abstract  \\\n",
       "46         The intelligent acoustic emission locator is...   \n",
       "49         Part I describes an intelligent acoustic emi...   \n",
       "303        This paper discusses the benefits of describ...   \n",
       "670        The problem of statistical learning is to co...   \n",
       "953        In a sensor network, in practice, the commun...   \n",
       "...                                                    ...   \n",
       "2443613    In this article we give several new results ...   \n",
       "2445483    In Quantum Physics, a measurement is represe...   \n",
       "2448330    Probability-like parameters appearing in som...   \n",
       "2450042    Graphical models of probabilistic dependenci...   \n",
       "2452122    Analogue computers use continuous properties...   \n",
       "\n",
       "                           categories  \n",
       "46                        cs.NE cs.AI  \n",
       "49                        cs.NE cs.AI  \n",
       "303      cs.IT cs.AI math.IT q-bio.PE  \n",
       "670               cs.IT cs.LG math.IT  \n",
       "953               cs.IT cs.LG math.IT  \n",
       "...                               ...  \n",
       "2443613                quant-ph cs.LG  \n",
       "2445483                quant-ph cs.AI  \n",
       "2448330                quant-ph cs.AI  \n",
       "2450042                quant-ph cs.AI  \n",
       "2452122                quant-ph cs.CV  \n",
       "\n",
       "[336892 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "topics = ['cs.AI', 'cs.CV', 'cs.IR', 'cs.LG', 'cs.CL']\n",
    "\n",
    "# Create a regular expression pattern that matches any of the topics\n",
    "# The pattern will look like 'cs.AI|cs.CV|cs.IR|cs.LG|cs.CL'\n",
    "pattern = '|'.join(topics)\n",
    "\n",
    "# Filter the DataFrame to include rows where the 'categories' column contains any of the topics\n",
    "# The na=False parameter makes sure that NaN values are treated as False\n",
    "df_filtered = df_data[df_data['categories'].str.contains(pattern, na=False)]\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we down to about 330K papers. Now, let's clean the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                    0707.0705\n",
       "title         Optimal Solutions for Sparse Principal Compone...\n",
       "abstract        Given a sample covariance matrix, we examine...\n",
       "categories                                          cs.AI cs.LG\n",
       "Name: 13875, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.iloc[110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>categories</th>\n",
       "      <th>prepared_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0704.0047</td>\n",
       "      <td>Intelligent location of simultaneously active ...</td>\n",
       "      <td>The intelligent acoustic emission locator is d...</td>\n",
       "      <td>cs.NE cs.AI</td>\n",
       "      <td>Intelligent location of simultaneously active ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0704.0050</td>\n",
       "      <td>Intelligent location of simultaneously active ...</td>\n",
       "      <td>Part I describes an intelligent acoustic emiss...</td>\n",
       "      <td>cs.NE cs.AI</td>\n",
       "      <td>Intelligent location of simultaneously active ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>0704.0304</td>\n",
       "      <td>The World as Evolving Information</td>\n",
       "      <td>This paper discusses the benefits of describin...</td>\n",
       "      <td>cs.IT cs.AI math.IT q-bio.PE</td>\n",
       "      <td>The World as Evolving Information\\n This paper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>0704.0671</td>\n",
       "      <td>Learning from compressed observations</td>\n",
       "      <td>The problem of statistical learning is to cons...</td>\n",
       "      <td>cs.IT cs.LG math.IT</td>\n",
       "      <td>Learning from compressed observations\\n The pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>0704.0954</td>\n",
       "      <td>Sensor Networks with Random Links: Topology De...</td>\n",
       "      <td>In a sensor network, in practice, the communic...</td>\n",
       "      <td>cs.IT cs.LG math.IT</td>\n",
       "      <td>Sensor Networks with Random Links: Topology De...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                              title  \\\n",
       "46   0704.0047  Intelligent location of simultaneously active ...   \n",
       "49   0704.0050  Intelligent location of simultaneously active ...   \n",
       "303  0704.0304                  The World as Evolving Information   \n",
       "670  0704.0671              Learning from compressed observations   \n",
       "953  0704.0954  Sensor Networks with Random Links: Topology De...   \n",
       "\n",
       "                                              abstract  \\\n",
       "46   The intelligent acoustic emission locator is d...   \n",
       "49   Part I describes an intelligent acoustic emiss...   \n",
       "303  This paper discusses the benefits of describin...   \n",
       "670  The problem of statistical learning is to cons...   \n",
       "953  In a sensor network, in practice, the communic...   \n",
       "\n",
       "                       categories  \\\n",
       "46                    cs.NE cs.AI   \n",
       "49                    cs.NE cs.AI   \n",
       "303  cs.IT cs.AI math.IT q-bio.PE   \n",
       "670           cs.IT cs.LG math.IT   \n",
       "953           cs.IT cs.LG math.IT   \n",
       "\n",
       "                                         prepared_text  \n",
       "46   Intelligent location of simultaneously active ...  \n",
       "49   Intelligent location of simultaneously active ...  \n",
       "303  The World as Evolving Information\\n This paper...  \n",
       "670  Learning from compressed observations\\n The pr...  \n",
       "953  Sensor Networks with Random Links: Topology De...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(x):\n",
    "    \n",
    "    # Replace newline characters with a space\n",
    "    new_text = \" \".join([c.strip() for c in x.replace(\"\\n\", \"\").split()])\n",
    "    # Remove leading and trailing spaces\n",
    "    new_text = new_text.strip()\n",
    "    \n",
    "    return new_text\n",
    "\n",
    "df_data['title'] = df_data['title'].apply(clean_text)\n",
    "df_data['abstract'] = df_data['abstract'].apply(clean_text)\n",
    "\n",
    "df_data['prepared_text'] = df_data['title'] + '\\n ' + df_data['abstract']\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gb2t/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "arxiv_documents = [Document(text=prepared_text, doc_id=id) for prepared_text,id in list(zip(df_data['prepared_text'], df_data['id']))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.2 Creating Index**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `VectorStoreIndex` is by far the most frequently used type of Index in llamaindex. This class takes your Documents and splits them up into Nodes. Then, it creates `vector_embeddings` of the text of every node. But what is `vector_embedding`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector embeddings are like turning the essence of your words into a mathematical sketch. Imagine every idea or concept in your text getting its unique numerical fingerprint. This is handy because even if two snippets of text use different words, if they're sharing the same idea, their numerical sketchesâ€”or embeddingsâ€”will be close neighbors in the numerical space. This magic is done using tools known as embedding models.\n",
    "\n",
    "Choosing the right embedding model is crucial. It's like picking the right artist to paint your portrait; you want the one who captures you best. A great place to start is the MTEB leaderboard, where the crÃ¨me de la crÃ¨me of embedding models are ranked. As we have quite a large dataset, the model size matters, we don't want to wait all day for the model to extract all the vector embeddings. When I last checked, the `BAAI/bge-small-en-v1.5` model was leading the pack, especially considering its size. It could be a solid choice if you're diving into the world of text embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "import chromadb\n",
    "import torch\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "Settings.llm = None\n",
    "# Create embed model\n",
    "device_type = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\", cache_folder=\"../models\", device=device_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we have to find somewhere to store all of the embeddings extracted by the model, and that's why we need a `vector store`. There are many to choose from, in this tutorial, I will choose the `chroma` vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.PersistentClient(path=\"../DB/arxiv\")\n",
    "chroma_collection = chroma_client.get_or_create_collection(\"gemma_assistant_arxiv_papers\")\n",
    "\n",
    "\n",
    "# Create vector store\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part takes quite a lot of time! So I precomputed the embedding and store them into chroma db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = VectorStoreIndex.from_documents(\n",
    "#     arxiv_documents, storage_context=storage_context, embed_model=embed_model, show_progress=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.3 Loading from arxiv vector store**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "import torch\n",
    "\n",
    "\n",
    "Settings.llm = None # Set this to none to make the index only do retrieval\n",
    "device_type = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\", cache_folder=\"../models\", device=device_type) # must be the same as the previous stage\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=\"../DB/arxiv\")\n",
    "chroma_collection = chroma_client.get_or_create_collection(\"gemma_assistant_arxiv_papers\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "# load the vectorstore\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_vector_store(vector_store, storage_context=storage_context, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_query_engine = index.as_query_engine(\n",
    "    similarity_top_k=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "deep image synthesis from intuitive user input: a review and perspectives\n",
      " In many applications of computer graphics, art and design, it is desirablefor a user to provide intuitive non-image input, such as text, sketch, stroke,graph or layout, and have a computer system automatically generatephoto-realistic images that adhere to the input content. While classic worksthat allow such automatic image content generation have followed a framework ofimage retrieval and composition, recent advances in deep generative models suchas generative adversarial networks (GANs), variational autoencoders (VAEs), andflow-based methods have enabled more powerful and versatile image generationtasks. This paper reviews recent works for image synthesis given intuitive userinput, covering advances in input versatility, image generation methodology,benchmark datasets, and evaluation metrics. This motivates new perspectives oninput representation and interactivity, cross pollination between major imagegeneration paradigms, and evaluation and comparison of generation methods.\n",
      "\n",
      "paint it black: generating paintings from text descriptions\n",
      " Two distinct tasks - generating photorealistic pictures from given textprompts and transferring the style of a painting to a real image to make itappear as though it were done by an artist, have been addressed many times, andseveral approaches have been proposed to accomplish them. However, theintersection of these two, i.e., generating paintings from a given caption, isa relatively unexplored area with little data available. In this paper, we haveexplored two distinct strategies and have integrated them together. Firststrategy is to generate photorealistic images and then apply style transfer andthe second strategy is to train an image generation model on real images withcaptions and then fine-tune it on captioned paintings later. These two modelsare evaluated using different metrics as well as a user study is conducted toget human feedback on the produced results.\n",
      "\n",
      "semantic draw engineering for text-to-image creation\n",
      " Text-to-image generation is conducted through Generative Adversarial Networks(GANs) or transformer models. However, the current challenge lies in accuratelygenerating images based on textual descriptions, especially in scenarios wherethe content and theme of the target image are ambiguous. In this paper, wepropose a method that utilizes artificial intelligence models for thematiccreativity, followed by a classification modeling of the actual paintingprocess. The method involves converting all visual elements into quantifiabledata structures before creating images. We evaluate the effectiveness of thisapproach in terms of semantic accuracy, image reproducibility, andcomputational efficiency, in comparison with existing image generationalgorithms.\n",
      "\n",
      "systematic analysis of image generation using gans\n",
      " Generative Adversarial Networks have been crucial in the developments made inunsupervised learning in recent times. Exemplars of image synthesis from textor other images, these networks have shown remarkable improvements overconventional methods in terms of performance. Trained on the adversarialtraining philosophy, these networks aim to estimate the potential distributionfrom the real data and then use this as input to generate the synthetic data.Based on this fundamental principle, several frameworks can be generated thatare paragon implementations in several real-life applications such as artsynthesis, generation of high resolution outputs and synthesis of images fromhuman drawn sketches, to name a few. While theoretically GANs present betterresults and prove to be an improvement over conventional methods in manyfactors, the implementation of these frameworks for dedicated applicationsremains a challenge. This study explores and presents a taxonomy of theseframeworks and their use in various image to image synthesis and text to imagesynthesis applications. The basic GANs, as well as a variety of different nicheframeworks, are critically analyzed. The advantages of GANs for imagegeneration over conventional methods as well their disadvantages amongst otherframeworks are presented. The future applications of GANs in industries such ashealthcare, art and entertainment are also discussed.\n",
      "\n",
      "text-guided image-and-shape editing and generation: a short survey\n",
      " Image and shape editing are ubiquitous among digital artworks. Graphicsalgorithms facilitate artists and designers to achieve desired editing intentswithout going through manually tedious retouching. In the recent advance ofmachine learning, artists' editing intents can even be driven by text, using avariety of well-trained neural networks. They have seen to be receiving anextensive success on such as generating photorealistic images, artworks andhuman poses, stylizing meshes from text, or auto-completion given image andshape priors. In this short survey, we provide an overview over 50 papers onstate-of-the-art (text-guided) image-and-shape generation techniques. We startwith an overview on recent editing algorithms in the introduction. Then, weprovide a comprehensive review on text-guided editing techniques for 2D and 3Dindependently, where each of its sub-section begins with a brief backgroundintroduction. We also contextualize editing algorithms under recent implicitneural representations. Finally, we conclude the survey with the discussionover existing methods and potential research ideas.\n",
      "\n",
      "text-to-image cross-modal generation: a systematic review\n",
      " We review research on generating visual data from text from the angle of\"cross-modal generation.\" This point of view allows us to draw parallelsbetween various methods geared towards working on input text and producingvisual output, without limiting the analysis to narrow sub-areas. It alsoresults in the identification of common templates in the field, which are thencompared and contrasted both within pools of similar methods and across linesof research. We provide a breakdown of text-to-image generation into variousflavors of image-from-text methods, video-from-text methods, image editing,self-supervised and graph-based approaches. In this discussion, we focus onresearch papers published at 8 leading machine learning conferences in theyears 2016-2022, also incorporating a number of relevant papers not matchingthe outlined search criteria. The conducted review suggests a significantincrease in the number of papers published in the area and highlights researchgaps and potential lines of investigation. To our knowledge, this is the firstreview to systematically look at text-to-image generation from the perspectiveof \"cross-modal generation.\"\n",
      "\n",
      "a taxonomy of prompt modifiers for text-to-image generation\n",
      " Text-to-image generation has seen an explosion of interest since 2021. Today,beautiful and intriguing digital images and artworks can be synthesized fromtextual inputs (\"prompts\") with deep generative models. Online communitiesaround text-to-image generation and AI generated art have quickly emerged. Thispaper identifies six types of prompt modifiers used by practitioners in theonline community based on a 3-month ethnographic study. The novel taxonomy ofprompt modifiers provides researchers a conceptual starting point forinvestigating the practice of text-to-image generation, but may also helppractitioners of AI generated art improve their images. We further outline howprompt modifiers are applied in the practice of \"prompt engineering.\" Wediscuss research opportunities of this novel creative practice in the field ofHuman-Computer Interaction (HCI). The paper concludes with a discussion ofbroader implications of prompt engineering from the perspective of Human-AIInteraction (HAI) in future applications beyond the use case of text-to-imagegeneration and AI generated art.\n",
      "\n",
      "figgen: text to scientific figure generation\n",
      " The generative modeling landscape has experienced tremendous growth in recentyears, particularly in generating natural images and art. Recent techniqueshave shown impressive potential in creating complex visual compositions whiledelivering impressive realism and quality. However, state-of-the-art methodshave been focusing on the narrow domain of natural images, while otherdistributions remain unexplored. In this paper, we introduce the problem oftext-to-figure generation, that is creating scientific figures of papers fromtext descriptions. We present FigGen, a diffusion-based approach fortext-to-figure as well as the main challenges of the proposed task. Code andmodels are available at https://github.com/joanrod/figure-diffusion\n",
      "\n",
      "a novel sampling scheme for text- and image-conditional image synthesis in quantized latent spaces\n",
      " Recent advancements in the domain of text-to-image synthesis have culminatedin a multitude of enhancements pertaining to quality, fidelity, and diversity.Contemporary techniques enable the generation of highly intricate visuals whichrapidly approach near-photorealistic quality. Nevertheless, as progress isachieved, the complexity of these methodologies increases, consequentlyintensifying the comprehension barrier between individuals within the field andthose external to it. In an endeavor to mitigate this disparity, we propose a streamlined approachfor text-to-image generation, which encompasses both the training paradigm andthe sampling process. Despite its remarkable simplicity, our method yieldsaesthetically pleasing images with few sampling iterations, allows forintriguing ways for conditioning the model, and imparts advantages absent instate-of-the-art techniques. To demonstrate the efficacy of this approach inachieving outcomes comparable to existing works, we have trained a one-billionparameter text-conditional model, which we refer to as \"Paella\". In theinterest of fostering future exploration in this field, we have made our sourcecode and models publicly accessible for the research community.\n",
      "\n",
      "text-to-image diffusion models in generative ai: a survey\n",
      " This survey reviews text-to-image diffusion models in the context thatdiffusion models have emerged to be popular for a wide range of generativetasks. As a self-contained work, this survey starts with a brief introductionof how a basic diffusion model works for image synthesis, followed by howcondition or guidance improves learning. Based on that, we present a review ofstate-of-the-art methods on text-conditioned image synthesis, i.e.,text-to-image. We further summarize applications beyond text-to-imagegeneration: text-guided creative generation and text-guided image editing.Beyond the progress made so far, we discuss existing challenges and promisingfuture directions.\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: What are some papers about image generation?\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "print(paper_query_engine.query(\"What are some papers about image generation?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Graph-based paper relationship search**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we dive into constructing a knowledge graph about the relationships of papers. This graph could be used for interactive visualization, searching relationships between papers (e.g. How is paper A related to paper B), or search for a specific relationship in a paper (e.g. What are works that paper A based on?). The steps of constructing this knowledge graph are:\n",
    "\n",
    "\n",
    "- Step 1: arXiv Data Extraction: The process starts with academic papers from the arXiv database, which undergo OCR (Optical Character Recognition) and PDF parsing, which organizes the content into structured data such as the title, abstract, sections, and references of the papers. \n",
    "\n",
    "- Step 2: Text Splitter: The text in each section is then processed by a Text Splitter, which split the paper section into smaller chunks, which could be easier for LLMs to process.  \n",
    "\n",
    "- Step 3 GPT-3.5 Processing: Gemma couldn't generate the knowledge graph out-of-the-box. So we need knowledge distillation from a bigger model, which I choose GPT-3.5. The structured data is passed to GPT-3.5 to extract citation relationships, such as \"Data Source\", \"Extension\", or \"Theoretical Foundation\", etc. Each relationship is paired with a dense explanation. I extracted a total of ~300 papers, which cost around 4$.\n",
    "\n",
    "- Step 4 Training Gemma - 7B: The distilled knowledge data are then used to train Gemma-7b. Then I use this model to generate citation relationships for as many papers as I can. In total, I extracted 7k papers, with around 150K triplets! Crazy!!\n",
    "\n",
    "- Step 5 Graph Store: Finally, a Graph Store is created containing 7K papers and 586K triplets. This could then be used for searching relationships or visualization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../figures/Graph-Paper-Search.jpg\" alt=\"graph-search\" width=1200/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.1 Download pre-extracted citation data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "parsed_article = load_dataset(\"BachNgoH/ParsedArxivPapers\")['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_article = parsed_article.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "for article in parsed_article:\n",
    "    if article['citation_data'] != None:\n",
    "        article['citation_data'] = json.loads(article['citation_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Category': 'Supporting Evidence',\n",
       "  'Citation': '(Tatman, 2017)',\n",
       "  'Explanation': 'The cited work by Tatman (2017) provides evidence of the potential impact of demographic biases in NLP models, highlighting the importance of addressing fairness in the field.'},\n",
       " {'Category': 'Supporting Evidence',\n",
       "  'Citation': '(Perez, 2019)',\n",
       "  'Explanation': 'The cited work by Perez (2019) emphasizes the need for accurate identification of speakers and their needs in NLP models, further supporting the importance of fairness in the field.'},\n",
       " {'Category': 'Supporting Evidence',\n",
       "  'Citation': '(Agarwal et al., 2019)',\n",
       "  'Explanation': 'The cited work by Agarwal et al. (2019) highlights the issue of hurtful stereotypes in NLP models, emphasizing the need for fairness in the field to address this problem.'},\n",
       " {'Category': 'Supporting Evidence',\n",
       "  'Citation': '(Nozza et al., 2022)',\n",
       "  'Explanation': 'The cited work by Nozza et al. (2022) provides further evidence of the need for fairness in NLP models, specifically in the context of the propagation of hurtful stereotypes.'},\n",
       " {'Category': 'Supporting Evidence',\n",
       "  'Citation': '(Carlini et al., 2019)',\n",
       "  'Explanation': 'The cited work by Carlini et al. (2019) highlights the potential harm of data leaks in NLP models, emphasizing the need for privacy in the field to protect individuals from sensitive personal data disclosure.'},\n",
       " {'Category': 'Extension or Continuation',\n",
       "  'Citation': '(Farrand et al., 2020)',\n",
       "  'Explanation': 'The cited work by Farrand et al. (2020) extends the research on the trade-off between privacy and fairness in NLP classifiers, providing new insights and perspectives on the topic.'},\n",
       " {'Category': 'Extension or Continuation',\n",
       "  'Citation': '(Hansen et al., 2022)',\n",
       "  'Explanation': 'The cited work by Hansen et al. (2022) builds upon the research on the trade-off between privacy and fairness in NLP classifiers, further exploring the relationship between the two dimensions.'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': '(Bagdasaryan et al., 2019)',\n",
       "  'Explanation': 'The cited work by Bagdasaryan et al. (2019) serves as a data source for the research on the trade-off between privacy and fairness in NLP classifiers, providing a foundational dataset for the study.'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': '(Cummings et al., 2019)',\n",
       "  'Explanation': 'The cited work by Cummings et al. (2019) serves as a data source for the research on the trade-off between privacy and fairness in NLP classifiers, providing a foundational dataset for the study.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Caliskan et al., 2017)',\n",
       "  'Explanation': 'The Word Embedding Association Test (WEAT) is a method for detecting biases in word embeddings that the citing paper adopts in their research to measure the association between target word sets and attribute sets in vector space.'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': '(May et al., 2019)',\n",
       "  'Explanation': 'The extension of the WEAT to sentence-level representations by May et al. (2019) is a data source that the citing paper utilizes in their research to measure the association between target word sets and attribute sets in vector space at a sentence level.'},\n",
       " {'Category': 'Extension or Continuation',\n",
       "  'Citation': '(Bartl et al., 2020)',\n",
       "  'Explanation': 'The Bias Evaluation Corpus with Professions (BEC-Pro) is an extension of the work by Bartl et al. (2020) that the citing paper builds upon to determine gender bias with regard to different professions and other characteristics in their research.'},\n",
       " {'Category': 'Extension or Continuation',\n",
       "  'Citation': '(Webster et al., 2020)',\n",
       "  'Explanation': 'The Discovery of Correlations (DisCo) dataset is another extension of the work by Webster et al. (2020) that the citing paper further builds upon to determine gender bias with regard to different professions and other characteristics in their research.'},\n",
       " {'Category': 'Extension or Continuation',\n",
       "  'Citation': '(Zhao et al., 2018)',\n",
       "  'Explanation': 'The Wino-Bias benchmark introduced by Zhao et al. (2018) is an extension of the Winograd Challenge (Levesque et al., 2012) that the citing paper further builds upon to follow a certain scheme in their research to measure gender bias with regard to person, pronouns and occupations.'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': '(Nadeem et al., 2020)',\n",
       "  'Explanation': 'StereoSet is a dataset used in the cited work to measure the level of stereotypical associations in terms of gender, occupation, race, and religion in a model.'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': '(De-Arteaga et al., 2019)',\n",
       "  'Explanation': 'Bias-in-Bios is a dataset used in the cited work to assess the ability of a model to read biographies and identify professions without making gender-based assumptions.'},\n",
       " {'Category': 'Extension or Continuation',\n",
       "  'Citation': '(Webster et al., 2020)',\n",
       "  'Explanation': 'The cited work by Webster et al. proposed a debiasing technique using dropout regularization, which the citing paper extends by exploring the use of dropout as a method for reducing gender correlations in models.'},\n",
       " {'Category': 'Extension or Continuation',\n",
       "  'Citation': '(Zhao et al., 2018)',\n",
       "  'Explanation': 'The cited work by Zhao et al. introduced the Counterfactual Data Augmentation (CDA) approach for rebalancing datasets, which the citing paper extends by discussing its use in bias mitigation methods.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(2020)',\n",
       "  'Explanation': 'The cited work proposed a method to mitigate biases in word embeddings, which the citing paper adopts in their research to address the issue of bias in word embeddings.'},\n",
       " {'Category': 'Supporting Evidence',\n",
       "  'Citation': 'INLP',\n",
       "  'Explanation': 'The cited work of INLP provides a method to remove linear dependencies between word embeddings and protected attributes, which the citing paper uses to support their research on mitigating biases in word embeddings.'},\n",
       " {'Category': 'Extension or Continuation',\n",
       "  'Citation': 'Schick et al., 2021',\n",
       "  'Explanation': \"The cited work of Self-Debias poses a post-hoc text generation debiasing technique that the citing paper extends by using the method to change the model's output distribution in their research on debiasing text generation.\"},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Abadi et al., 2016)',\n",
       "  'Explanation': 'The cited work introduces DP-SGD, which the citing paper adopts to implement DP in the training of language models.'},\n",
       " {'Category': 'Extension or Continuation',\n",
       "  'Citation': '(Yu et al., 2021b)',\n",
       "  'Explanation': 'The cited work addresses the computational and memory overhead of DP by introducing RGP, which the citing paper further extends to create a low-dimensional projection of the gradient and implement privacy through clipping and noise addition.'},\n",
       " {'Category': 'Extension or Continuation',\n",
       "  'Citation': '(Shi et al., 2021)',\n",
       "  'Explanation': 'The cited work highlights the importance of understanding the trade-off between privacy and utility in language models and introduces the approach of S-DP to allow for different privacy levels in different attributes of the data.'},\n",
       " {'Category': 'Supporting Evidence',\n",
       "  'Citation': '(Shokri et al., 2017)',\n",
       "  'Explanation': 'The cited work by Shokri et al. provides a classification of membership inference attacks based on the level of access the attacker has to the deep learning algorithm, which the citing paper uses to structure its discussion on the topic.'},\n",
       " {'Category': 'Supporting Evidence',\n",
       "  'Citation': '(Nasr et al., 2019)',\n",
       "  'Explanation': 'The work by Nasr et al. defines the concept of privacy-sensitive leakage in models and quantifies the information an adversary can learn from a model, which the citing paper uses to discuss the risk of information leakage in deep learning models.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(2022a)',\n",
       "  'Explanation': 'The cited work by Murakonda et al. (2021), Ye et al. (2021), and Carlini et al. (2022) provides a methodological basis for the whitebox setting in the approach of reference-based likelihood ratio attacks used in the citing paper.'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': '(Bartl et al., 2020)',\n",
       "  'Explanation': 'The cited work provides the BEC-Pro dataset, which the citing paper uses to evaluate gender bias in language models.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(May et al., 2019)',\n",
       "  'Explanation': 'The cited work introduces the SEAT method, which the citing paper adopts to evaluate the gender bias in language models by measuring the differential association between target word sets and attribute sets.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(2020)',\n",
       "  'Explanation': 'The cited work by Nadeem et al. introduces the Context Association Test (CAT) as a method for measuring language modeling ability and stereotypical bias in language models. The citing paper adopts this method to evaluate the performance of language models in terms of language modeling ability and bias.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Mireshghallah et al., 2022a,b)',\n",
       "  'Explanation': 'The cited works provide the methodology for performing reference-based likelihood ratio attacks, which the citing paper adopts in their research to examine the leakage in their models.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Carlini et al., 2022)',\n",
       "  'Explanation': 'The cited work by Carlini et al. provides a method for using a hypothesis test to guess whether a data point was used to train a target model, which the citing paper uses in their research to perform a whitebox attack on a trained model.'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': '(Mireshghallah et al., 2022b)',\n",
       "  'Explanation': 'The cited work by Mireshghallah et al. provides a pre-trained GPT-2 model that the citing paper uses as a reference model in their attack on a fine-tuned model.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Mireshghallah et al., 2022b)',\n",
       "  'Explanation': 'The cited work by Mireshghallah et al. provides a method for computing the threshold t used in the attack, which the citing paper adopts in their own research to evaluate the leakage of models trained with CDA.'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': '(Wang et al., 2018)',\n",
       "  'Explanation': 'The cited work introduces the General Language Understanding Evaluation (GLUE) benchmark, which the citing paper uses as a downstream task for evaluating the language modeling capabilities of a model.'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': '(Zhu et al., 2015)',\n",
       "  'Explanation': 'The cited work is the source of the BookCorpus dataset used in the fine-tuning process in the citing paper.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Lauscher et al., 2021)',\n",
       "  'Explanation': 'The cited work provides the approach of uniformly subsampling the BookCorpus dataset, which the citing paper adopts in creating the training dataset for fine-tuning.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Radford et al., 2019)',\n",
       "  'Explanation': 'The cited work by Radford et al. serves as the basis for the training of the GPT-2 model used in the citing paper. The authors refer to the work to establish the model architecture and training process.'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': 'huggingface2',\n",
       "  'Explanation': 'The cited work of huggingface provides the pre-trained GPT-2 model used in the study conducted in the citing paper. The model is utilized as a baseline for comparison and analysis of the debiasing and DP methods.'},\n",
       " {'Category': 'Extension or Continuation',\n",
       "  'Citation': 'Figure 2',\n",
       "  'Explanation': 'The cited figure in the text is a reference to the different objectives used in the training of the GPT-2 model. The citing paper extends the research by exploring the use of these objectives in the training process.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Hu et al., 2021)',\n",
       "  'Explanation': 'The cited work by Hu et al. provides the method of LoRA (Low-Resource Adversarial training) for reducing the number of trainable parameters in GPT-2, which the citing paper adopts in their experiments to reduce the GPU memory requirements.'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': '(Webster et al., 2020)',\n",
       "  'Explanation': 'The cited work by Webster et al. introduces the use of Dropout as a bias mitigation method in the original dataset, which the citing paper uses in their experiments to increase dropout regularization and mitigate bias in the data.'},\n",
       " {'Category': 'Extension or Continuation',\n",
       "  'Citation': '(Meade et al., 2021)',\n",
       "  'Explanation': 'The cited work by Meade et al. discusses the use of two-sided counterfactual data augmentation in the training process, which the citing paper extends by applying the same method in their experiments to further mitigate bias in the data.'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': '(Yousefpour et al., 2021)',\n",
       "  'Explanation': 'The cited work provides the open-source PyTorch library Opacus that the citing paper uses for implementing DP in their research.'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': '(Wutschitz et al., 2022)',\n",
       "  'Explanation': 'The cited work provides the dp-transformers repository that the citing paper utilizes in their research for training with privacy as an objective.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Nadeem et al., 2020)',\n",
       "  'Explanation': 'The cited work by Nadeem et al. provides a method for training language models with debiasing and privacy objectives, which the citing paper adopts in their research.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Warstadt et al., 2019)',\n",
       "  'Explanation': 'The cited work by Warstadt et al. (2019) is used as a data source for the CoLA (acceptability task) in the citing paper, providing a benchmark for evaluating the model performance in the study conducted.'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': '(Socher et al., 2013)',\n",
       "  'Explanation': 'The cited work by Socher et al. (2013) is used as a data source for the STS-B (sentence similarity task) in the citing paper, providing a benchmark for evaluating the model performance in the study conducted.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Bagdasaryan et al., 2019)',\n",
       "  'Explanation': 'The cited work by Bagdasaryan et al. (2019) provides a framework for understanding the effects of DP on classification tasks, which the citing paper builds upon in their research on the effects of DP on language modeling.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Bagdasaryan et al., 2019)',\n",
       "  'Explanation': 'The cited work by Bagdasaryan et al. provides a method for assessing the effect of data poisoning in the context of self-supervised language modeling, which the citing paper builds upon in their own research.'},\n",
       " {'Category': 'Supporting Evidence',\n",
       "  'Citation': '(Farrand et al., 2020)',\n",
       "  'Explanation': 'The cited work by Farrand et al. provides evidence of a negative trade-off between differential privacy and fairness in NLP classifiers, which the citing paper uses to support the claim that minorities are classified worse with lower accuracy.'},\n",
       " {'Category': 'Supporting Evidence',\n",
       "  'Citation': '(Hansen et al., 2022)',\n",
       "  'Explanation': 'The cited work by Hansen et al. further substantiates the claim of a negative trade-off between differential privacy and fairness in NLP classifiers, providing additional evidence to the citing paper.'},\n",
       " {'Category': 'Supporting Evidence',\n",
       "  'Citation': '(Bagdasaryan et al., 2019)',\n",
       "  'Explanation': 'The cited work by Bagdasaryan et al. contributes to the discussion of a negative trade-off between differential privacy and fairness in NLP classifiers, providing a foundational understanding for the citing paper.'},\n",
       " {'Category': 'Supporting Evidence',\n",
       "  'Citation': '(Cummings et al., 2019)',\n",
       "  'Explanation': 'The cited work by Cummings et al. further supports the claim of a negative trade-off between differential privacy and fairness in NLP classifiers, providing additional evidence to the citing paper.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Yu et al., 2021a;2021b)',\n",
       "  'Explanation': 'The cited works by Yu et al. provide a detailed discussion on the memory requirements for training with DP, which the citing paper adopts in its own research to limit the training batch size and increase the gradient accumulation steps.'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': '(Yousefpour et al., 2021)',\n",
       "  'Explanation': 'The cited work by Yousefpour et al. (2021) is acknowledged for providing the computational resources and the Opacus framework that the citing paper utilized in their research.'},\n",
       " {'Category': 'Extension or Continuation',\n",
       "  'Citation': '(Kurita et al., 2019;May et al., 2019;Meade et al., 2021)',\n",
       "  'Explanation': 'The cited works by Kurita et al. (2019), May et al. (2019), and Meade et al. (2021) are mentioned in the context of the findings regarding the reliability of SEAT, indicating a continuation of the research on the subject.'},\n",
       " {'Category': 'Supporting Evidence',\n",
       "  'Citation': '(Dwork et al., 2006b,a)',\n",
       "  'Explanation': 'The cited work by Dwork et al. provides the foundational concept of Differential Privacy (DP), which the citing paper uses in their experiments to report a quantifiable guarantee of disclosure risk.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Abadi et al., 2016, Song et al., 2013, Bassily et al., 2014)',\n",
       "  'Explanation': 'The cited works provide the methodological basis for implementing Differentially Private Stochastic Gradient Descent (DP-SGD) in the citing paper to ensure data privacy during training.'},\n",
       " {'Category': 'Extension or Continuation',\n",
       "  'Citation': '(Senge et al., 2022;Igamberdiev and Habernal, 2022;Yin and Habernal, 2022)',\n",
       "  'Explanation': 'The cited works extend the discussion of DP-SGD in NLP tasks by providing a more in-depth overview of the method and its application in the field.'},\n",
       " {'Category': 'Extension or Continuation',\n",
       "  'Citation': '(Habernal, 2021(Habernal, , 2022;;Igamberdiev et al., 2022)',\n",
       "  'Explanation': 'The cited works build upon the general discussion of DP in NLP by providing specific details and examples of its use in the field.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Zhao et al., 2018)',\n",
       "  'Explanation': 'The cited work introduces the concept of rebalancing a dataset by exchanging bias attribute words in an automated process, which the citing paper adopts in their research to address the issue of bias in the training dataset.'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': '(Lauscher et al., 2021)',\n",
       "  'Explanation': 'The cited work provides a set of word pairs between the dominant and minorized groups, which the citing paper uses to identify and replace words in the training dataset to rebalance the data and address bias.'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': '(Lauscher et al. 2021)',\n",
       "  'Explanation': 'The cited work provides a dataset of name pairs from US Social Security Name Statistics that the citing paper uses to augment the texts for training the CDA and CDA+DP models.'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': '(Zhao et al. 2018)',\n",
       "  'Explanation': 'The cited work provides a dataset of general noun pairs that the citing paper utilizes in its research on gender-based language.'},\n",
       " {'Category': 'Supporting Evidence',\n",
       "  'Citation': '(Zhao et al. 2018)',\n",
       "  'Explanation': 'The Extra Word List provided by Zhao et al. (2018) serves as a foundational source of information for the study conducted in the citing paper, contributing to the development of a more comprehensive understanding of gender-based language.'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': '(Wang et al., 2018)',\n",
       "  'Explanation': 'The cited work is the source of the General Language Understanding Evaluation (GLUE) benchmark, which the citing paper uses to evaluate the performance of language models in downstream tasks.'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': '(Warstadt et al., 2019)',\n",
       "  'Explanation': 'The cited work provides the Corpus of Linguistic Acceptability (CoLA) dataset, which the citing paper uses in its research on grammaticality and language acceptability.'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': '(Socher et al., 2013)',\n",
       "  'Explanation': 'The cited work provides the Stanford Sentiment Treebank (STS-B) dataset, which the citing paper uses in its research on sentiment analysis in movie reviews.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Dolan and Brockett, 2005)',\n",
       "  'Explanation': 'The cited work provides the Microsoft Research Paraphrase Corpus (MRPC) dataset, which the citing paper uses to evaluate the performance of the model in the task of predicting the meaning of one text in relation to another.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Haim et al., 2006)',\n",
       "  'Explanation': 'The cited work provides the RTE2 dataset, which the citing paper uses to evaluate the model in the task of recognizing textual entailment (RTE).'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Giampiccolo et al., 2007)',\n",
       "  'Explanation': 'The cited work provides the RTE3 dataset, which the citing paper uses to evaluate the model in the task of recognizing textual entailment (RTE).'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Bentivogli et al., 2009)',\n",
       "  'Explanation': 'The cited work provides the RTE5 dataset, which the citing paper uses to evaluate the model in the task of recognizing textual entailment (RTE).'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Cer et al., 2017)',\n",
       "  'Explanation': 'The cited work provides the Semantic Textual Similarity Benchmark (STS-B) dataset, which the citing paper uses to evaluate the model in the task of predicting the meaning of one text in relation to another.'},\n",
       " {'Category': 'Supporting Evidence',\n",
       "  'Citation': '(Wang et al., 2018)',\n",
       "  'Explanation': 'The cited work introduces the task of determining whether a sentence with a substituted pronoun is entailed by the original sentence, which serves as a basis for the research in the citing paper on evaluating the performance of language models in this task.'}]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_article[104]['citation_data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the number of annotated papers for now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_article = [x for x in parsed_article if x['citation_data'] is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotated Papers:  7243\n"
     ]
    }
   ],
   "source": [
    "print(\"Annotated Papers: \", len(annotated_article))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2 Parsing generated data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From my observation, there are 2 main citation styles in AI papers, Author-year style and Numeric style:\n",
    "\n",
    "Example of Author-year style:\n",
    "- (Bassignana and Plank, 2022a) \n",
    "- (Liu et al., 2021)\n",
    "- (KÃ¶ksal and Ã–zgÃ¼r, 2020)\n",
    "\n",
    "Example of Numeric style:\n",
    "- [1], [2], [3]\n",
    "- [2, 56, 67]\n",
    "- [7 - 9]\n",
    "\n",
    "Therefore, we need different strategy to handle each style of citation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2.1 Handle Author-Year citation style**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling this citation style can be quite frustrating. Initially, we must separate combined citations like (Liu et al., 2021; Littell et al., 201) into individual entries. Then, we need to identify the first author and publication year. Subsequently, we have to locate the corresponding reference within our reference list based on the author's name and publication year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(Cohn et al., 1996)',\n",
       " '(Settles, 2009)',\n",
       " '(Dasgupta, 2011)',\n",
       " '(Gururangan et al., 2020)',\n",
       " '(Houlsby et al., 2019)',\n",
       " '(Pfeiffer et al., 2023)',\n",
       " '(He et al., 2021;Li and Liang, 2021;Karimi Mahabadi et al., 2021)',\n",
       " '(Toneva et al., 2019)',\n",
       " '(Ein-Dor et al., 2020)',\n",
       " '(Margatina et al., 2021)',\n",
       " '(Shelmanov et al., 2021)',\n",
       " '(Karamcheti et al., 2021)',\n",
       " '(SchrÃ¶der et al., 2022)',\n",
       " '(Mosbach et al., 2021)',\n",
       " '(Zhang et al., 2021)',\n",
       " '(Dodge et al., 2020)',\n",
       " '(GrieÃŸhaber et al., 2020)',\n",
       " '(Yuan et al., 2020)',\n",
       " '(Yu et al., 2022)',\n",
       " '(Margatina et al., 2022)',\n",
       " '(JukiÄ‡ and Å najder, 2023)',\n",
       " '(Ansell et al., 2021)',\n",
       " '(Lee et al., 2022)',\n",
       " '(ParoviÄ‡ et al., 2022)',\n",
       " '(Li and Liang, 2021)',\n",
       " '(Mao et al., 2022)',\n",
       " '(He et al., 2021)',\n",
       " '(Kim et al., 2021)',\n",
       " '(Pang and Lee, 2004)',\n",
       " '(Li and Roth, 2002)',\n",
       " '(Socher et al., 2013)',\n",
       " '(Zhang et al., 2015)',\n",
       " '(Houlsby et al., 2019)',\n",
       " '(Li and Liang, 2021)',\n",
       " '(Hu et al., 2022)',\n",
       " '(Mao et al., 2022)',\n",
       " '(Devlin et al., 2019)',\n",
       " '(Lewis and Gale, 1994)',\n",
       " '(Gal and Ghahramani, 2016)',\n",
       " '(Srivastava et al., 2014)',\n",
       " '(Sener and Savarese, 2018)',\n",
       " '(SchrÃ¶der et al., 2022)',\n",
       " '(JukiÄ‡ and Å najder, 2023)',\n",
       " '(Li and Liang, 2021)',\n",
       " '(Mao et al., 2022)',\n",
       " '(He et al., 2021)',\n",
       " '(Li and Liang, 2021)',\n",
       " '(Mao et al., 2022)',\n",
       " '(He et al., 2021)',\n",
       " '(Toneva et al., 2019)',\n",
       " '(He et al., 2021)',\n",
       " '(Li and Liang, 2021)',\n",
       " '(Mao et al., 2022)',\n",
       " '(Stephenson et al., 2021)',\n",
       " '(Baldock et al., 2021)',\n",
       " '(Pfeiffer et al., 2020)']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse annotated articles\n",
    "import re\n",
    "\n",
    "# Function to normalize author names for comparison\n",
    "def normalize_author_name(name):\n",
    "    # Convert to lowercase and remove middle initials\n",
    "    name = name.lower()\n",
    "    name = re.sub(r\"\\s+[a-z]\\.\", \"\", name)  # Remove middle initials\n",
    "    return name\n",
    "\n",
    "\n",
    "citation_names = [c['Citation'] for c in annotated_article[0]['citation_data']]\n",
    "citation_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refined function to identify and normalize the first author from a citation\n",
    "def identify_and_normalize_first_author(citation_authors):\n",
    "    # Check for 'et al.' and 'and' to find the first author\n",
    "    if 'et al.' in citation_authors:\n",
    "        first_author = citation_authors.split('et al.')[0].strip()\n",
    "    elif ' and ' in citation_authors:\n",
    "        first_author = citation_authors.rsplit(' and ', 1)[0].split(',')[0].strip()\n",
    "    else:\n",
    "        first_author = citation_authors.split(',')[0].strip()\n",
    "    # Normalize the first author's name for comparison\n",
    "    return first_author.lower()\n",
    "\n",
    "\n",
    "# Function to split and parse citations in cases of citation \n",
    "# like (Culotta and Sorensen 2004; Bunescu and Mooney 2005; Ittoo and Bouma 2013)\n",
    "def split_and_parse_citation(citation):\n",
    "\n",
    "    # Remove outer parentheses\n",
    "    citation = citation.strip(\"()\")\n",
    "    # Split on semicolon if it's present, indicating multiple citations within one\n",
    "    if ';' in citation:\n",
    "        sub_citations = citation.split(';')\n",
    "    else:\n",
    "        sub_citations = [citation]\n",
    "    \n",
    "    # Parse each sub-citation for author names and year\n",
    "    for sub_citation in sub_citations:\n",
    "        # Splitting based on the last occurrence of space which is assumed to be before the year\n",
    "        *authors, year = sub_citation.rsplit(' ', 1)\n",
    "        authors = ' '.join(authors)  # Joining back the authors in case there are multiple names\n",
    "        parsed_citation = {'Author': identify_and_normalize_first_author(authors), 'Year': year}\n",
    "    \n",
    "    return parsed_citation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'authors': 'Alan Ansell; Maria Edoardo; Jonas Ponti; Sebastian Pfeiffer; Goran Ruder; Ivan GlavaÅ¡; Anna VuliÄ‡;  Korhonen',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b0',\n",
       "  'title': 'MAD-G: Multilingual adapter generation for efficient cross-lingual transfer',\n",
       "  'year': '2021'},\n",
       " {'authors': 'Robert Baldock; Hartmut Maennel; Behnam Neyshabur',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b1',\n",
       "  'title': 'Deep learning through the lens of example difficulty',\n",
       "  'year': '2021'},\n",
       " {'authors': 'Curran Associates; Inc ',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b2',\n",
       "  'title': '',\n",
       "  'year': ''},\n",
       " {'authors': 'Zoubin David A Cohn; Michael I Ghahramani;  Jordan',\n",
       "  'journal': 'Journal of artificial intelligence research',\n",
       "  'ref_id': 'b3',\n",
       "  'title': 'Active learning with statistical models',\n",
       "  'year': '1996'},\n",
       " {'authors': 'Sanjoy Dasgupta',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b4',\n",
       "  'title': 'Two faces of active learning',\n",
       "  'year': '2009'},\n",
       " {'authors': 'Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b5',\n",
       "  'title': 'BERT: Pre-training of deep bidirectional transformers for language understanding',\n",
       "  'year': '2019'},\n",
       " {'authors': 'Jesse Dodge; Gabriel Ilharco; Roy Schwartz; Ali Farhadi; Hannaneh Hajishirzi; Noah Smith',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b6',\n",
       "  'title': 'Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping',\n",
       "  'year': '2020'},\n",
       " {'authors': 'Liat Ein-Dor; Alon Halfon; Ariel Gera; Eyal Shnarch; Lena Dankin; Leshem Choshen; Marina Danilevsky; Ranit Aharonov; Yoav Katz; Noam Slonim',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b7',\n",
       "  'title': 'Active Learning for BERT: An Empirical Study',\n",
       "  'year': '2020'},\n",
       " {'authors': 'Yarin Gal; Zoubin Ghahramani',\n",
       "  'journal': 'PMLR',\n",
       "  'ref_id': 'b8',\n",
       "  'title': 'Dropout as a bayesian approximation: Representing model uncertainty in deep learning',\n",
       "  'year': '2016'},\n",
       " {'authors': 'Daniel Gissin; Shai Shalev-Shwartz',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b9',\n",
       "  'title': 'Discriminative active learning',\n",
       "  'year': '2019'},\n",
       " {'authors': 'Daniel GrieÃŸhaber; Johannes Maucher; Ngoc Thang Vu',\n",
       "  'journal': 'International Committee on Computational Linguistics',\n",
       "  'ref_id': 'b10',\n",
       "  'title': 'Fine-tuning BERT for low-resource natural language understanding via active learning',\n",
       "  'year': '2020'},\n",
       " {'authors': 'Suchin Gururangan; Ana MarasoviÄ‡; Swabha Swayamdipta; Kyle Lo; Iz Beltagy; Doug Downey; Noah A Smith',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b11',\n",
       "  'title': \"Don't stop pretraining: Adapt language models to domains and tasks\",\n",
       "  'year': '2020'},\n",
       " {'authors': 'Junxian He; Chunting Zhou; Xuezhe Ma; Taylor Berg-Kirkpatrick; Graham Neubig',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b12',\n",
       "  'title': 'Towards a unified view of parameter-efficient transfer learning',\n",
       "  'year': '2022'},\n",
       " {'authors': 'Ruidan He; Linlin Liu; Hai Ye; Qingyu Tan; Bosheng Ding; Liying Cheng; Jiawei Low; Lidong Bing; Luo Si',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b13',\n",
       "  'title': 'On the effectiveness of adapter-based tuning for pretrained language model adaptation',\n",
       "  'year': '2021'},\n",
       " {'authors': 'Neil Houlsby; Andrei Giurgiu; Stanislaw Jastrzebski; Bruna Morrone; Quentin De Laroussilhe; Andrea Gesmundo; Mona Attariyan; Sylvain Gelly',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b14',\n",
       "  'title': 'Parameter-efficient transfer learning for NLP',\n",
       "  'year': '2019'},\n",
       " {'authors': ' Pmlr', 'journal': '', 'ref_id': 'b15', 'title': '', 'year': ''},\n",
       " {'authors': 'J Edward; Yelong Hu; Phillip Shen; Zeyuan Wallis; Yuanzhi Allen-Zhu; Shean Li; Lu Wang; Weizhu Wang;  Chen',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b16',\n",
       "  'title': 'LoRA: Low-rank adaptation of large language models',\n",
       "  'year': '2022'},\n",
       " {'authors': 'Josip JukiÄ‡; Jan Å najder',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b17',\n",
       "  'title': 'Smooth sailing: Improving active learning for pre-trained language models with representation smoothness analysis',\n",
       "  'year': '2023'},\n",
       " {'authors': 'Siddharth Karamcheti; Ranjay Krishna; Li Fei-Fei; Christopher Manning',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b18',\n",
       "  'title': 'Mind your outliers! investigating the negative impact of outliers on active learning for visual question answering',\n",
       "  'year': '2021'},\n",
       " {'authors': 'Rabeeh Karimi Mahabadi; Sebastian Ruder; Mostafa Dehghani; James Henderson',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b19',\n",
       "  'title': 'Parameterefficient multi-task fine-tuning for transformers via shared hypernetworks',\n",
       "  'year': '2021'},\n",
       " {'authors': 'Seungwon Kim; Alex Shum; Nathan Susanj; Jonathan Hilgart',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b20',\n",
       "  'title': 'Revisiting pretraining with adapters',\n",
       "  'year': '2021'},\n",
       " {'authors': 'Simon Kornblith; Mohammad Norouzi; Honglak Lee; Geoffrey Hinton',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b21',\n",
       "  'title': 'Similarity of neural network representations revisited',\n",
       "  'year': '2019'},\n",
       " {'authors': ' Pmlr', 'journal': '', 'ref_id': 'b22', 'title': '', 'year': ''},\n",
       " {'authors': 'Jaeseong Lee; Seung-Won Hwang; Taesup Kim',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b23',\n",
       "  'title': 'FAD-X: Fusing adapters for cross-lingual transfer to low-resource languages',\n",
       "  'year': '2022'},\n",
       " {'authors': 'D David; William A Lewis;  Gale',\n",
       "  'journal': 'Springer',\n",
       "  'ref_id': 'b24',\n",
       "  'title': 'A sequential algorithm for training text classifiers',\n",
       "  'year': '1994'},\n",
       " {'authors': 'Lisa Xiang; Percy Li;  Liang',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b25',\n",
       "  'title': 'Prefix-tuning: Optimizing continuous prompts for generation',\n",
       "  'year': '2021'},\n",
       " {'authors': 'Xin Li; Dan Roth',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b26',\n",
       "  'title': 'Learning question classifiers',\n",
       "  'year': '2002'},\n",
       " {'authors': 'Yuning Mao; Lambert Mathias; Rui Hou; Amjad Almahairi; Hao Ma; Jiawei Han; Scott Yih; Madian Khabsa',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b27',\n",
       "  'title': 'UniPELT: A unified framework for parameter-efficient language model tuning',\n",
       "  'year': '2022'},\n",
       " {'authors': 'Katerina Margatina; Loic Barrault; Nikolaos Aletras',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b28',\n",
       "  'title': 'On the importance of effectively adapting pretrained language models for active learning',\n",
       "  'year': '2022'},\n",
       " {'authors': 'Katerina Margatina; Giorgos Vernikos; LoÃ¯c Barrault; Nikolaos Aletras',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b29',\n",
       "  'title': 'Active learning by acquiring contrastive examples',\n",
       "  'year': '2021'},\n",
       " {'authors': 'Marius Mosbach; Maksym Andriushchenko; Dietrich Klakow',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b30',\n",
       "  'title': 'On the stability of fine-tuning BERT: Misconceptions, explanations, and strong baselines',\n",
       "  'year': '2021'},\n",
       " {'authors': 'Bo Pang; Lillian Lee',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b31',\n",
       "  'title': 'A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts',\n",
       "  'year': '2004'},\n",
       " {'authors': 'Marinela ParoviÄ‡; Goran GlavaÅ¡; Ivan VuliÄ‡; Anna Korhonen',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b32',\n",
       "  'title': 'BAD-X: Bilingual adapters improve zero-shot cross-lingual transfer',\n",
       "  'year': '2022'},\n",
       " {'authors': 'Jonas Pfeiffer; Andreas RÃ¼cklÃ©; Clifton Poth; Aishwarya Kamath; Ivan VuliÄ‡; Sebastian Ruder; Kyunghyun Cho; Iryna Gurevych',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b33',\n",
       "  'title': 'AdapterHub: A framework for adapting transformers',\n",
       "  'year': '2020'},\n",
       " {'authors': 'Jonas Pfeiffer; Sebastian Ruder; Ivan VuliÄ‡; Maria Edoardo;  Ponti',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b34',\n",
       "  'title': 'Modular deep learning',\n",
       "  'year': '2023'},\n",
       " {'authors': 'Christopher SchrÃ¶der; Andreas Niekler; Martin Potthast',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b35',\n",
       "  'title': 'Revisiting uncertainty-based query strategies for active learning with transformers',\n",
       "  'year': '2022'},\n",
       " {'authors': 'Ozan Sener; Silvio Savarese',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b36',\n",
       "  'title': 'Active learning for convolutional neural networks: A core-set approach',\n",
       "  'year': '2018'},\n",
       " {'authors': 'Burr Settles',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b37',\n",
       "  'title': 'Active learning literature survey',\n",
       "  'year': '2009'},\n",
       " {'authors': 'Artem Shelmanov; Dmitri Puzyrev; Lyubov Kupriyanova; Denis Belyakov; Daniil Larionov; Nikita Khromov; Olga Kozlova; Ekaterina Artemova; V Dmitry; Alexander Dylov;  Panchenko',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b38',\n",
       "  'title': 'Active learning for sequence tagging with deep pre-trained models and Bayesian uncertainty estimates',\n",
       "  'year': '2021'},\n",
       " {'authors': 'Richard Socher; John Bauer; Christopher D Manning; Andrew Y Ng',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b39',\n",
       "  'title': 'Parsing with compositional vector grammars',\n",
       "  'year': '2013'},\n",
       " {'authors': 'Nitish Srivastava; Geoffrey Hinton; Alex Krizhevsky; Ilya Sutskever; Ruslan Salakhutdinov',\n",
       "  'journal': 'Journal of Machine Learning Research',\n",
       "  'ref_id': 'b40',\n",
       "  'title': 'Dropout: A simple way to prevent neural networks from overfitting',\n",
       "  'year': '2014'},\n",
       " {'authors': 'Cory Stephenson; Suchismita Padhy; Abhinav Ganesh; Yue Hui; Hanlin Tang; Sueyeon Chung',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b41',\n",
       "  'title': 'On the geometry of generalization and memorization in deep neural networks',\n",
       "  'year': '2021'},\n",
       " {'authors': 'Mariya Toneva; Alessandro Sordoni; Remi Tachet Des Combes; Adam Trischler; Yoshua Bengio; Geoffrey J Gordon',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b42',\n",
       "  'title': 'An empirical study of example forgetting during deep neural network learning',\n",
       "  'year': '2019'},\n",
       " {'authors': 'Yue Yu; Lingkai Kong; Jieyu Zhang; Rongzhi Zhang; Chao Zhang',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b43',\n",
       "  'title': 'AcTune: Uncertainty-based active self-training for active fine-tuning of pretrained language models',\n",
       "  'year': '2022'},\n",
       " {'authors': 'Michelle Yuan; Hsuan-Tien Lin; Jordan Boyd-Graber',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b44',\n",
       "  'title': 'Cold-start active learning through selfsupervised language modeling',\n",
       "  'year': '2020'},\n",
       " {'authors': 'Tianyi Zhang; Felix Wu; Arzoo Katiyar; Kilian Q Weinberger; Yoav Artzi',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b45',\n",
       "  'title': 'Revisiting few-sample BERT fine-tuning',\n",
       "  'year': '2021'},\n",
       " {'authors': 'Xiang Zhang; Junbo Zhao; Yann Lecun',\n",
       "  'journal': 'Advances in neural information processing systems',\n",
       "  'ref_id': 'b46',\n",
       "  'title': 'Character-level convolutional networks for text classification',\n",
       "  'year': '2015'}]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "references = annotated_article[0]['references']\n",
    "references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize and extract the first author's name\n",
    "def get_first_author(authors_str):\n",
    "    first_author = authors_str.split(';')[0].strip()\n",
    "    # Normalize the first author's name for comparison\n",
    "    return first_author.lower()\n",
    "\n",
    "# Generalized regular expression for detecting years in various date formats and standalone years\n",
    "\n",
    "# Function to detect various year patterns and extract the year\n",
    "def extract_years(string):\n",
    "    general_year_pattern = re.compile(r'(?:\\b|\\D)(\\d{4})(?:\\b|\\D)')\n",
    "    # Find all matches for the general year pattern\n",
    "\n",
    "    matches = general_year_pattern.findall(string)\n",
    "    # Add all unique years found in this string\n",
    "    year = matches[0] if matches else None\n",
    "    return year\n",
    "\n",
    "# Function to match citations with references\n",
    "def match_citations_with_references(citation, references):\n",
    "    match = None\n",
    "    citation_first_author = citation['Author']\n",
    "    citation_year = citation['Year'].strip()\n",
    "    for ref in references:\n",
    "        ref_first_author = get_first_author(ref['authors'])\n",
    "        ref_year = extract_years(ref['year']) if ref['year'] is not None else None\n",
    "        # Check for match by first author and year\n",
    "        if citation_first_author in ref_first_author: #and (citation_year == ref_year or ref_year is None):\n",
    "            match = {\n",
    "                'ref_id': ref['ref_id']\n",
    "            }\n",
    "    return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with the first sample\n",
    "for citation in annotated_article[0]['citation_data']:\n",
    "    parsed_name = split_and_parse_citation(citation['Citation'])\n",
    "    match = match_citations_with_references(parsed_name, references)\n",
    "    citation['ref_id'] = match['ref_id'] if match else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Category': 'Methodological Basis',\n",
       "  'Citation': '(Cohn et al., 1996)',\n",
       "  'Explanation': 'The cited work introduces the concept of active learning as a potential solution to the challenge of data labeling in low-resource settings, which the citing paper builds upon in its research on efficient finetuning methods for PLMs.',\n",
       "  'ref_id': 'b3'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Settles, 2009)',\n",
       "  'Explanation': 'The cited work provides a more in-depth discussion of active learning and its potential benefits in reducing labeling costs, which the citing paper further explores in the context of PLMs and low-resource settings.',\n",
       "  'ref_id': 'b37'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Dasgupta, 2011)',\n",
       "  'Explanation': 'The cited work highlights the importance of label complexity in active learning and the need to reduce it for efficient model training, which the citing paper addresses in its research on efficient finetuning methods for PLMs in low-resource settings.',\n",
       "  'ref_id': 'b4'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Gururangan et al., 2020)',\n",
       "  'Explanation': 'The cited work introduces the concept of task-adaptive pre-training (TAPT), which the citing paper adopts in their research to further reduce the label complexity in AL research.',\n",
       "  'ref_id': 'b11'},\n",
       " {'Category': 'Extension or Continuation',\n",
       "  'Citation': '(Houlsby et al., 2019)',\n",
       "  'Explanation': 'The cited work introduces the concept of adapters as compact modules for fine-tuning PLMs, which the citing paper extends by discussing the use of adapters for parameter-efficient fine-tuning (PEFT) in AL research.',\n",
       "  'ref_id': 'b14'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': '(Pfeiffer et al., 2023)',\n",
       "  'Explanation': 'The cited work discusses the use of modular learning in PEFT, which the citing paper references as a method for parameter-efficient fine-tuning in AL research.',\n",
       "  'ref_id': 'b34'},\n",
       " {'Category': 'Supporting Evidence',\n",
       "  'Citation': '(He et al., 2021;Li and Liang, 2021;Karimi Mahabadi et al., 2021)',\n",
       "  'Explanation': 'The cited works have revealed that PEFT methods outperform full fine-tuning in low-resource settings, which is a key finding that supports the claims made in the citing paper about the potential benefits of PEFT in this context.',\n",
       "  'ref_id': 'b19'},\n",
       " {'Category': 'Supporting Evidence',\n",
       "  'Citation': '(Toneva et al., 2019)',\n",
       "  'Explanation': 'The cited work by Toneva et al. (2019) provides a method for analyzing the properties of PEFT and FFT, which the citing paper uses to understand the reason for the improved performance of PEFT in low-resource AL scenarios.',\n",
       "  'ref_id': 'b42'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Ein-Dor et al., 2020)',\n",
       "  'Explanation': 'The cited work by Ein-Dor et al. (2020) provides a conventional approach for integrating PLMs with AL, which the citing paper adopts in their research to investigate the use of PEFT techniques in low-resource settings.',\n",
       "  'ref_id': 'b7'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Margatina et al., 2021)',\n",
       "  'Explanation': 'The cited work by Margatina et al. (2021) also contributes to the research on combining PLMs with AL, providing a method for fine-tuning the model in each AL step.',\n",
       "  'ref_id': 'b29'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Shelmanov et al., 2021)',\n",
       "  'Explanation': 'The cited work by Shelmanov et al. (2021) further adds to the research on integrating PLMs with AL, by discussing the use of fine-tuning in each AL step.',\n",
       "  'ref_id': 'b38'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Karamcheti et al., 2021)',\n",
       "  'Explanation': 'The cited work by Karamcheti et al. (2021) also contributes to the research on combining PLMs with AL, by exploring the use of fine-tuning in each AL step.',\n",
       "  'ref_id': 'b18'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(SchrÃ¶der et al., 2022)',\n",
       "  'Explanation': 'The cited work by SchrÃ¶der et al. (2022) further adds to the research on integrating PLMs with AL, by discussing the use of fine-tuning in each AL step.',\n",
       "  'ref_id': 'b35'},\n",
       " {'Category': 'Extension or Continuation',\n",
       "  'Citation': '(Mosbach et al., 2021)',\n",
       "  'Explanation': 'The cited work by Mosbach et al. (2021) extends the research on fine-tuning in low-resource settings, by discussing the instability of the process and its impact on AL.',\n",
       "  'ref_id': 'b30'},\n",
       " {'Category': 'Extension or Continuation',\n",
       "  'Citation': '(Zhang et al., 2021)',\n",
       "  'Explanation': 'The cited work by Zhang et al. (2021) also extends the research on fine-tuning in low-resource settings, by discussing the instability of the process and its impact on AL.',\n",
       "  'ref_id': 'b46'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': '(Dodge et al., 2020)',\n",
       "  'Explanation': 'The cited work by Dodge et al. (2020) provides a data source for the research on fine-tuning in low-resource settings, by discussing the sensitivity of the process to weight initialization and data ordering.',\n",
       "  'ref_id': 'b6'},\n",
       " {'Category': 'Supporting Evidence',\n",
       "  'Citation': '(GrieÃŸhaber et al., 2020)',\n",
       "  'Explanation': 'The cited work by GrieÃŸhaber et al. (2020) provides evidence that the choice of training regime is more critical than the choice of the AL method in improving AL performance.',\n",
       "  'ref_id': 'b10'},\n",
       " {'Category': 'Supporting Evidence',\n",
       "  'Citation': '(Yuan et al., 2020)',\n",
       "  'Explanation': 'The cited work by Yuan et al. (2020) further supports the claim that the training regime is more important than the AL method in enhancing AL performance.',\n",
       "  'ref_id': 'b44'},\n",
       " {'Category': 'Supporting Evidence',\n",
       "  'Citation': '(Yu et al., 2022)',\n",
       "  'Explanation': 'The cited work by Yu et al. (2022) provides additional evidence that the training regime is a critical factor in improving AL performance.',\n",
       "  'ref_id': 'b44'},\n",
       " {'Category': 'Extension or Continuation',\n",
       "  'Citation': '(Margatina et al., 2022)',\n",
       "  'Explanation': 'The cited work by Margatina et al. (2022) extends the research on the effectiveness of TAPT in enhancing AL performance by providing further insights and data.',\n",
       "  'ref_id': 'b29'},\n",
       " {'Category': 'Extension or Continuation',\n",
       "  'Citation': '(JukiÄ‡ and Å najder, 2023)',\n",
       "  'Explanation': 'The cited work by JukiÄ‡ and Å najder (2023) continues the research on TAPT by exploring new dimensions and variables in enhancing AL performance.',\n",
       "  'ref_id': 'b17'},\n",
       " {'Category': 'Supporting Evidence',\n",
       "  'Citation': '(Ansell et al., 2021)',\n",
       "  'Explanation': 'The cited work by Ansell et al. (2021) provides evidence on the effectiveness of cross-lingual transfer for low-resource languages in the context of adapters.',\n",
       "  'ref_id': 'b0'},\n",
       " {'Category': 'Supporting Evidence',\n",
       "  'Citation': '(Lee et al., 2022)',\n",
       "  'Explanation': 'The cited work by Lee et al. (2022) further supports the research on the use of adapters in low-resource settings for cross-lingual transfer.',\n",
       "  'ref_id': 'b23'},\n",
       " {'Category': 'Supporting Evidence',\n",
       "  'Citation': '(ParoviÄ‡ et al., 2022)',\n",
       "  'Explanation': 'The cited work by ParoviÄ‡ et al. (2022) provides additional insights on the use of adapters in low-resource settings for cross-lingual transfer.',\n",
       "  'ref_id': 'b32'},\n",
       " {'Category': 'Supporting Evidence',\n",
       "  'Citation': '(Li and Liang, 2021)',\n",
       "  'Explanation': 'The cited work by Li and Liang (2021) supports the research on the use of adapters in monolingual settings with scarce data.',\n",
       "  'ref_id': 'b26'},\n",
       " {'Category': 'Supporting Evidence',\n",
       "  'Citation': '(Mao et al., 2022)',\n",
       "  'Explanation': 'The cited work by Mao et al. (2022) further supports the research on the use of adapters in monolingual settings with scarce data.',\n",
       "  'ref_id': 'b27'},\n",
       " {'Category': 'Supporting Evidence',\n",
       "  'Citation': '(He et al., 2021)',\n",
       "  'Explanation': 'The cited work by He et al. (2021) provides evidence on the stability and generalization capabilities of adapter-based tuning in monolingual settings with scarce data.',\n",
       "  'ref_id': 'b44'},\n",
       " {'Category': 'Supporting Evidence',\n",
       "  'Citation': '(Kim et al., 2021)',\n",
       "  'Explanation': 'The cited work by Kim et al. (2021) provides evidence that the benefits of integrating TAPT with adapters tend to taper off as the amount of data increases, which is relevant to the discussion in the citing paper about the limitations of using adapters in low-resource setups.',\n",
       "  'ref_id': 'b20'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': '(Pang and Lee, 2004)',\n",
       "  'Explanation': 'The cited work by Pang and Lee serves as the data source for the SUBJ dataset used in the citing paper for the single-text classification task.',\n",
       "  'ref_id': 'b31'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': '(Li and Roth, 2002)',\n",
       "  'Explanation': 'The cited work by Li and Roth is the data source for the TREC dataset used in the single-text classification task in the citing paper.',\n",
       "  'ref_id': 'b26'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': '(Socher et al., 2013)',\n",
       "  'Explanation': 'The cited work by Socher et al. is the data source for the SST dataset used in the single-text classification task in the citing paper.',\n",
       "  'ref_id': 'b39'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': '(Zhang et al., 2015)',\n",
       "  'Explanation': 'The cited work by Zhang et al. is the data source for the AGN dataset used in the single-text classification task in the citing paper.',\n",
       "  'ref_id': 'b46'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Houlsby et al., 2019)',\n",
       "  'Explanation': 'The cited work introduces the concept of trainable bottleneck layers in Transformer layers, which the citing paper adopts in the development of the Adapter PEFT technique.',\n",
       "  'ref_id': 'b14'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Li and Liang, 2021)',\n",
       "  'Explanation': 'The cited work presents the Prefix-tuning PEFT technique, which the citing paper incorporates in the development of the UniPELT method by adding new parameters in the multi-head attention blocks of Transformer layers.',\n",
       "  'ref_id': 'b26'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Hu et al., 2022)',\n",
       "  'Explanation': 'The cited work introduces the LoRA PEFT technique, which the citing paper incorporates in the development of the UniPELT method by representing an additive method that incorporates trainable low-rank decomposition matrices in the layers of a pre-trained model.',\n",
       "  'ref_id': None},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Mao et al., 2022)',\n",
       "  'Explanation': 'The cited work presents the UniPELT PEFT method, which the citing paper considers as a combination of multiple PEFT approaches, including LoRA, Prefix-tuning, and Adapter, in a single unified setup with gating mechanisms for effective activation.',\n",
       "  'ref_id': 'b27'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Devlin et al., 2019)',\n",
       "  'Explanation': 'The cited work by Devlin et al. (2019) provides the base PLM (BERT) that the citing paper uses as the foundation for their research on adapters.',\n",
       "  'ref_id': 'b5'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Lewis and Gale, 1994)',\n",
       "  'Explanation': 'The cited work by Lewis and Gale (1994) provides the maximum entropy (ENT) strategy for sampling instances in the field of uncertainty strategies, which the citing paper adopts as a method for instance selection.',\n",
       "  'ref_id': None},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Gal and Ghahramani, 2016)',\n",
       "  'Explanation': 'The cited work by Gal and Ghahramani (2016) introduces the Monte Carlo dropout (MC) method for instance selection based on the stochasticity of forward passes with dropout layers, which the citing paper utilizes in the field of uncertainty strategies.',\n",
       "  'ref_id': 'b8'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Srivastava et al., 2014)',\n",
       "  'Explanation': 'The cited work by Srivastava et al. (2014) presents the use of dropout layers in forward passes, which the citing paper references in the context of the Monte Carlo dropout (MC) method for instance selection in the field of uncertainty strategies.',\n",
       "  'ref_id': 'b40'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Sener and Savarese, 2018)',\n",
       "  'Explanation': 'The cited work by Sener and Savarese (2018) introduces the core-set (CS) method for instance selection in the field of learning representations of the acquisition model, which the citing paper adopts as a method for encouraging instance diversity.',\n",
       "  'ref_id': 'b36'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(SchrÃ¶der et al., 2022)',\n",
       "  'Explanation': 'The cited work provides a recommendation for using AUC as a suitable approximation of AL feasibility, which the citing paper adopts in their research to evaluate the performance of AL methods.',\n",
       "  'ref_id': 'b35'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(JukiÄ‡ and Å najder, 2023)',\n",
       "  'Explanation': 'The cited work also recommends using AUC as a summary numeric score in AL, which the citing paper adopts in their research to evaluate the performance of AL methods.',\n",
       "  'ref_id': 'b17'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Li and Liang, 2021)',\n",
       "  'Explanation': 'The cited work by Li and Liang provides the basis for the use of adapters in low-resource settings in the citing paper.',\n",
       "  'ref_id': 'b26'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Mao et al., 2022)',\n",
       "  'Explanation': 'The cited work by Mao et al. contributes to the understanding of the use of adapters in low-resource settings in the citing paper.',\n",
       "  'ref_id': 'b27'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(He et al., 2021)',\n",
       "  'Explanation': 'The cited work by He et al. further builds upon the research on the use of adapters in low-resource settings in the citing paper.',\n",
       "  'ref_id': 'b44'},\n",
       " {'Category': 'Extension or Continuation',\n",
       "  'Citation': '(Li and Liang, 2021)',\n",
       "  'Explanation': 'The citing paper extends the research on the use of adapters in low-resource settings by conducting a more nuanced analysis and comparing multiple adapter variants with FFT under the passive learning setup.',\n",
       "  'ref_id': 'b26'},\n",
       " {'Category': 'Extension or Continuation',\n",
       "  'Citation': '(Mao et al., 2022)',\n",
       "  'Explanation': 'The citing paper further extends the research on the use of adapters in low-resource settings by generating detailed learning curves to facilitate the comparison of multiple adapters with FFT in the passive learning setup.',\n",
       "  'ref_id': 'b27'},\n",
       " {'Category': 'Extension or Continuation',\n",
       "  'Citation': '(He et al., 2021)',\n",
       "  'Explanation': \"The citing paper continues the research on the use of adapters in low-resource settings by looking into how the models' performance changes as the training set increases.\",\n",
       "  'ref_id': 'b44'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Toneva et al., 2019)',\n",
       "  'Explanation': 'The cited work by Toneva et al. (2019) provides a methodology for analyzing forgetting dynamics in training examples, which the citing paper adopts to study the occurrence of forgetting events in adapters and their impact on AL data selection.',\n",
       "  'ref_id': 'b42'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(He et al., 2021)',\n",
       "  'Explanation': 'The cited work by He et al. (2021) provides the inspiration for the layerwise examination of similarity in the citing paper, which is used to analyze the effect of PEFT and FFT on AL selection with respect to their layerwise similarity to the base model.',\n",
       "  'ref_id': 'b44'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Li and Liang, 2021)',\n",
       "  'Explanation': 'The cited work by Li and Liang (2021) is used to bolster the findings of the citing paper by exploring the stability of representations in scenarios with limited resources.',\n",
       "  'ref_id': 'b26'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Mao et al., 2022)',\n",
       "  'Explanation': 'The cited work by Mao et al. (2022) contributes to the analysis of the stability of representations in the citing paper, providing insights into the use of adapters in scenarios with limited resources.',\n",
       "  'ref_id': 'b27'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': '(Stephenson et al., 2021)',\n",
       "  'Explanation': 'The data source cited by Stephenson et al. (2021) is used to draw inspiration for the layerwise examination of similarity in the citing paper, which is conducted to analyze the effect of PEFT and FFT on AL selection with respect to their layerwise similarity to the base model.',\n",
       "  'ref_id': 'b41'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': '(Baldock et al., 2021)',\n",
       "  'Explanation': 'The data source cited by Baldock et al. (2021) is used in the citing paper to support the claim that different layers of networks specialize in different features, with earlier layers acquiring more generalized knowledge and deeper layers focusing on task-specific information.',\n",
       "  'ref_id': 'b1'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Pfeiffer et al., 2020)',\n",
       "  'Explanation': 'The cited work provides the implementation of adapters used in the citing paper, which serves as a methodological basis for the research conducted in the citing paper.',\n",
       "  'ref_id': 'b34'}]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated_article[0]['citation_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'authors': 'Xin Li; Dan Roth',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b26',\n",
       "  'title': 'Learning question classifiers',\n",
       "  'year': '2002'},\n",
       " {'authors': 'Yuning Mao; Lambert Mathias; Rui Hou; Amjad Almahairi; Hao Ma; Jiawei Han; Scott Yih; Madian Khabsa',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b27',\n",
       "  'title': 'UniPELT: A unified framework for parameter-efficient language model tuning',\n",
       "  'year': '2022'},\n",
       " {'authors': 'Katerina Margatina; Loic Barrault; Nikolaos Aletras',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b28',\n",
       "  'title': 'On the importance of effectively adapting pretrained language models for active learning',\n",
       "  'year': '2022'},\n",
       " {'authors': 'Katerina Margatina; Giorgos Vernikos; LoÃ¯c Barrault; Nikolaos Aletras',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b29',\n",
       "  'title': 'Active learning by acquiring contrastive examples',\n",
       "  'year': '2021'},\n",
       " {'authors': 'Marius Mosbach; Maksym Andriushchenko; Dietrich Klakow',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b30',\n",
       "  'title': 'On the stability of fine-tuning BERT: Misconceptions, explanations, and strong baselines',\n",
       "  'year': '2021'},\n",
       " {'authors': 'Bo Pang; Lillian Lee',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b31',\n",
       "  'title': 'A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts',\n",
       "  'year': '2004'},\n",
       " {'authors': 'Marinela ParoviÄ‡; Goran GlavaÅ¡; Ivan VuliÄ‡; Anna Korhonen',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b32',\n",
       "  'title': 'BAD-X: Bilingual adapters improve zero-shot cross-lingual transfer',\n",
       "  'year': '2022'},\n",
       " {'authors': 'Jonas Pfeiffer; Andreas RÃ¼cklÃ©; Clifton Poth; Aishwarya Kamath; Ivan VuliÄ‡; Sebastian Ruder; Kyunghyun Cho; Iryna Gurevych',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b33',\n",
       "  'title': 'AdapterHub: A framework for adapting transformers',\n",
       "  'year': '2020'},\n",
       " {'authors': 'Jonas Pfeiffer; Sebastian Ruder; Ivan VuliÄ‡; Maria Edoardo;  Ponti',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b34',\n",
       "  'title': 'Modular deep learning',\n",
       "  'year': '2023'},\n",
       " {'authors': 'Christopher SchrÃ¶der; Andreas Niekler; Martin Potthast',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b35',\n",
       "  'title': 'Revisiting uncertainty-based query strategies for active learning with transformers',\n",
       "  'year': '2022'},\n",
       " {'authors': 'Ozan Sener; Silvio Savarese',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b36',\n",
       "  'title': 'Active learning for convolutional neural networks: A core-set approach',\n",
       "  'year': '2018'},\n",
       " {'authors': 'Burr Settles',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b37',\n",
       "  'title': 'Active learning literature survey',\n",
       "  'year': '2009'},\n",
       " {'authors': 'Artem Shelmanov; Dmitri Puzyrev; Lyubov Kupriyanova; Denis Belyakov; Daniil Larionov; Nikita Khromov; Olga Kozlova; Ekaterina Artemova; V Dmitry; Alexander Dylov;  Panchenko',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b38',\n",
       "  'title': 'Active learning for sequence tagging with deep pre-trained models and Bayesian uncertainty estimates',\n",
       "  'year': '2021'},\n",
       " {'authors': 'Richard Socher; John Bauer; Christopher D Manning; Andrew Y Ng',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b39',\n",
       "  'title': 'Parsing with compositional vector grammars',\n",
       "  'year': '2013'},\n",
       " {'authors': 'Nitish Srivastava; Geoffrey Hinton; Alex Krizhevsky; Ilya Sutskever; Ruslan Salakhutdinov',\n",
       "  'journal': 'Journal of Machine Learning Research',\n",
       "  'ref_id': 'b40',\n",
       "  'title': 'Dropout: A simple way to prevent neural networks from overfitting',\n",
       "  'year': '2014'},\n",
       " {'authors': 'Cory Stephenson; Suchismita Padhy; Abhinav Ganesh; Yue Hui; Hanlin Tang; Sueyeon Chung',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b41',\n",
       "  'title': 'On the geometry of generalization and memorization in deep neural networks',\n",
       "  'year': '2021'},\n",
       " {'authors': 'Mariya Toneva; Alessandro Sordoni; Remi Tachet Des Combes; Adam Trischler; Yoshua Bengio; Geoffrey J Gordon',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b42',\n",
       "  'title': 'An empirical study of example forgetting during deep neural network learning',\n",
       "  'year': '2019'},\n",
       " {'authors': 'Yue Yu; Lingkai Kong; Jieyu Zhang; Rongzhi Zhang; Chao Zhang',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b43',\n",
       "  'title': 'AcTune: Uncertainty-based active self-training for active fine-tuning of pretrained language models',\n",
       "  'year': '2022'},\n",
       " {'authors': 'Michelle Yuan; Hsuan-Tien Lin; Jordan Boyd-Graber',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b44',\n",
       "  'title': 'Cold-start active learning through selfsupervised language modeling',\n",
       "  'year': '2020'},\n",
       " {'authors': 'Tianyi Zhang; Felix Wu; Arzoo Katiyar; Kilian Q Weinberger; Yoav Artzi',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b45',\n",
       "  'title': 'Revisiting few-sample BERT fine-tuning',\n",
       "  'year': '2021'},\n",
       " {'authors': 'Xiang Zhang; Junbo Zhao; Yann Lecun',\n",
       "  'journal': 'Advances in neural information processing systems',\n",
       "  'ref_id': 'b46',\n",
       "  'title': 'Character-level convolutional networks for text classification',\n",
       "  'year': '2015'}]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "references[26:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to group the citation data by ref_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'b3': [{'Category': 'Methodological Basis', 'Citation': '(Cohn et al., 1996)', 'Explanation': 'The cited work introduces the concept of active learning as a potential solution to the challenge of data labeling in low-resource settings, which the citing paper builds upon in its research on efficient finetuning methods for PLMs.'}], 'b37': [{'Category': 'Methodological Basis', 'Citation': '(Settles, 2009)', 'Explanation': 'The cited work provides a more in-depth discussion of active learning and its potential benefits in reducing labeling costs, which the citing paper further explores in the context of PLMs and low-resource settings.'}], 'b4': [{'Category': 'Methodological Basis', 'Citation': '(Dasgupta, 2011)', 'Explanation': 'The cited work highlights the importance of label complexity in active learning and the need to reduce it for efficient model training, which the citing paper addresses in its research on efficient finetuning methods for PLMs in low-resource settings.'}], 'b11': [{'Category': 'Methodological Basis', 'Citation': '(Gururangan et al., 2020)', 'Explanation': 'The cited work introduces the concept of task-adaptive pre-training (TAPT), which the citing paper adopts in their research to further reduce the label complexity in AL research.'}], 'b14': [{'Category': 'Extension or Continuation', 'Citation': '(Houlsby et al., 2019)', 'Explanation': 'The cited work introduces the concept of adapters as compact modules for fine-tuning PLMs, which the citing paper extends by discussing the use of adapters for parameter-efficient fine-tuning (PEFT) in AL research.'}, {'Category': 'Methodological Basis', 'Citation': '(Houlsby et al., 2019)', 'Explanation': 'The cited work introduces the concept of trainable bottleneck layers in Transformer layers, which the citing paper adopts in the development of the Adapter PEFT technique.'}], 'b34': [{'Category': 'Data Source', 'Citation': '(Pfeiffer et al., 2023)', 'Explanation': 'The cited work discusses the use of modular learning in PEFT, which the citing paper references as a method for parameter-efficient fine-tuning in AL research.'}, {'Category': 'Methodological Basis', 'Citation': '(Pfeiffer et al., 2020)', 'Explanation': 'The cited work provides the implementation of adapters used in the citing paper, which serves as a methodological basis for the research conducted in the citing paper.'}], 'b19': [{'Category': 'Supporting Evidence', 'Citation': '(He et al., 2021;Li and Liang, 2021;Karimi Mahabadi et al., 2021)', 'Explanation': 'The cited works have revealed that PEFT methods outperform full fine-tuning in low-resource settings, which is a key finding that supports the claims made in the citing paper about the potential benefits of PEFT in this context.'}], 'b42': [{'Category': 'Supporting Evidence', 'Citation': '(Toneva et al., 2019)', 'Explanation': 'The cited work by Toneva et al. (2019) provides a method for analyzing the properties of PEFT and FFT, which the citing paper uses to understand the reason for the improved performance of PEFT in low-resource AL scenarios.'}, {'Category': 'Methodological Basis', 'Citation': '(Toneva et al., 2019)', 'Explanation': 'The cited work by Toneva et al. (2019) provides a methodology for analyzing forgetting dynamics in training examples, which the citing paper adopts to study the occurrence of forgetting events in adapters and their impact on AL data selection.'}], 'b7': [{'Category': 'Methodological Basis', 'Citation': '(Ein-Dor et al., 2020)', 'Explanation': 'The cited work by Ein-Dor et al. (2020) provides a conventional approach for integrating PLMs with AL, which the citing paper adopts in their research to investigate the use of PEFT techniques in low-resource settings.'}], 'b29': [{'Category': 'Methodological Basis', 'Citation': '(Margatina et al., 2021)', 'Explanation': 'The cited work by Margatina et al. (2021) also contributes to the research on combining PLMs with AL, providing a method for fine-tuning the model in each AL step.'}, {'Category': 'Extension or Continuation', 'Citation': '(Margatina et al., 2022)', 'Explanation': 'The cited work by Margatina et al. (2022) extends the research on the effectiveness of TAPT in enhancing AL performance by providing further insights and data.'}], 'b38': [{'Category': 'Methodological Basis', 'Citation': '(Shelmanov et al., 2021)', 'Explanation': 'The cited work by Shelmanov et al. (2021) further adds to the research on integrating PLMs with AL, by discussing the use of fine-tuning in each AL step.'}], 'b18': [{'Category': 'Methodological Basis', 'Citation': '(Karamcheti et al., 2021)', 'Explanation': 'The cited work by Karamcheti et al. (2021) also contributes to the research on combining PLMs with AL, by exploring the use of fine-tuning in each AL step.'}], 'b35': [{'Category': 'Methodological Basis', 'Citation': '(SchrÃ¶der et al., 2022)', 'Explanation': 'The cited work by SchrÃ¶der et al. (2022) further adds to the research on integrating PLMs with AL, by discussing the use of fine-tuning in each AL step.'}, {'Category': 'Methodological Basis', 'Citation': '(SchrÃ¶der et al., 2022)', 'Explanation': 'The cited work provides a recommendation for using AUC as a suitable approximation of AL feasibility, which the citing paper adopts in their research to evaluate the performance of AL methods.'}], 'b30': [{'Category': 'Extension or Continuation', 'Citation': '(Mosbach et al., 2021)', 'Explanation': 'The cited work by Mosbach et al. (2021) extends the research on fine-tuning in low-resource settings, by discussing the instability of the process and its impact on AL.'}], 'b46': [{'Category': 'Extension or Continuation', 'Citation': '(Zhang et al., 2021)', 'Explanation': 'The cited work by Zhang et al. (2021) also extends the research on fine-tuning in low-resource settings, by discussing the instability of the process and its impact on AL.'}, {'Category': 'Data Source', 'Citation': '(Zhang et al., 2015)', 'Explanation': 'The cited work by Zhang et al. is the data source for the AGN dataset used in the single-text classification task in the citing paper.'}], 'b6': [{'Category': 'Data Source', 'Citation': '(Dodge et al., 2020)', 'Explanation': 'The cited work by Dodge et al. (2020) provides a data source for the research on fine-tuning in low-resource settings, by discussing the sensitivity of the process to weight initialization and data ordering.'}], 'b10': [{'Category': 'Supporting Evidence', 'Citation': '(GrieÃŸhaber et al., 2020)', 'Explanation': 'The cited work by GrieÃŸhaber et al. (2020) provides evidence that the choice of training regime is more critical than the choice of the AL method in improving AL performance.'}], 'b44': [{'Category': 'Supporting Evidence', 'Citation': '(Yuan et al., 2020)', 'Explanation': 'The cited work by Yuan et al. (2020) further supports the claim that the training regime is more important than the AL method in enhancing AL performance.'}, {'Category': 'Supporting Evidence', 'Citation': '(Yu et al., 2022)', 'Explanation': 'The cited work by Yu et al. (2022) provides additional evidence that the training regime is a critical factor in improving AL performance.'}, {'Category': 'Supporting Evidence', 'Citation': '(He et al., 2021)', 'Explanation': 'The cited work by He et al. (2021) provides evidence on the stability and generalization capabilities of adapter-based tuning in monolingual settings with scarce data.'}, {'Category': 'Methodological Basis', 'Citation': '(He et al., 2021)', 'Explanation': 'The cited work by He et al. further builds upon the research on the use of adapters in low-resource settings in the citing paper.'}, {'Category': 'Extension or Continuation', 'Citation': '(He et al., 2021)', 'Explanation': \"The citing paper continues the research on the use of adapters in low-resource settings by looking into how the models' performance changes as the training set increases.\"}, {'Category': 'Methodological Basis', 'Citation': '(He et al., 2021)', 'Explanation': 'The cited work by He et al. (2021) provides the inspiration for the layerwise examination of similarity in the citing paper, which is used to analyze the effect of PEFT and FFT on AL selection with respect to their layerwise similarity to the base model.'}], 'b17': [{'Category': 'Extension or Continuation', 'Citation': '(JukiÄ‡ and Å najder, 2023)', 'Explanation': 'The cited work by JukiÄ‡ and Å najder (2023) continues the research on TAPT by exploring new dimensions and variables in enhancing AL performance.'}, {'Category': 'Methodological Basis', 'Citation': '(JukiÄ‡ and Å najder, 2023)', 'Explanation': 'The cited work also recommends using AUC as a summary numeric score in AL, which the citing paper adopts in their research to evaluate the performance of AL methods.'}], 'b0': [{'Category': 'Supporting Evidence', 'Citation': '(Ansell et al., 2021)', 'Explanation': 'The cited work by Ansell et al. (2021) provides evidence on the effectiveness of cross-lingual transfer for low-resource languages in the context of adapters.'}], 'b23': [{'Category': 'Supporting Evidence', 'Citation': '(Lee et al., 2022)', 'Explanation': 'The cited work by Lee et al. (2022) further supports the research on the use of adapters in low-resource settings for cross-lingual transfer.'}], 'b32': [{'Category': 'Supporting Evidence', 'Citation': '(ParoviÄ‡ et al., 2022)', 'Explanation': 'The cited work by ParoviÄ‡ et al. (2022) provides additional insights on the use of adapters in low-resource settings for cross-lingual transfer.'}], 'b26': [{'Category': 'Supporting Evidence', 'Citation': '(Li and Liang, 2021)', 'Explanation': 'The cited work by Li and Liang (2021) supports the research on the use of adapters in monolingual settings with scarce data.'}, {'Category': 'Data Source', 'Citation': '(Li and Roth, 2002)', 'Explanation': 'The cited work by Li and Roth is the data source for the TREC dataset used in the single-text classification task in the citing paper.'}, {'Category': 'Methodological Basis', 'Citation': '(Li and Liang, 2021)', 'Explanation': 'The cited work presents the Prefix-tuning PEFT technique, which the citing paper incorporates in the development of the UniPELT method by adding new parameters in the multi-head attention blocks of Transformer layers.'}, {'Category': 'Methodological Basis', 'Citation': '(Li and Liang, 2021)', 'Explanation': 'The cited work by Li and Liang provides the basis for the use of adapters in low-resource settings in the citing paper.'}, {'Category': 'Extension or Continuation', 'Citation': '(Li and Liang, 2021)', 'Explanation': 'The citing paper extends the research on the use of adapters in low-resource settings by conducting a more nuanced analysis and comparing multiple adapter variants with FFT under the passive learning setup.'}, {'Category': 'Methodological Basis', 'Citation': '(Li and Liang, 2021)', 'Explanation': 'The cited work by Li and Liang (2021) is used to bolster the findings of the citing paper by exploring the stability of representations in scenarios with limited resources.'}], 'b27': [{'Category': 'Supporting Evidence', 'Citation': '(Mao et al., 2022)', 'Explanation': 'The cited work by Mao et al. (2022) further supports the research on the use of adapters in monolingual settings with scarce data.'}, {'Category': 'Methodological Basis', 'Citation': '(Mao et al., 2022)', 'Explanation': 'The cited work presents the UniPELT PEFT method, which the citing paper considers as a combination of multiple PEFT approaches, including LoRA, Prefix-tuning, and Adapter, in a single unified setup with gating mechanisms for effective activation.'}, {'Category': 'Methodological Basis', 'Citation': '(Mao et al., 2022)', 'Explanation': 'The cited work by Mao et al. contributes to the understanding of the use of adapters in low-resource settings in the citing paper.'}, {'Category': 'Extension or Continuation', 'Citation': '(Mao et al., 2022)', 'Explanation': 'The citing paper further extends the research on the use of adapters in low-resource settings by generating detailed learning curves to facilitate the comparison of multiple adapters with FFT in the passive learning setup.'}, {'Category': 'Methodological Basis', 'Citation': '(Mao et al., 2022)', 'Explanation': 'The cited work by Mao et al. (2022) contributes to the analysis of the stability of representations in the citing paper, providing insights into the use of adapters in scenarios with limited resources.'}], 'b20': [{'Category': 'Supporting Evidence', 'Citation': '(Kim et al., 2021)', 'Explanation': 'The cited work by Kim et al. (2021) provides evidence that the benefits of integrating TAPT with adapters tend to taper off as the amount of data increases, which is relevant to the discussion in the citing paper about the limitations of using adapters in low-resource setups.'}], 'b31': [{'Category': 'Data Source', 'Citation': '(Pang and Lee, 2004)', 'Explanation': 'The cited work by Pang and Lee serves as the data source for the SUBJ dataset used in the citing paper for the single-text classification task.'}], 'b39': [{'Category': 'Data Source', 'Citation': '(Socher et al., 2013)', 'Explanation': 'The cited work by Socher et al. is the data source for the SST dataset used in the single-text classification task in the citing paper.'}], None: [{'Category': 'Methodological Basis', 'Citation': '(Hu et al., 2022)', 'Explanation': 'The cited work introduces the LoRA PEFT technique, which the citing paper incorporates in the development of the UniPELT method by representing an additive method that incorporates trainable low-rank decomposition matrices in the layers of a pre-trained model.'}, {'Category': 'Methodological Basis', 'Citation': '(Lewis and Gale, 1994)', 'Explanation': 'The cited work by Lewis and Gale (1994) provides the maximum entropy (ENT) strategy for sampling instances in the field of uncertainty strategies, which the citing paper adopts as a method for instance selection.'}], 'b5': [{'Category': 'Methodological Basis', 'Citation': '(Devlin et al., 2019)', 'Explanation': 'The cited work by Devlin et al. (2019) provides the base PLM (BERT) that the citing paper uses as the foundation for their research on adapters.'}], 'b8': [{'Category': 'Methodological Basis', 'Citation': '(Gal and Ghahramani, 2016)', 'Explanation': 'The cited work by Gal and Ghahramani (2016) introduces the Monte Carlo dropout (MC) method for instance selection based on the stochasticity of forward passes with dropout layers, which the citing paper utilizes in the field of uncertainty strategies.'}], 'b40': [{'Category': 'Methodological Basis', 'Citation': '(Srivastava et al., 2014)', 'Explanation': 'The cited work by Srivastava et al. (2014) presents the use of dropout layers in forward passes, which the citing paper references in the context of the Monte Carlo dropout (MC) method for instance selection in the field of uncertainty strategies.'}], 'b36': [{'Category': 'Methodological Basis', 'Citation': '(Sener and Savarese, 2018)', 'Explanation': 'The cited work by Sener and Savarese (2018) introduces the core-set (CS) method for instance selection in the field of learning representations of the acquisition model, which the citing paper adopts as a method for encouraging instance diversity.'}], 'b41': [{'Category': 'Data Source', 'Citation': '(Stephenson et al., 2021)', 'Explanation': 'The data source cited by Stephenson et al. (2021) is used to draw inspiration for the layerwise examination of similarity in the citing paper, which is conducted to analyze the effect of PEFT and FFT on AL selection with respect to their layerwise similarity to the base model.'}], 'b1': [{'Category': 'Data Source', 'Citation': '(Baldock et al., 2021)', 'Explanation': 'The data source cited by Baldock et al. (2021) is used in the citing paper to support the claim that different layers of networks specialize in different features, with earlier layers acquiring more generalized knowledge and deeper layers focusing on task-specific information.'}]}\n"
     ]
    }
   ],
   "source": [
    "# Function to regroup citations by ref_id\n",
    "def regroup_citations_by_ref_id(citations):\n",
    "    grouped_citations = {}\n",
    "    for citation in citations:\n",
    "        if 'ref_id' in citation.keys():\n",
    "            ref_id = citation['ref_id']\n",
    "            # Create a copy of the citation without the ref_id\n",
    "            citation_copy = {k: v for k, v in citation.items() if k != 'ref_id'}\n",
    "            # Append the citation to the list associated with its ref_id\n",
    "            if ref_id in grouped_citations:\n",
    "                grouped_citations[ref_id].append(citation_copy)\n",
    "            else:\n",
    "                grouped_citations[ref_id] = [citation_copy]\n",
    "    return grouped_citations\n",
    "\n",
    "\n",
    "# Regroup the citationb list by ref_id\n",
    "grouped_citations = regroup_citations_by_ref_id(annotated_article[0]['citation_data'])\n",
    "print(grouped_citations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine all the steps together into one function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_citation_author_year(article):\n",
    "    for citation in article['citation_data']:\n",
    "        try:\n",
    "            parsed_name = split_and_parse_citation(citation['Citation'])\n",
    "            match = match_citations_with_references(parsed_name, article['references'])\n",
    "            citation['ref_id'] = match['ref_id'] if match else None\n",
    "        except:\n",
    "            citation['ref_id'] = None\n",
    "    return article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a grouped citation data for author-year citation style, let's start solving cases with numeric-style. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2.2 Handle Numeric Citation Style**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This style of citation seems simple at first, but there are many edge cases that we have to deal with. From my observation, there are 3 main types:\n",
    "\n",
    "- Singular citations such as [1] or [4]: These are processed conventionally, where the reference ID equals the citation number minus one.\n",
    "- Lists, for instance [1, 4, 6]: In this scenario, the citations are split into individual entries: [1], [4], and [6].\n",
    "- Ranges, like [1 - 5]: Here, the citation is divided into separate entries: [1], [2], [3], [4], [5].\n",
    "- Mixed ranges, such as [1] - [5]: These are split into distinct citations: [1] and [5]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_numeric_citations(citations):\n",
    "    # Helper function to parse ranges and individual numbers\n",
    "    def parse_part(part):\n",
    "        if '-' in part:  # Handle ranges\n",
    "            start, end = map(int, part.split('-'))\n",
    "            return list(range(start, end + 1))\n",
    "        else:  # Handle individual numbers\n",
    "            return [int(part)]\n",
    "\n",
    "    # Initialize the result list\n",
    "    result = []\n",
    "\n",
    "    # Find all parts of the input that match the patterns\n",
    "    parts = re.findall(r'\\[([^]]+)]', citations)\n",
    "    \n",
    "    for part in parts:\n",
    "        # For each part, remove spaces, split by commas and extend the result list\n",
    "        for subpart in part.replace(' ', '').split(','):\n",
    "            try:\n",
    "                result.extend(parse_part(subpart))\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    return [f\"[{num}]\" for num in result]\n",
    "\n",
    "# Function to apply citation splitting to a list of citation entries\n",
    "def split_citations_in_entries(citation_entries):\n",
    "    expanded_citation_entries = []\n",
    "    for entry in citation_entries:\n",
    "        try:\n",
    "        # Use the split_citations function to get a list of individual citations from the Citation field\n",
    "            split_citations_list = split_numeric_citations(entry['Citation'])\n",
    "            for citation in split_citations_list:\n",
    "                # Create a new citation entry for each split citation, keeping other fields the same\n",
    "                \n",
    "                new_entry = {\n",
    "                    'Citation': citation,\n",
    "                    'Category': entry['Category'],\n",
    "                    'Explanation': entry['Explanation']\n",
    "                }\n",
    "                expanded_citation_entries.append(new_entry)\n",
    "        except:\n",
    "            continue\n",
    "    return expanded_citation_entries\n",
    "\n",
    "\n",
    "def match_numeric_citation(citations):\n",
    "    for citation in citations:\n",
    "        # Regular expression to find single numbers inside square brackets\n",
    "        pattern = re.compile(r'\\[\\(?(?P<number>\\d+)\\)?\\]')\n",
    "        try:\n",
    "            #Find all matches in the text and convert them to integers\n",
    "            reference_num = [int(match.group('number')) for match in pattern.finditer(citation['Citation'])][0]\n",
    "            citation['ref_id'] = f\"b{reference_num -1}\"\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "    return citations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Category': 'Methodological Basis',\n",
       "  'Citation': '[29,72]',\n",
       "  'Explanation': 'The cited works provide a method of using the features learned by predicting different auxiliary maps to assist in predicting saliency maps, which the citing paper adopts in its research on saliency object detection.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '[52]',\n",
       "  'Explanation': 'The cited work uses auxiliary maps as input to guide the training process, which the citing paper adopts in its research on saliency object detection.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '[46,14]',\n",
       "  'Explanation': 'The cited works introduce a boundary-aware loss to make the models pay more attention to edge pixels, which the citing paper adopts in its research on saliency object detection.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '[58,60]',\n",
       "  'Explanation': 'The cited works provide a method of using multiple heads in a single encoder to learn different semantic information, which the citing paper adopts as a basis for their research on saliency map prediction.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '[74]',\n",
       "  'Explanation': 'The cited work highlights the inefficiency of using multiple branches in parallel for saliency map prediction, which the citing paper uses to guide their method design for efficient saliency map prediction.'}]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated_article[108]['citation_data'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_article[108]['citation_data'] = split_citations_in_entries(annotated_article[108]['citation_data'])\n",
    "annotated_article[108]['citation_data'] =  match_numeric_citation(annotated_article[108]['citation_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Citation': '[29]',\n",
       "  'Category': 'Methodological Basis',\n",
       "  'Explanation': 'The cited works provide a method of using the features learned by predicting different auxiliary maps to assist in predicting saliency maps, which the citing paper adopts in its research on saliency object detection.',\n",
       "  'ref_id': 'b28'},\n",
       " {'Citation': '[72]',\n",
       "  'Category': 'Methodological Basis',\n",
       "  'Explanation': 'The cited works provide a method of using the features learned by predicting different auxiliary maps to assist in predicting saliency maps, which the citing paper adopts in its research on saliency object detection.',\n",
       "  'ref_id': 'b71'},\n",
       " {'Citation': '[52]',\n",
       "  'Category': 'Methodological Basis',\n",
       "  'Explanation': 'The cited work uses auxiliary maps as input to guide the training process, which the citing paper adopts in its research on saliency object detection.',\n",
       "  'ref_id': 'b51'},\n",
       " {'Citation': '[46]',\n",
       "  'Category': 'Methodological Basis',\n",
       "  'Explanation': 'The cited works introduce a boundary-aware loss to make the models pay more attention to edge pixels, which the citing paper adopts in its research on saliency object detection.',\n",
       "  'ref_id': 'b45'},\n",
       " {'Citation': '[14]',\n",
       "  'Category': 'Methodological Basis',\n",
       "  'Explanation': 'The cited works introduce a boundary-aware loss to make the models pay more attention to edge pixels, which the citing paper adopts in its research on saliency object detection.',\n",
       "  'ref_id': 'b13'},\n",
       " {'Citation': '[58]',\n",
       "  'Category': 'Methodological Basis',\n",
       "  'Explanation': 'The cited works provide a method of using multiple heads in a single encoder to learn different semantic information, which the citing paper adopts as a basis for their research on saliency map prediction.',\n",
       "  'ref_id': 'b57'}]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated_article[108]['citation_data'][:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, single citation like [14], [58] will be parsed normaly. But for citation like [29, 72], they will get split to 2 separated citations [29] and [72].\n",
    "\n",
    "Now, let's combine the steps together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proprocess_citation_numeric(article):\n",
    "    \n",
    "    article['citation_data'] = split_citations_in_entries(article['citation_data'])\n",
    "    article['citation_data'] = match_numeric_citation(article['citation_data'])\n",
    "    return article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2.3 Process 2 citation style**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to detect the citation style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_citation_style(text):\n",
    "    # Pattern to match numeric citations like [1], [1, 2], [1-6], [1, 2-6], [1, 2, 3-6], etc.\n",
    "    numeric_pattern = re.compile(r'\\[\\d+(-\\d+)?(,\\s*\\d+(-\\d+)?)*\\]')\n",
    "    # Pattern for \"Author-Year\" citations like (Author, Year)\n",
    "    author_year_pattern = re.compile(r'\\([A-Za-z]+,\\s*\\d{4}\\)')\n",
    "\n",
    "    # Check for numeric citation style\n",
    "    if numeric_pattern.search(text):\n",
    "        return \"Numeric\"\n",
    "    # Check for author-year citation style\n",
    "    elif author_year_pattern.search(text):\n",
    "        return \"Author-Year\"\n",
    "    else:\n",
    "        return \"Author-Year\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Author-Year'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect_citation_style(\"(Amin et al., 2019)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Numeric'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect_citation_style(\"[1,6]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–         | 157/7243 [00:00<00:27, 255.67it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7243/7243 [00:11<00:00, 643.55it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for article in tqdm(annotated_article, total=len(annotated_article)):\n",
    "    references = article['references']\n",
    "    if len(article['citation_data']) == 0:\n",
    "        continue\n",
    "    citation_style = detect_citation_style(article['citation_data'][0][\"Citation\"])\n",
    "    # try:\n",
    "    if citation_style == \"Author-Year\":\n",
    "        article = preprocess_citation_author_year(article)\n",
    "    elif citation_style == \"Numeric\":\n",
    "        article = proprocess_citation_numeric(article)\n",
    "    else:\n",
    "        print(f\"Uncertain citation style: {citation_style}\")\n",
    "        continue\n",
    "    # except Exception as e:\n",
    "    #     print(article['citation_data'])\n",
    "    #     print(e)\n",
    "    #     break\n",
    "\n",
    "    grouped_citations = regroup_citations_by_ref_id(article['citation_data'])\n",
    "    article['grouped_citations'] = grouped_citations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[4]'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article['citation_data'][0]['Citation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b14': [{'Category': 'Methodological Basis',\n",
       "   'Citation': '(Liu et al., 2020)',\n",
       "   'Explanation': 'The cited work by Liu et al. introduces a fusion forget gate to control the flow of information between multimodal sequences, which the citing paper adopts as a method to address the problem of redundancy and noise in video multimodal fusion.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Liu et al., 2020)',\n",
       "   'Explanation': 'The cited work proposed a multistage fusion network with a fusion forget gate module for controlling redundant information in multimodal long sequences, which the citing paper adopts in their video multimodal summarization research.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Liu et al., 2018b)',\n",
       "   'Explanation': 'The cited work introduces the LMF model, which the citing paper references for text generation tasks.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Liu et al., 2018b)',\n",
       "   'Explanation': 'The cited work by Liu et al. provides a method for multimodal sentiment analysis that is used in the comparison in the citing paper.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Liu et al., 2020)',\n",
       "   'Explanation': 'The cited work by Liu et al. provides a method for multimodal summarization that is used in the comparison in the citing paper.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Liu et al., 2018b)',\n",
       "   'Explanation': 'The cited work introduces the LMF model as an advanced version of the TFN model, which the citing paper may have used in their research for multimodal sentiment analysis.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Liu et al., 2020)',\n",
       "   'Explanation': 'The cited work presents the multistage fusion network with the fusion forget gate module, which the citing paper adopts to control the flow of redundant information between multimodal long sequences.'}],\n",
       " 'b16': [{'Category': 'Methodological Basis',\n",
       "   'Citation': '(Nagrani et al., 2021)',\n",
       "   'Explanation': 'The cited work by Nagrani et al. (2021) introduces the concept of a bottleneck module, which the citing paper adopts to restrict redundant and noisy information in video multimodal fusion.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Nagrani et al., 2021)',\n",
       "   'Explanation': 'The cited work by Nagrani et al. provides a foundational understanding of the redundant and noisy nature of audio and visual inputs in video, which the citing paper leverages in their research to focus on removing noise and preserving critical information.'}],\n",
       " 'b4': [{'Category': 'Data Source',\n",
       "   'Citation': '(Gutmann and HyvÃ¤rinen, 2010)',\n",
       "   'Explanation': 'The cited work by Gutmann and HyvÃ¤rinen (2010) provides the noise-contrastive estimation framework, which the citing paper utilizes to maximize mutual information between fusion results and unimodal inputs.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Gutmann and HyvÃ¤rinen, 2010)',\n",
       "   'Explanation': 'The cited work introduces the noise-contrastive estimation framework, which the citing paper adopts to produce the InfoNCE loss for measuring the lower bound of mutual information in the fusion process.'}],\n",
       " 'b36': [{'Category': 'Methodological Basis',\n",
       "   'Citation': '(Zadeh et al., 2016)',\n",
       "   'Explanation': 'The cited work, MOSI, is a dataset used in the experiments conducted in the citing paper to evaluate the performance of the proposed model in the field of multimodal sentiment analysis.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Zadeh et al., 2018b)',\n",
       "   'Explanation': 'The cited work, MOSEI, is another dataset used in the experiments of the citing paper to further assess the performance of the model in the same field of multimodal sentiment analysis.'},\n",
       "  {'Category': 'Data Source',\n",
       "   'Citation': '(Li et al., 2017)',\n",
       "   'Explanation': 'The cited work introduced a multimodal summarization dataset in Chinese and English, which the citing paper utilizes in their research on video multimodal summarization.'},\n",
       "  {'Category': 'Supporting Evidence',\n",
       "   'Citation': '(Zadeh et al., 2016)',\n",
       "   'Explanation': 'The cited work provides the MOSI dataset, which is used in the citing paper for video multimodal sentiment analysis.'},\n",
       "  {'Category': 'Supporting Evidence',\n",
       "   'Citation': '(Zadeh et al., 2018b)',\n",
       "   'Explanation': 'The cited work provides the MOSEI dataset, which is used in the citing paper for video multimodal sentiment analysis.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Zadeh et al., 2017)',\n",
       "   'Explanation': 'The cited work presents the TFN model, which the citing paper uses as a method for text generation.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Zadeh et al., 2017)',\n",
       "   'Explanation': 'The cited work by Zadeh et al. also serves as a methodological basis for the comparison in the citing paper, as it provides a method for multimodal sentiment analysis that is used to compare against the performance of DBF.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Zadeh et al., 2017)',\n",
       "   'Explanation': 'The cited work presents the TFN model based on tensor outer product to capture multiple-modal interactions, which the citing paper uses in their research for multimodal sentiment analysis.'}],\n",
       " 'b22': [{'Category': 'Data Source',\n",
       "   'Citation': '(Sanabria et al., 2018)',\n",
       "   'Explanation': \"The cited work, How2, is a benchmark dataset used in the experiments of the citing paper to evaluate the model's performance in the field of multimodal summarization.\"},\n",
       "  {'Category': 'Data Source',\n",
       "   'Citation': '(Sanabria et al., 2018)',\n",
       "   'Explanation': 'The cited work proposed the How2 dataset of short instructional videos with summaries, which the citing paper uses in their study of video multimodal summarization.'},\n",
       "  {'Category': 'Data Source',\n",
       "   'Citation': '(Sanabria et al., 2018)',\n",
       "   'Explanation': 'The cited work provides the How2 dataset, which is a large-scale collection of instructional videos and their corresponding transcripts, that the citing paper uses as a benchmark for the summary task.'}],\n",
       " None: [{'Category': 'Methodological Basis',\n",
       "   'Explanation': 'The cited work in the last sentence of the response is not provided in the format of a citation number. However, it is worth noting that the cited work is likely to be a methodological basis for the model proposed in the citing paper, as it is used to demonstrate the effectiveness of the model in reducing noise and retaining key information in video multimodal fusion.'}],\n",
       " 'b28': [{'Category': 'Methodological Basis',\n",
       "   'Citation': '(Vaswani et al., 2017)',\n",
       "   'Explanation': 'The cited work by Vaswani et al. (2017) introduced the concept of Transformer, which has influenced the design of more recent models in the field of video multimodal fusion.'}],\n",
       " 'b37': [{'Category': 'Extension or Continuation',\n",
       "   'Citation': '(Zhang et al., 2019)',\n",
       "   'Explanation': 'The cited work by Zhang et al. (2019) built upon the use of cross attention mechanism in the decoder of Transformer to perform multimodal translation tasks, which is further extended in the field of video multimodal fusion.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Han et al., 2021)',\n",
       "   'Explanation': 'The cited work builds a hierarchical mutual information maximization guided model that the citing paper uses to improve the fusion outcome and performance in the downstream multimodal sentiment analysis task.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Han et al., 2021)',\n",
       "   'Explanation': 'The cited work introduces the concept of mutual information in information theory, which the citing paper utilizes to estimate the relationship between pairs of variables and capture modality-invariant cues in fusion results.'},\n",
       "  {'Category': 'Data Source',\n",
       "   'Citation': 'Facet1',\n",
       "   'Explanation': 'The Facet1 model is cited as the source of the facial expression features used in the citing paper for sentiment analysis.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Han et al., 2021)',\n",
       "   'Explanation': 'The cited work by Han et al. provides a method for multimodal sentiment analysis that is used in the comparison in the citing paper.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Han et al., 2021)',\n",
       "   'Explanation': 'The cited work presents the MMIM model that hierarchically maximizes the mutual information in unimodal input pairs and between multimodal fusion result and unimodal input, which the citing paper may have used in their research for multimodal sentiment analysis.'}],\n",
       " 'b30': [{'Category': 'Extension or Continuation',\n",
       "   'Citation': '(Wu et al., 2021)',\n",
       "   'Explanation': 'The cited work by Wu et al. (2021) proposed a text-centric multimodal fusion shared private framework for multimodal fusion, which builds upon the use of crossmodal prediction and sentiment regression parts in the field of video multimodal fusion.'}],\n",
       " 'b25': [{'Category': 'Data Source',\n",
       "   'Citation': '(Sun et al., 2019)',\n",
       "   'Explanation': 'The cited work by Sun et al. (2019) introduced the use of three pre-training tasks in video-language pre-training, which is a data source for the field of video multimodal fusion.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Sun et al., 2020)',\n",
       "   'Explanation': 'The cited work introduces the ICCN model, which the citing paper uses in their research for text generation.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Sun et al., 2020)',\n",
       "   'Explanation': 'The cited work by Sun et al. provides a method for multimodal sentiment analysis that is used in the comparison in the citing paper.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Sun et al., 2020)',\n",
       "   'Explanation': 'The cited work introduces the ICCN model with an adversarial encoder-decoder classifier framework to learn a modality-invariant embedding space, which the citing paper may have used in their research for multimodal sentiment analysis.'}],\n",
       " 'b32': [{'Category': 'Methodological Basis',\n",
       "   'Citation': '(Yu et al., 2021a)',\n",
       "   'Explanation': 'The cited work first introduced pre-trained language models in multimodal summarization and experimented with the optimal injection layer of visual features, which the citing paper builds upon in their video multimodal summarization research.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Yu et al., 2021b)',\n",
       "   'Explanation': 'The cited work introduces a multi-label training scheme that the citing paper adopts to generate extra unimodal labels and train the model concurrently with the main task.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Yu et al., 2021a)',\n",
       "   'Explanation': 'The cited work provides a set of evaluation metrics for summarization, which the citing paper adopts to assess the performance of the generated abstractive summaries in the How2 dataset.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Yu et al., 2021b)',\n",
       "   'Explanation': 'The cited work by Yu et al. provides a method for multimodal sentiment analysis that is used in the comparison in the citing paper.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Yu et al., 2021a)',\n",
       "   'Explanation': 'The cited work by Yu et al. provides a method for multimodal summarization that is used in the comparison in the citing paper.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Yu et al., 2021b)',\n",
       "   'Explanation': 'The cited work introduces the Self-MM model with a label generation module based on self-supervised learning strategy to acquire independent unimodal supervision, which the citing paper may have used in their research for multimodal sentiment analysis.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Yu et al., 2021a)',\n",
       "   'Explanation': 'The cited work introduces the BART-based and vision guided model for multimodal summarization task, which the citing paper uses as a reference for incorporating visual information in the model.'}],\n",
       " 'b15': [{'Category': 'Methodological Basis',\n",
       "   'Citation': '(Luo et al., 2021)',\n",
       "   'Explanation': 'The cited work proposes a multiscale fusion method that the citing paper uses to align different granularity information from multiple modalities in multimodal sentiment analysis.'}],\n",
       " 'b17': [{'Category': 'Methodological Basis',\n",
       "   'Citation': '(Oord et al., 2018)',\n",
       "   'Explanation': 'The cited work presents the InfoNCE loss as a method for measuring the lower bound of mutual information in the fusion process, which the citing paper incorporates into the noise-contrastive estimation framework to produce the final loss function.'}],\n",
       " 'b7': [{'Category': 'Supporting Evidence',\n",
       "   'Citation': '(Hazarika et al., 2020)',\n",
       "   'Explanation': 'The cited work by Hazarika et al. provides the metric set used to evaluate sentiment intensity predictions in the citing paper, serving as a foundational element for the research conducted.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Hazarika et al., 2020)',\n",
       "   'Explanation': 'The cited work presents the MISA model, which the citing paper references for text generation tasks.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Hazarika et al., 2020)',\n",
       "   'Explanation': 'The cited work by Hazarika et al. provides a method for multimodal sentiment analysis that is used in the comparison in the citing paper.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Hazarika et al., 2020)',\n",
       "   'Explanation': 'The cited work presents the MISA model that projects each modality to two distinct subspaces, which the citing paper may have used in their research for multimodal sentiment analysis.'}],\n",
       " 'b3': [{'Category': 'Data Source',\n",
       "   'Citation': '(Devlin et al., 2018)',\n",
       "   'Explanation': 'The cited work by Devlin et al. (2018) is the source of the BERT-base model used in the citing paper for text input encoding and [CLS] embedding extraction.'}],\n",
       " 'b1': [{'Category': 'Data Source',\n",
       "   'Citation': '(Degottex et al., 2014)',\n",
       "   'Explanation': 'The cited work by Degottex et al. (2014) is the source of the COVAREP model used in the citing paper to extract audio features for sentiment analysis.'},\n",
       "  {'Category': 'Data Source',\n",
       "   'Citation': '(Degottex et al., 2014)',\n",
       "   'Explanation': 'The cited work by Degottex et al. (2014) is the source of the COVAREP model used in the citing paper to extract audio features for sentiment analysis.'}],\n",
       " 'b27': [{'Category': 'Methodological Basis',\n",
       "   'Citation': '(Tsai et al., 2019)',\n",
       "   'Explanation': 'The cited work introduces the MulT model, which the citing paper adopts in their research for text generation tasks.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Tsai et al., 2018)',\n",
       "   'Explanation': 'The cited work presents the MFM model, which the citing paper adopts in their research for text generation tasks.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Tsai et al., 2019)',\n",
       "   'Explanation': 'The cited work by Tsai et al. serves as a methodological basis for the comparison in the citing paper, as it provides a method for multimodal sentiment analysis that is used to compare against the performance of DBF.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Tsai et al., 2018)',\n",
       "   'Explanation': 'The cited work by Tsai et al. provides a method for multimodal sentiment analysis that is used in the comparison in the citing paper.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Tsai et al., 2019)',\n",
       "   'Explanation': 'The cited work introduces the MulT model with directional pairwise cross-attention, which the citing paper adopts in their research for multimodal sentiment analysis.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Tsai et al., 2018)',\n",
       "   'Explanation': 'The cited work presents the MFM model that factorizes representations into two sets of independent factors, which the citing paper may have used in their research for multimodal sentiment analysis.'}],\n",
       " 'b9': [{'Category': 'Data Source',\n",
       "   'Citation': '(Lewis et al., 2019)',\n",
       "   'Explanation': 'The cited work introduces the BART model, which the citing paper uses as a feature extractor in their research for text generation tasks.'}],\n",
       " 'b8': [{'Category': 'Data Source',\n",
       "   'Citation': '(Kay et al., 2017)',\n",
       "   'Explanation': 'The cited work provides the Kinetics dataset, which is used as a pre-training data source for the vision model in the citing paper.'}],\n",
       " 'b20': [{'Category': 'Methodological Basis',\n",
       "   'Citation': '(Paszke et al., 2017)',\n",
       "   'Explanation': 'The cited work introduces the deep learning framework PyTorch, which the citing paper uses to implement the code for sentiment analysis and summarization experiments.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Paszke et al., 2017)',\n",
       "   'Explanation': 'The cited work also provides the deep learning framework PyTorch, which the citing paper uses to implement the code for sentiment analysis and summarization experiments.'}],\n",
       " 'b18': [{'Category': 'Methodological Basis',\n",
       "   'Citation': '(Palaskar et al., 2019)',\n",
       "   'Explanation': 'The cited work by Palaskar et al. provides a method for multimodal summarization that is used in the comparison in the citing paper.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Palaskar et al., 2019)',\n",
       "   'Explanation': 'The cited work introduces the sequence-to-sequence multimodal fusion model with hierarchical attention, which serves as the basis for the DBF model in the citing paper.'}]}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated_article[17]['grouped_citations']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.3 Building citation graph**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.3.1 Parsing annotated triplets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationships_dict = {\n",
    "    \"Supporting Evidence\": \"Is Evidence For\",\n",
    "    \"Methodological Basis\": \"Is Methodological Basis For\",\n",
    "    \"Theoretical Foundation\": \"Is Theoretical Foundation For\", \n",
    "    \"Data Source\": \"Is Data Source For\",\n",
    "    \"Extension or Continuation\": \"Is Extension or Continuation Of\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have grouped citation data; now we need to find the papers cited in the arXiv dataset by name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7243 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7243/7243 [1:54:35<00:00,  1.05it/s]  \n"
     ]
    }
   ],
   "source": [
    "df_data['title'] = df_data['title'].str.lower()\n",
    "titles = df_data['title'].tolist()\n",
    "\n",
    "\n",
    "def search_paper_by_name(name):\n",
    "    # matches = df_data['title'].str.contains(name, case=False, na=False, regex=False)\n",
    "    # filtered_df = df_data[matches]\n",
    "    # if len(filtered_df) == 0:\n",
    "    #     return None\n",
    "    # return filtered_df.iloc[0]['id']\n",
    "    titles = df_data['title'].tolist()\n",
    "    for idx, title in enumerate(titles):\n",
    "        if name in title:\n",
    "            return df_data.iloc[idx]['id']\n",
    "    return None\n",
    "\n",
    "for article_dict in tqdm(annotated_article, total=len(annotated_article)):\n",
    "\n",
    "    article_dict[\"arxiv_id\"] = search_paper_by_name(article_dict['title'].lower())\n",
    "\n",
    "    if \"grouped_citations\" in article_dict.keys():\n",
    "        article_dict[\"mapped_citation\"] = {}\n",
    "        for key,val in article_dict['grouped_citations'].items():\n",
    "            for ref in article_dict[\"references\"]:\n",
    "                if ref[\"ref_id\"] == key:\n",
    "                    title = ref[\"title\"]\n",
    "\n",
    "            title = title.lower()\n",
    "            arxiv_id = search_paper_by_name(title)\n",
    "            article_dict['mapped_citation'][key] = {\"title\": title, 'arxiv_id': arxiv_id, 'citation': val}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../outputs/annotated_articles.json\", \"w\") as f:\n",
    "    json.dump(annotated_article, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b3': {'title': 'an image is worth 16x16 words: transformers for image recognition at scale',\n",
       "  'arxiv_id': '2010.11929',\n",
       "  'citation': [{'Citation': '[4]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': 'The cited works introduce the concept of using image transformers as a backbone for video descriptor extraction, which the citing paper adopts for its video copy detection model.'}]},\n",
       " 'b6': {'title': 'swin transformer v2: scaling up capacity and resolution',\n",
       "  'arxiv_id': '2111.09883',\n",
       "  'citation': [{'Citation': '[7]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': 'The cited works introduce the concept of using image transformers as a backbone for video descriptor extraction, which the citing paper adopts for its video copy detection model.'}]},\n",
       " 'b0': {'title': 'a simple framework for contrastive learning of visual representations',\n",
       "  'arxiv_id': '2002.05709',\n",
       "  'citation': [{'Citation': '[1]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': \"The cited work, SimCLR, provides the method of combining with entropy loss for self-supervised learning, which is utilized in the training process of the citing paper's basic model.\"}]},\n",
       " 'b7': {'title': 'a self-supervised descriptor for image copy detection',\n",
       "  'arxiv_id': '2202.10261',\n",
       "  'citation': [{'Citation': '[8]',\n",
       "    'Category': 'Extension or Continuation',\n",
       "    'Explanation': 'The citing paper builds upon the SSCD method to train its basic model in a self-supervised manner, indicating an extension of the research conducted in the cited work.'}]},\n",
       " 'b9': {'title': 'spreading vectors for similarity search',\n",
       "  'arxiv_id': '1806.03198',\n",
       "  'citation': [{'Citation': '[10]',\n",
       "    'Category': 'Supporting Evidence',\n",
       "    'Explanation': \"The cited work proposes the entropy loss used in the training process of the citing paper's basic model, providing a foundational element for the loss function formulation.\"}]},\n",
       " 'b8': {'title': 'learning transferable visual models from natural language supervision',\n",
       "  'arxiv_id': '2103.00020',\n",
       "  'citation': [{'Citation': '[9]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': 'The citing paper adopts the CLIP model to extract frame features without post-processing, which serves as the foundation for the edited video detection method proposed in the study.'}]},\n",
       " 'b5': {'title': 'roberta: a robustly optimized bert pretraining approach',\n",
       "  'arxiv_id': '1907.11692',\n",
       "  'citation': [{'Citation': '[6]',\n",
       "    'Category': 'Theoretical Foundation',\n",
       "    'Explanation': 'The citing paper leverages the RoBERTa model to process frame features extracted by CLIP, providing a theoretical framework for the binary classification approach used in identifying edited videos.'}]}}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_dict['mapped_citation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a class for a paper node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperNode:\n",
    "    title: str\n",
    "    arxiv_id: str\n",
    "    \n",
    "    def __init__(self, title, arxiv_id):\n",
    "        self.title = title\n",
    "        self.arxiv_id = arxiv_id\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"Title: {self.title},\\n Arxiv ID: {self.arxiv_id}\"\n",
    "\n",
    "class PaperEdge:\n",
    "    category: str\n",
    "    explanation: str\n",
    "    verbose = True\n",
    "\n",
    "    def __init__(self, category, explanation):\n",
    "        self.category = category\n",
    "        self.explanation = explanation\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        if self.verbose:\n",
    "            return f\"Category: {self.category},\\n Explanation: {self.explanation}\"\n",
    "        else:\n",
    "            return f\"Category: {self.category}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7243/7243 [00:00<00:00, 23971.08it/s]\n"
     ]
    }
   ],
   "source": [
    "paper_dict = {}\n",
    "\n",
    "for article_dict in tqdm(annotated_article, total=len(annotated_article)):\n",
    "    paper_dict[article_dict['title'].lower()] = PaperNode(title=article_dict['title'], arxiv_id=article_dict['arxiv_id'])\n",
    "\n",
    "    if \"mapped_citation\" in article_dict.keys():\n",
    "        for key,val in article_dict['mapped_citation'].items():\n",
    "            title = val['title']\n",
    "            if title not in paper_dict.keys():\n",
    "                paper_node = PaperNode(title=val['title'], arxiv_id=val['arxiv_id'])\n",
    "                paper_dict[title] = paper_node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b3': {'title': 'an image is worth 16x16 words: transformers for image recognition at scale',\n",
       "  'arxiv_id': '2010.11929',\n",
       "  'citation': [{'Citation': '[4]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': 'The cited works introduce the concept of using image transformers as a backbone for video descriptor extraction, which the citing paper adopts for its video copy detection model.'}]},\n",
       " 'b6': {'title': 'swin transformer v2: scaling up capacity and resolution',\n",
       "  'arxiv_id': '2111.09883',\n",
       "  'citation': [{'Citation': '[7]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': 'The cited works introduce the concept of using image transformers as a backbone for video descriptor extraction, which the citing paper adopts for its video copy detection model.'}]},\n",
       " 'b0': {'title': 'a simple framework for contrastive learning of visual representations',\n",
       "  'arxiv_id': '2002.05709',\n",
       "  'citation': [{'Citation': '[1]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': \"The cited work, SimCLR, provides the method of combining with entropy loss for self-supervised learning, which is utilized in the training process of the citing paper's basic model.\"}]},\n",
       " 'b7': {'title': 'a self-supervised descriptor for image copy detection',\n",
       "  'arxiv_id': '2202.10261',\n",
       "  'citation': [{'Citation': '[8]',\n",
       "    'Category': 'Extension or Continuation',\n",
       "    'Explanation': 'The citing paper builds upon the SSCD method to train its basic model in a self-supervised manner, indicating an extension of the research conducted in the cited work.'}]},\n",
       " 'b9': {'title': 'spreading vectors for similarity search',\n",
       "  'arxiv_id': '1806.03198',\n",
       "  'citation': [{'Citation': '[10]',\n",
       "    'Category': 'Supporting Evidence',\n",
       "    'Explanation': \"The cited work proposes the entropy loss used in the training process of the citing paper's basic model, providing a foundational element for the loss function formulation.\"}]},\n",
       " 'b8': {'title': 'learning transferable visual models from natural language supervision',\n",
       "  'arxiv_id': '2103.00020',\n",
       "  'citation': [{'Citation': '[9]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': 'The citing paper adopts the CLIP model to extract frame features without post-processing, which serves as the foundation for the edited video detection method proposed in the study.'}]},\n",
       " 'b5': {'title': 'roberta: a robustly optimized bert pretraining approach',\n",
       "  'arxiv_id': '1907.11692',\n",
       "  'citation': [{'Citation': '[6]',\n",
       "    'Category': 'Theoretical Foundation',\n",
       "    'Explanation': 'The citing paper leverages the RoBERTa model to process frame features extracted by CLIP, providing a theoretical framework for the binary classification approach used in identifying edited videos.'}]}}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated_article[-1]['mapped_citation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112611"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paper_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.PaperNode at 0x70bff98d5900>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_dict[\"make-a-video: text-to-video generation without text-video data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Category': 'Methodological Basis', 'Citation': 'The citing paper adopts the data and results from the cited works by Shridhar et al. and Yang et al. to demonstrate the on-par success rates of AutoPlan with human-written demonstrations.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The proposed LPS metric is used in the study conducted in the citing paper to further split test nodes into different sensitive groups based on the LPS values.'}\n",
      "{'Category': 'Data Source', 'Citation': 'The cited work is used to acknowledge the origin of a dataset or specific information that the citing paper utilizes in their research or analysis.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is extended in the citing paper by choosing the optimal number of prompt features and keeping the neural network architecture and hyperparameters the same as in PLOT.'}\n",
      "{'Category': 'Methodological Basis', 'Citation': 'The cited work contributes a method for choosing the featurizer in Figure ( 5), which the citing paper uses in their experiment.'}\n",
      "{'Category': 'Methodological Basis', 'Citation': 'The cited work provides a method for using a 3-layer MLP architecture for Ïˆ r in equation 10, which the citing paper adopts in their research.'}\n",
      "{'Category': 'Supporting Evidence', 'Citation': '(Wen et al., '}\n",
      "{'Category': 'Data Source', 'Citation': 'The original data and the back-translated data are combined using indicator tags to train the new En-Indic model from scratch. The cited work is acknowledged for providing the data used in the study.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'In summary, 1. We use entropy-regularized r l to obtain a reward for which a behavioral-cloned policy is optimal, which we use to improve the policy and overcome the covariate shift problem. 2. We introduce approximate stationary stochastic process policies to implement this approach for continuous control by constructing specialized neural network architectures. 3. We show strong performance on online and offline imitation for high-dimensional and image-based continuous control tasks compared to current state-of-the-art methods.'}\n",
      "{'Category': 'Data Source', 'Citation': '(Chakraborty Mainstream solutions are typically a detached two-stage framework. Specifically, they first rank the importance scores of all tokens according to the original input and then orderly substitute these tokens via heuristic rules.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The citing paper extends the research by introducing a new self-attention module that comprises two self-attention layers to inject object location information in the text-to-layout component.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The citing paper extends the idea of training an agent to explore training domains using a maximum entropy objective by exploring the possibility of improving exploration at test time.'}\n",
      "{'Category': 'Data Source', 'Citation': 'The cited work provides the data source for the training of the agent to explore training domains using a maximum entropy objective, which the citing paper utilizes in their approach to improve exploration at test time.'}\n",
      "{'Category': 'Data Source', 'Citation': 'The number of communicated floats is calculated by all clients in the experiments conducted in the citing paper.'}\n",
      "{'Category': 'Methodological Basis', 'Citation': 'The exact version of the parameters outlined in Corollary 5.5 is employed in the experiments evaluating the Multisampling strategy, providing a methodological basis for the study.'}\n",
      "{'Category': 'Supporting Evidence', 'Citation': 'The study uses BERT-base as the pretrained language model, which is cited to support the use of this model in the study as a foundational element for the research conducted.'}\n",
      "{'Category': 'Hyper parameters', 'Citation': 'The study provides the values of the learning rate, dropout rate, and mixing ratio in the dynamic STE and loss function, which are cited to highlight the specific parameters used in the study and their impact on the results.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The citing paper extends the research on readability evaluation in English by developing a new benchmark dataset in Turkish, which is a low-resource language with limited access to digital text resources.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The citing paper further builds upon the research of the cited works by developing a new approach for re-ID performance improvement, which is highlighted with bold fonts.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The performance of the cited works is highlighted in the citing paper, with the performance of the approach developed in the citing paper being presented in bold fonts.'}\n",
      "{'Category': 'Data Source', 'Citation': 'The cited works are acknowledged for the memory bank methods used in the citing paper, with the performance of the full approach being presented as a new state-of-the-art performance on certain benchmarks.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not specified in the given text.'}\n",
      "{'Category': 'Data Source', 'Citation': 'The cited work is not specified in the context provided, but the citing paper likely utilizes the data or pre-existing models from the work in their research or analysis.'}\n",
      "{'Category': 'Data Source', 'Citation': 'The cited work is not provided in the context, but the text mentions the need for vast amounts of external labeled data from different vision-language tasks for training the VLP methods. This highlights the reliance on external data as a foundational element for the study conducted in the citing paper.'}\n",
      "{'Category': 'Data Source', 'Citation': 'The cited work by Hong et al. and Jiang et al. provide a method of augmenting rendering samples around the face to improve facial details, which the citing paper adopts to improve the performance of personalized avatar generation.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work by Hong et al. and Jiang et al. provide a method of augmenting rendering samples around the face to improve facial details, which the citing paper further extends to improve the performance of personalized avatar generation.'}\n",
      "{'Category': 'Data Source', 'Citation': 'The linear dataset and the XOR dataset are used in the experiments to provide a controlled setting for the data model in the citing paper.'}\n",
      "{'Category': 'Data Source', \"* Contact Author ' 'Explanation\": 'The cited work by Chakraborty and Meher (2013) is mentioned as a method of extracting target features through neural networks for matching in different frames of sports videos. The citing paper acknowledges the popularity of this approach in the field.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The citing paper extends the research on object detection and tracking by proposing two rounds of object detection and tracking with the addition of context information. The first round uses a tracker to capture continuous trajectories of objects, while the second round extends adjacent trajectory segments to each other through predicting likely regions and redetecting them using a more fine-grained method.'}\n",
      "{'Category': 'Data Source', 'Citation': 'The dataset with labeled badminton ball images and sequences from high-ranking matches is built to train in the citing paper, which serves as a data source for the training process.'}\n",
      "{'Category': 'Methodological Basis', 'Citation': 'The proposed Vit3d tracking method in the citing paper classifies the badminton ball by tracking the information of trajectory and appearance, which is a methodological basis for the ball detection and tracking system based on context multi-feature fusion.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The current work is an extension of previous research on AADS in French, building upon the limitations identified in the cited works to create an unified framework for designing and evaluating AADS models in French.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The use of masks produced by the CNN branch to guide the patch grouping process in the Patch-Grouping Wavelet Transformer (GPWFormer) network is an extension of the work by Yao et al. in the cited work.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The exploitation of the congruence constraint between the two branches in the GPWFormer network is a continuation of the work by Yao et al. in the cited work to maintain spatial consistency among the patches.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'Our results provide insights into the relative performance of various methods in the field of Federated DG, which extends the research on domain separation in the field.'}\n",
      "{'Category': 'Data Source', 'Citation': 'Our results are based on various datasets and heterogeneity settings, which serve as a data source for the study of domain separation in Federated DG.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The training of the structure encoder on the same training set with the pre-trained base model frozen is an extension of the base model training to equip the model with depth guidance.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The finetuning of the depth-guided text-to-video model along with specific textural tokens optimization is a continuation of the base model training to customize the model for specific tasks.'}\n",
      "{'Category': 'Data Source', 'Citation': 'The cited work in the data source category is not provided in the given text.'}\n",
      "{'Category': 'Methodological Basis', 'Citation': 'The cited work is the ACL anthology, which is used as a data source for the full-text search conducted in the citing paper to collect relevant papers.'}\n",
      "{'Category': 'Methodological Basis', 'Citation': 'The cited work is Papers With Code, which is used as a data source for the curated list of datasets used in machine learning research and referenced in the citing paper.'}\n",
      "{'Category': 'Methodological Basis', 'Citation': 'The original settings of the baselines are used in the citing paper, indicating a methodological basis for the study conducted.'}\n",
      "Causal Laws and Multi-Valued Fluents\n",
      "{'Category': 'Data Source', 'Citation': 'The self-attention mechanism in transformers is mentioned as a data source for the research on token pruning techniques.'}\n",
      "{'Category': 'Methodological Basis', 'Citation': 'The most token pruning approaches are discussed in the context of calculating importance scores and removing least important tokens, which the citing paper may have adopted in its research on token pruning techniques.'}\n",
      "{'Category': 'Methodological Basis', 'Citation': 'The loss of information due to token pruning is mentioned as a challenge in the research on token pruning techniques, which the citing paper may have addressed in its approach.'}\n",
      "{'Category': 'Methodological Basis', 'Citation': 'The retraining required to recover from the loss of information in token pruning is mentioned as a limitation in the research on token pruning techniques, which the citing paper may have discussed in its approach.'}\n",
      "{'Category': 'Data Source', 'Citation': 'The cited work is used to acknowledge the origin of the known fixed target for which the system is stable in the research conducted in the citing paper.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is used to extend the research on PIRL by providing a new approach of using higher-order functions in the controller symbol to improve the synthesis of a PID controller policy in the citing paper.'}\n",
      "{'Category': 'Supporting Evidence', 'Citation': \"(Goodfellow et al., Another work line is adversarial detection (AD), which attempts to detect adversarial samples before the misled results are adopted. AD methods can be roughly separated into supervised and unsupervised methods (Aldahdooh et al., 2022). In supervised methods, it's possible to reach a satisfying accuracy for detecting adversarial samples. However, they can only detect the attacks they have already known, not to mention the extra training for classifying the given attacked samples (Aldahdooh et al., 2022). The state-ofthe-art method now is LiBRe (2021), which reaches overwhelming detection accuracy and conquers most defects of other supervised methods.\"}\n",
      "{'Category': 'Methodological Basis', 'Citation': 'The citing paper modifies the original NeuS by changing the output dimension of the last fully-connected layer to 12, which is the same dimension as the feature vector used in the study.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited work is not provided in the context, but the citing paper extends the research by training the model for a different number of iterations on different datasets and using a different batch size and data augmentation techniques.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The proposed VGG-CNN architecture is an extension of the base Transfer learning model (VGG16), as it incorporates new layers and modifications to the original model.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': \"The proposed foundation model in the citing paper is an extension of the pre-training strategy used in the cited work, which includes a more diverse set of medical imaging datasets and a larger number of image-text pairs to improve the model's performance in understanding biomedical images.\"}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The citing paper extends the research of the cited works by training a latent gaussian model based on Variational Autoencoder (VAE) to model the probabilistic motion and use the modeled distributions as good initial values for the denoising process.'}\n",
      "{'Category': 'Methodological Basis', 'Citation': 'The citing paper proposes a physics module in the reverse diffusion model to provide implicit guidance for the denoising process, which builds upon the methods and techniques established in the cited works.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The proposed algorithm for calibrating neural model predictions by the degree of confidence not only produces high-quality estimates specifically for the NLP task, but also opens up another potential area of application of the attention mechanism -uncertainty estimation.'}\n",
      "{'Category': 'Data Source', 'Citation': 'The cited work by Shelmanov et al. (2021) and Vazhentsev et al. (2022) are used as data sources for the NLP task in the citing paper, providing the necessary information and methods for producing high-quality estimates.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The citing paper extends the research on simulated noisy audios by building a replicable noise-robustness evaluation benchmark based on Fleurs, which covers a wider range of language families, speech tasks, and noise types.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The cited works on traffic sign recognition and text image retrieval provide a basis for the discussion of scene text recognition in computer vision, which the citing paper further expands upon by focusing on the specific challenges and features of Chinese text recognition.'}\n",
      "{'Category': 'Data Source', 'Citation': 'The cited work by the authors of the paper is used to provide a framework for the study of disentangling content and orientation information from visual features in order to improve the accuracy of vertical Chinese text recognition.'}\n",
      "{'Category': 'Methodological Basis', 'Citation': '(Piao et al. 2019(Piao et al. , 2020;;Zhang et al. 2020;Liu et al. 2021bLiu et al. , 2020a;;Ma et al. 2023Ma et al. , 2022;;Liu et al.'}\n",
      "{'Category': 'Methodological Basis', 'Citation': '(Liu et al. 2021bLiu et al. , 2020a;;Ma et al. 2023Ma et al. , 2022;;Liu et al.'}\n",
      "{'Category': 'Methodological Basis', 'Citation': '(Ma et al. 2023Ma et al. , 2022;;Liu et al.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The idea of domain adaptation is further discussed in the citing paper to address the challenges of data collection and training in deep learning models for stress prediction in industrial problems involving 3D data.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The citing paper proposes the Audio Bridged Temporal Interaction (ABTI) module to bridge the cross-frame interaction by audio feature, which is an extension of the research in the cited works.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': '(Zhang et al. 2019b), (Wang, Jamnik, and Lio 2020), (Zheng, Zha, and Wei 2019), (Zhuo and Kankanhalli 2021), (Hu et al. 2021), (Sahu, Basioti, and Pavlovic 2023), (Hahne et al. 2019), (Spratley, Ehinger, and Miller 2020), (Wu et al.'}\n",
      "{'Category': 'Extension or Continuation', 'Citation': 'The citing paper extends the research on the impact of generated explanations on GEC performance by evaluating the quality of the explanations and their effect on the performance of the GEC system.'}\n",
      "{'Category': 'Data Source', 'Citation': 'The cited work by the PI is used as a source of human-written explanations and explanations generated by the PI for the few-shot learning task in the CoNLL2014, W&I, and JFLEG test datasets.'}\n"
     ]
    }
   ],
   "source": [
    "triplets = []\n",
    "\n",
    "for article_dict in annotated_article:\n",
    "    if \"mapped_citation\" not in article_dict.keys():\n",
    "        print(article_dict['title'])\n",
    "        continue\n",
    "    for key, val in article_dict['mapped_citation'].items():\n",
    "        title = val['title']\n",
    "        citation = val['citation']\n",
    "        \n",
    "        # Use a dictionary to group explanations by category\n",
    "        category_explanations = {}\n",
    "        for rel in citation:\n",
    "            # try:\n",
    "                category = rel['Category']\n",
    "                explanation = rel['Explanation']\n",
    "                if category not in category_explanations:\n",
    "                    category_explanations[category] = []\n",
    "                category_explanations[category].append(explanation)\n",
    "\n",
    "        source_node = paper_dict[title]\n",
    "        target_node = paper_dict[article_dict['title'].lower()]\n",
    "\n",
    "        # Construct triplets with aggregated explanations for each category\n",
    "        for category, explanations in category_explanations.items():\n",
    "            if category not in relationships_dict.keys():\n",
    "                relationships_dict[category] = f\"Is {category} Of\"\n",
    "\n",
    "            aggregated_explanation = \"; \".join(set(explanations))  # Remove duplicates and join explanations\n",
    "            rel = PaperEdge(category=category, explanation=aggregated_explanation)\n",
    "            reverse_rel = PaperEdge(category=relationships_dict[category], explanation=aggregated_explanation)\n",
    "\n",
    "            # Add the relationship in both directions\n",
    "            triplets.append((source_node, rel, target_node))\n",
    "            triplets.append((target_node, reverse_rel, source_node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'PaperEdge' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[155], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrel\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCategory\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'PaperEdge' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "rel['Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Assuming 'triplets' is your list of relationships, \n",
    "# and each PaperNode object in the triplets has an 'arxiv_id' attribute\n",
    "\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add nodes and edges\n",
    "for source_node, relationship, target_node in triplets:\n",
    "    # Add nodes if they are not already in the graph\n",
    "    if source_node.arxiv_id not in G:\n",
    "        G.add_node(source_node.title, title=str(source_node), arxiv_id=source_node.arxiv_id)\n",
    "    if target_node.arxiv_id not in G:\n",
    "        G.add_node(target_node.title, title=str(target_node), arxiv_id=target_node.arxiv_id)\n",
    "    \n",
    "    # Add edge with relationship details\n",
    "    G.add_edge(source_node.title, target_node.title, title=str(relationship), category=relationship.category, explanation=relationship.explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "586952\n"
     ]
    }
   ],
   "source": [
    "print(len(triplets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3506"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(G.nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.3.2 Visualizing citation graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112611"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paper_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics related to cogvideo: []\n",
      "Paper not found in the graph.\n"
     ]
    }
   ],
   "source": [
    "def find_connected_nodes(graph, node, relationship=None):\n",
    "    \"\"\"\n",
    "    Find nodes connected to the given node with an optional filter on the type of relationship.\n",
    "    \"\"\"\n",
    "    connected_nodes = []\n",
    "    for n, nbrs in graph.adj.items():\n",
    "        if n == node:\n",
    "            for nbr, eattr in nbrs.items():\n",
    "                if relationship is None or eattr['label'] == relationship:\n",
    "                    connected_nodes.append(nbr)\n",
    "    return connected_nodes\n",
    "\n",
    "# Function to search for a node by arxiv_id and return its details\n",
    "def find_nodes_by_arxiv_id(graph, arxiv_id):\n",
    "    for node, data in graph.nodes(data=True):\n",
    "        if data.get('arxiv_id') == arxiv_id:\n",
    "            return data  # or return data['paper_node'] to return the PaperNode object itself\n",
    "    return \"Paper not found in the graph.\"\n",
    "\n",
    "\n",
    "def find_shortest_path(graph, source, target):\n",
    "    \"\"\"\n",
    "    Find the shortest path between two nodes.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        path = nx.shortest_path(graph, source=source, target=target)\n",
    "        return path\n",
    "    except nx.NetworkXNoPath:\n",
    "        return None\n",
    "\n",
    "# Example Usage\n",
    "phenaki_related_topics = find_connected_nodes(G, 'cogview: mastering text-to-image generation via transformers')\n",
    "print(\"Topics related to cogvideo:\", phenaki_related_topics)\n",
    "\n",
    "# Example search\n",
    "search_result = find_nodes_by_arxiv_id(G, \"2209.14792\")\n",
    "print(search_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node: google's ai chatbot bard makes factual error in first demo\n",
      "  Connected to: DecipherPref: Analyzing Influential Factors in Human Preference Judgments via GPT-4 via Category: Data Source,\n",
      "Node: what makes a good conversation? how controllable attributes affect human judgments\n",
      "  Connected to: LEFTOVER-LUNCH: ADVANTAGE-BASED OFFLINE REINFORCEMENT LEARNING FOR LANGUAGE MODELS via Category: Extension or Continuation,\n",
      "Node: does bert make any sense? interpretable word sense disambiguation with contextualized embeddings\n",
      "  Connected to: SENTECON: Leveraging Lexicons to Learn Human-Interpretable Language Representations via Category: Methodological Basis,\n",
      "Node: learning the difference that makes a difference with counterfactually-augmented data\n",
      "  Connected to: Prompting Large Language Models for Counterfactual Generation: An Empirical Study via Category: Methodological Basis,\n",
      "Node: rethinking the role of demonstrations: what makes in-context learning work? in proceedings of empirical methods in natural language processing\n",
      "  Connected to: BUFFET: Benchmarking Large Language Models for Few-shot Cross-lingual Transfer via Category: Extension or Continuation,\n",
      "Node: transunet: transformers make strong encoders for medical image segmentation\n",
      "  Connected to: A T E X template Multiresolution Feature Guidance Based Transformer for Anomaly Detection via Category: Methodological Basis,\n",
      "Node: offline evaluation to make decisions about playlistrecommendation algorithms\n",
      "  Connected to: How Graph Convolutions Amplify Popularity Bias for Recommendation? via Category: Methodological Basis,\n",
      "Node: a simple way to make neural networks robust against diverse image corruptions\n",
      "  Connected to: HARD: Hard Augmentations for Robust Distillation via Category: Methodological Basis,\n",
      "Node: guess the instruction! flipped learning makes language models stronger zero-shot learners\n",
      "  Connected to: PURR: Efficiently Editing Language Model Hallucinations by Denoising Language Model Corruptions via Category: Methodological Basis,\n",
      "Node: pathologies of neural models make interpretations difficult\n",
      "  Connected to: Measuring Faithful and Plausible Visual Grounding in VQA via Category: Supporting Evidence,\n",
      "Node: what makes good in-context examples for gpt-3?\n",
      "  Connected to: SELF-ICL: Zero-Shot In-Context Learning with Self-Generated Demonstrations via Category: Methodological Basis,\n"
     ]
    }
   ],
   "source": [
    "def find_nodes_by_keyword(graph, keyword):\n",
    "    \"\"\"\n",
    "    Find nodes that contain the given keyword in their name and retrieve their connected nodes and relationships.\n",
    "    \"\"\"\n",
    "    keyword = keyword.lower()  # Convert keyword to lowercase for case-insensitive matching\n",
    "    matching_nodes = [node for node in graph.nodes if keyword in node.lower()]\n",
    "\n",
    "    related_nodes = {}\n",
    "    for node in matching_nodes:\n",
    "        connections = []\n",
    "        for neighbor, details in graph[node].items():\n",
    "            connections.append((neighbor, details['title'].split('\\n')[0]))\n",
    "        related_nodes[node] = connections\n",
    "\n",
    "    return related_nodes\n",
    "\n",
    "# Example Usage\n",
    "keyword = \"make\"\n",
    "phenaki_related = find_nodes_by_keyword(G, keyword)\n",
    "for node, connections in phenaki_related.items():\n",
    "    print(f\"Node: {node}\")\n",
    "    for conn in connections:\n",
    "        print(f\"  Connected to: {conn[0]} via {conn[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "ename": "NodeNotFound",
     "evalue": "Source make-a-video: text-to-video generation without text-video data is not in G",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNodeNotFound\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[140], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyvis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnetwork\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Network\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Assuming G is your original graph\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Step 1: Create the subgraph for \"Node1\" and its neighbors\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m subgraph \u001b[38;5;241m=\u001b[39m \u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mego_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmake-a-video: text-to-video generation without text-video data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mradius\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mundirected\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Nodes to be removed because they have a degree of 1 in the full graph\u001b[39;00m\n\u001b[1;32m      8\u001b[0m nodes_to_remove \u001b[38;5;241m=\u001b[39m [node \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m subgraph \u001b[38;5;28;01mif\u001b[39;00m subgraph\u001b[38;5;241m.\u001b[39mdegree(node) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m3\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/networkx/utils/backends.py:412\u001b[0m, in \u001b[0;36m_dispatch.__call__\u001b[0;34m(self, backend, *args, **kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m backends:\n\u001b[1;32m    411\u001b[0m         \u001b[38;5;66;03m# Fast path if no backends are installed\u001b[39;00m\n\u001b[0;32m--> 412\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morig_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;66;03m# Use `backend_name` in this function instead of `backend`\u001b[39;00m\n\u001b[1;32m    415\u001b[0m     backend_name \u001b[38;5;241m=\u001b[39m backend\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/networkx/generators/ego.py:60\u001b[0m, in \u001b[0;36mego_graph\u001b[0;34m(G, n, radius, center, undirected, distance)\u001b[0m\n\u001b[1;32m     58\u001b[0m         sp, _ \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39msingle_source_dijkstra(G, n, cutoff\u001b[38;5;241m=\u001b[39mradius, weight\u001b[38;5;241m=\u001b[39mdistance)\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m         sp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msingle_source_shortest_path_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcutoff\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mradius\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     62\u001b[0m H \u001b[38;5;241m=\u001b[39m G\u001b[38;5;241m.\u001b[39msubgraph(sp)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m center:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/networkx/utils/backends.py:412\u001b[0m, in \u001b[0;36m_dispatch.__call__\u001b[0;34m(self, backend, *args, **kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m backends:\n\u001b[1;32m    411\u001b[0m         \u001b[38;5;66;03m# Fast path if no backends are installed\u001b[39;00m\n\u001b[0;32m--> 412\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morig_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;66;03m# Use `backend_name` in this function instead of `backend`\u001b[39;00m\n\u001b[1;32m    415\u001b[0m     backend_name \u001b[38;5;241m=\u001b[39m backend\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/networkx/algorithms/shortest_paths/unweighted.py:58\u001b[0m, in \u001b[0;36msingle_source_shortest_path_length\u001b[0;34m(G, source, cutoff)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the shortest path lengths from source to all reachable nodes.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03mshortest_path_length\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m G:\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m nx\u001b[38;5;241m.\u001b[39mNodeNotFound(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSource \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not in G\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cutoff \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     cutoff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNodeNotFound\u001b[0m: Source make-a-video: text-to-video generation without text-video data is not in G"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "\n",
    "# Assuming G is your original graph\n",
    "# Step 1: Create the subgraph for \"Node1\" and its neighbors\n",
    "subgraph = nx.ego_graph(G, 'make-a-video: text-to-video generation without text-video data', radius=3, center=True, undirected=False)\n",
    "# Nodes to be removed because they have a degree of 1 in the full graph\n",
    "nodes_to_remove = [node for node in subgraph if subgraph.degree(node) < 3]\n",
    "\n",
    "# Remove the nodes from the ego graph\n",
    "subgraph.remove_nodes_from(nodes_to_remove)\n",
    "\n",
    "nt = Network(notebook=True, font_color='#10000000')\n",
    "nt.from_nx(subgraph)\n",
    "nt.show(\"nx.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.4 Building Graph Query Engine**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_retriever = index.as_retriever(\n",
    "    similarity_top_k=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'paper_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpaper_dict\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmake-a-video: text-to-video generation without text-video data\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'paper_dict' is not defined"
     ]
    }
   ],
   "source": [
    "paper_dict['make-a-video: text-to-video generation without text-video data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a survey on video diffusion models\n",
      " The recent wave of AI-generated content (AIGC) has witnessed substantialsuccess in computer vision, with the diffusion model playing a crucial role inthis achievement. Due to their impressive generative capabilities, diffusionmodels are gradually superseding methods based on GANs and auto-regressiveTransformers, demonstrating exceptional performance not only in imagegeneration and editing, but also in the realm of video-related research.However, existing surveys mainly focus on diffusion models in the context ofimage generation, with few up-to-date reviews on their application in the videodomain. To address this gap, this paper presents a comprehensive review ofvideo diffusion models in the AIGC era. Specifically, we begin with a conciseintroduction to the fundamentals and evolution of diffusion models.Subsequently, we present an overview of research on diffusion models in thevideo domain, categorizing the work into three key areas: video generation,video editing, and other video understanding tasks. We conduct a thoroughreview of the literature in these three key areas, including furthercategorization and practical contributions in the field. Finally, we discussthe challenges faced by research in this domain and outline potential futuredevelopmental trends. A comprehensive list of video diffusion models studied inthis survey is available athttps://github.com/ChenHsing/Awesome-Video-Diffusion-Models.\n",
      "{}\n",
      "frame by familiar frame: understanding replication in video diffusion models\n",
      " Building on the momentum of image generation diffusion models, there is anincreasing interest in video-based diffusion models. However, video generationposes greater challenges due to its higher-dimensional nature, the scarcity oftraining data, and the complex spatiotemporal relationships involved. Imagegeneration models, due to their extensive data requirements, have alreadystrained computational resources to their limits. There have been instances ofthese models reproducing elements from the training samples, leading toconcerns and even legal disputes over sample replication. Video diffusionmodels, which operate with even more constrained datasets and are tasked withgenerating both spatial and temporal content, may be more prone to replicatingsamples from their training sets. Compounding the issue, these models are oftenevaluated using metrics that inadvertently reward replication. In our paper, wepresent a systematic investigation into the phenomenon of sample replication invideo diffusion models. We scrutinize various recent diffusion models for videosynthesis, assessing their tendency to replicate spatial and temporal contentin both unconditional and conditional generation scenarios. Our studyidentifies strategies that are less likely to lead to replication. Furthermore,we propose new evaluation strategies that take replication into account,offering a more accurate measure of a model's ability to generate the originalcontent.\n",
      "{}\n",
      "efficient video diffusion models via content-frame motion-latent decomposition\n",
      " Video diffusion models have recently made great progress in generationquality, but are still limited by the high memory and computationalrequirements. This is because current video diffusion models often attempt toprocess high-dimensional videos directly. To tackle this issue, we proposecontent-motion latent diffusion model (CMD), a novel efficient extension ofpretrained image diffusion models for video generation. Specifically, wepropose an autoencoder that succinctly encodes a video as a combination of acontent frame (like an image) and a low-dimensional motion latentrepresentation. The former represents the common content, and the latterrepresents the underlying motion in the video, respectively. We generate thecontent frame by fine-tuning a pretrained image diffusion model, and wegenerate the motion latent representation by training a new lightweightdiffusion model. A key innovation here is the design of a compact latent spacethat can directly utilizes a pretrained image diffusion model, which has notbeen done in previous latent video diffusion models. This leads to considerablybetter quality generation and reduced computational costs. For instance, CMDcan sample a video 7.7$\\times$ faster than prior approaches by generating avideo of 512$\\times$1024 resolution and length 16 in 3.1 seconds. Moreover, CMDachieves an FVD score of 212.7 on WebVid-10M, 27.3% better than the previousstate-of-the-art of 292.4.\n",
      "{}\n",
      "stable video diffusion: scaling latent video diffusion models to large datasets\n",
      " We present Stable Video Diffusion - a latent video diffusion model forhigh-resolution, state-of-the-art text-to-video and image-to-video generation.Recently, latent diffusion models trained for 2D image synthesis have beenturned into generative video models by inserting temporal layers and finetuningthem on small, high-quality video datasets. However, training methods in theliterature vary widely, and the field has yet to agree on a unified strategyfor curating video data. In this paper, we identify and evaluate threedifferent stages for successful training of video LDMs: text-to-imagepretraining, video pretraining, and high-quality video finetuning. Furthermore,we demonstrate the necessity of a well-curated pretraining dataset forgenerating high-quality videos and present a systematic curation process totrain a strong base model, including captioning and filtering strategies. Wethen explore the impact of finetuning our base model on high-quality data andtrain a text-to-video model that is competitive with closed-source videogeneration. We also show that our base model provides a powerful motionrepresentation for downstream tasks such as image-to-video generation andadaptability to camera motion-specific LoRA modules. Finally, we demonstratethat our model provides a strong multi-view 3D-prior and can serve as a base tofinetune a multi-view diffusion model that jointly generates multiple views ofobjects in a feedforward fashion, outperforming image-based methods at afraction of their compute budget. We release code and model weights athttps://github.com/Stability-AI/generative-models .\n",
      "{}\n",
      "preserve your own correlation: a noise prior for video diffusion models\n",
      " Despite tremendous progress in generating high-quality images using diffusionmodels, synthesizing a sequence of animated frames that are both photorealisticand temporally coherent is still in its infancy. While off-the-shelfbillion-scale datasets for image generation are available, collecting similarvideo data of the same scale is still challenging. Also, training a videodiffusion model is computationally much more expensive than its imagecounterpart. In this work, we explore finetuning a pretrained image diffusionmodel with video data as a practical solution for the video synthesis task. Wefind that naively extending the image noise prior to video noise prior in videodiffusion leads to sub-optimal performance. Our carefully designed video noiseprior leads to substantially better performance. Extensive experimentalvalidation shows that our model, Preserve Your Own Correlation (PYoCo), attainsSOTA zero-shot text-to-video results on the UCF-101 and MSR-VTT benchmarks. Italso achieves SOTA video generation quality on the small-scale UCF-101benchmark with a $10\\times$ smaller model using significantly less computationthan the prior art.\n",
      "{}\n",
      "diffusion models: a comprehensive survey of methods and applications\n",
      " Diffusion models have emerged as a powerful new family of deep generativemodels with record-breaking performance in many applications, including imagesynthesis, video generation, and molecule design. In this survey, we provide anoverview of the rapidly expanding body of work on diffusion models,categorizing the research into three key areas: efficient sampling, improvedlikelihood estimation, and handling data with special structures. We alsodiscuss the potential for combining diffusion models with other generativemodels for enhanced results. We further review the wide-ranging applications ofdiffusion models in fields spanning from computer vision, natural languagegeneration, temporal data modeling, to interdisciplinary applications in otherscientific disciplines. This survey aims to provide a contextualized, in-depthlook at the state of diffusion models, identifying the key areas of focus andpointing to potential areas for further exploration. Github:https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy.\n",
      "{}\n",
      "diffusion probabilistic modeling for video generation\n",
      " Denoising diffusion probabilistic models are a promising new class ofgenerative models that mark a milestone in high-quality image generation. Thispaper showcases their ability to sequentially generate video, surpassing priormethods in perceptual and probabilistic forecasting metrics. We propose anautoregressive, end-to-end optimized video diffusion model inspired by recentadvances in neural video compression. The model successively generates futureframes by correcting a deterministic next-frame prediction using a stochasticresidual generated by an inverse diffusion process. We compare this approachagainst five baselines on four datasets involving natural and simulation-basedvideos. We find significant improvements in terms of perceptual quality for alldatasets. Furthermore, by introducing a scalable version of the ContinuousRanked Probability Score (CRPS) applicable to video, we show that our modelalso outperforms existing approaches in their probabilistic frame forecastingability.\n",
      "{}\n",
      "diffusion models for time series applications: a survey\n",
      " Diffusion models, a family of generative models based on deep learning, havebecome increasingly prominent in cutting-edge machine learning research. With adistinguished performance in generating samples that resemble the observeddata, diffusion models are widely used in image, video, and text synthesisnowadays. In recent years, the concept of diffusion has been extended to timeseries applications, and many powerful models have been developed. Consideringthe deficiency of a methodical summary and discourse on these models, weprovide this survey as an elementary resource for new researchers in this areaand also an inspiration to motivate future research. For better understanding,we include an introduction about the basics of diffusion models. Except forthis, we primarily focus on diffusion-based methods for time seriesforecasting, imputation, and generation, and present them respectively in threeindividual sections. We also compare different methods for the same applicationand highlight their connections if applicable. Lastly, we conclude the commonlimitation of diffusion-based methods and highlight potential future researchdirections.\n",
      "{}\n",
      "video diffusion models\n",
      " Generating temporally coherent high fidelity video is an important milestonein generative modeling research. We make progress towards this milestone byproposing a diffusion model for video generation that shows very promisinginitial results. Our model is a natural extension of the standard imagediffusion architecture, and it enables jointly training from image and videodata, which we find to reduce the variance of minibatch gradients and speed upoptimization. To generate long and higher resolution videos we introduce a newconditional sampling technique for spatial and temporal video extension thatperforms better than previously proposed methods. We present the first resultson a large text-conditioned video generation task, as well as state-of-the-artresults on established benchmarks for video prediction and unconditional videogeneration. Supplementary material is available athttps://video-diffusion.github.io/\n",
      "{}\n",
      "medm: mediating image diffusion models for video-to-video translation with temporal correspondence guidance\n",
      " This study introduces an efficient and effective method, MeDM, that utilizespre-trained image Diffusion Models for video-to-video translation withconsistent temporal flow. The proposed framework can render videos from sceneposition information, such as a normal G-buffer, or perform text-guided editingon videos captured in real-world scenarios. We employ explicit optical flows toconstruct a practical coding that enforces physical constraints on generatedframes and mediates independent frame-wise scores. By leveraging this coding,maintaining temporal consistency in the generated videos can be framed as anoptimization problem with a closed-form solution. To ensure compatibility withStable Diffusion, we also suggest a workaround for modifying observation-spacescores in latent Diffusion Models. Notably, MeDM does not require fine-tuningor test-time optimization of the Diffusion Models. Through extensivequalitative, quantitative, and subjective experiments on various benchmarks,the study demonstrates the effectiveness and superiority of the proposedapproach. Our project page can be found at https://medm2023.github.io\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "results = paper_retriever.retrieve(\"Give me some paper about Video diffusion models\")\n",
    "nodes = []\n",
    "for r in results:\n",
    "    title = r.text.split(\"/n\")[0]\n",
    "    print(title)\n",
    "    node = find_nodes_by_keyword(G, title)\n",
    "    print(node)\n",
    "    nodes.append(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Basic Data Science Assistant**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.1 Download Wikipedia Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data science questions, I will use the source from wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U wikipedia-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Pre-compile the regular expression pattern for better performance\n",
    "BRACES_PATTERN = re.compile(r'\\{.*?\\}|\\}')\n",
    "\n",
    "def remove_braces_and_content(text):\n",
    "    \"\"\"Remove all occurrences of curly braces and their content from the given text\"\"\"\n",
    "    return BRACES_PATTERN.sub('', text)\n",
    "\n",
    "def clean_string(input_string):\n",
    "    \"\"\"Clean the input string.\"\"\"\n",
    "    \n",
    "    # Remove extra spaces by splitting the string by spaces and joining back together\n",
    "    cleaned_string = ' '.join(input_string.split())\n",
    "    \n",
    "    # Remove consecutive carriage return characters until there are no more consecutive occurrences\n",
    "    cleaned_string = re.sub(r'\\r+', '\\r', cleaned_string)\n",
    "    \n",
    "    # Remove all occurrences of curly braces and their content from the cleaned string\n",
    "    cleaned_string = remove_braces_and_content(cleaned_string)\n",
    "    \n",
    "    # Return the cleaned string\n",
    "    return cleaned_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_wikipedia_pages(wiki_wiki, category_name):\n",
    "    \"\"\"Extract all references from a category on Wikipedia\"\"\"\n",
    "    \n",
    "    # Get the Wikipedia page corresponding to the provided category name\n",
    "    category = wiki_wiki.page(\"Category:\" + category_name)\n",
    "    \n",
    "    # Initialize an empty list to store page titles\n",
    "    pages = []\n",
    "    \n",
    "    # Check if the category exists\n",
    "    if category.exists():\n",
    "        # Iterate through each article in the category and append its title to the list\n",
    "        for article in category.categorymembers.values():\n",
    "            pages.append(article.title)\n",
    "    \n",
    "    # Return the list of page titles\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_wikipedia_pages(categories):\n",
    "    \"\"\"Retrieve Wikipedia pages from a list of categories and extract their content\"\"\"\n",
    "    \n",
    "    # Create a Wikipedia object\n",
    "    wiki_wiki = wikipediaapi.Wikipedia('Kaggle Data Science Assistant with Gemma', 'en')\n",
    "    \n",
    "    # Initialize lists to store explored categories and Wikipedia pages\n",
    "    explored_categories = []\n",
    "    wikipedia_pages = []\n",
    "\n",
    "    # Iterate through each category\n",
    "    print(\"- Processing Wikipedia categories:\")\n",
    "    for category_name in categories:\n",
    "        print(f\"\\tExploring {category_name} on Wikipedia\")\n",
    "        \n",
    "        # Get the Wikipedia page corresponding to the category\n",
    "        category = wiki_wiki.page(\"Category:\" + category_name)\n",
    "        \n",
    "        # Extract Wikipedia pages from the category and extend the list\n",
    "        wikipedia_pages.extend(extract_wikipedia_pages(wiki_wiki, category_name))\n",
    "        \n",
    "        # Add the explored category to the list\n",
    "        explored_categories.append(category_name)\n",
    "\n",
    "    # Extract subcategories and remove duplicate categories\n",
    "    categories_to_explore = [item.replace(\"Category:\", \"\") for item in wikipedia_pages if \"Category:\" in item]\n",
    "    wikipedia_pages = list(set([item for item in wikipedia_pages if \"Category:\" not in item]))\n",
    "    \n",
    "    # Explore subcategories recursively\n",
    "    while categories_to_explore:\n",
    "        category_name = categories_to_explore.pop()\n",
    "        print(f\"\\tExploring {category_name} on Wikipedia\")\n",
    "        \n",
    "        # Extract more references from the subcategory\n",
    "        more_refs = extract_wikipedia_pages(wiki_wiki, category_name)\n",
    "\n",
    "        # Iterate through the references\n",
    "        for ref in more_refs:\n",
    "            # Check if the reference is a category\n",
    "            if \"Category:\" in ref:\n",
    "                new_category = ref.replace(\"Category:\", \"\")\n",
    "                # Add the new category to the explored categories list\n",
    "                if new_category not in explored_categories:\n",
    "                    explored_categories.append(new_category)\n",
    "            else:\n",
    "                # Add the reference to the Wikipedia pages list\n",
    "                if ref not in wikipedia_pages:\n",
    "                    wikipedia_pages.append(ref)\n",
    "\n",
    "    # Initialize a list to store extracted texts\n",
    "    extracted_texts = []\n",
    "    \n",
    "    # Iterate through each Wikipedia page\n",
    "    print(\"- Processing Wikipedia pages:\")\n",
    "    for page_title in tqdm(wikipedia_pages, total=len(wikipedia_pages)):\n",
    "        # Get the Wikipedia page\n",
    "        page = wiki_wiki.page(page_title)\n",
    "\n",
    "        # Append the page title and summary to the extracted texts list\n",
    "        if len(page.summary) > len(page.title):\n",
    "            extracted_texts.append(page.title + \" : \" + clean_string(page.summary))\n",
    "        \n",
    "        # Iterate through the sections in the page\n",
    "        for section in page.sections:\n",
    "            # Append the page title and section text to the extracted texts list\n",
    "            if len(section.text) > len(page.title):\n",
    "                extracted_texts.append(page.title + \" : \" + clean_string(section.text))\n",
    "                \n",
    "    # Return the extracted texts\n",
    "    return extracted_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Processing Wikipedia categories:\n",
      "\tExploring Machine_learning on Wikipedia\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tExploring Data_science on Wikipedia\n",
      "\tExploring Statistics on Wikipedia\n",
      "\tExploring Deep_learning on Wikipedia\n",
      "\tExploring Artificial_intelligence on Wikipedia\n",
      "\tExploring Artificial intelligence stubs on Wikipedia\n",
      "\tExploring Works created using artificial intelligence on Wikipedia\n",
      "\tExploring Virtual assistants on Wikipedia\n",
      "\tExploring Turing tests on Wikipedia\n",
      "\tExploring AI software on Wikipedia\n",
      "\tExploring Rule engines on Wikipedia\n",
      "\tExploring Artificial intelligence publications on Wikipedia\n",
      "\tExploring Philosophy of artificial intelligence on Wikipedia\n",
      "\tExploring Artificial intelligence people on Wikipedia\n",
      "\tExploring Open-source artificial intelligence on Wikipedia\n",
      "\tExploring Non-fiction books about Artificial intelligence on Wikipedia\n",
      "\tExploring Neural networks on Wikipedia\n",
      "\tExploring Multi-agent systems on Wikipedia\n",
      "\tExploring Mindâ€“body problem on Wikipedia\n",
      "\tExploring Machine learning on Wikipedia\n",
      "\tExploring Artificial intelligence laboratories on Wikipedia\n",
      "\tExploring Knowledge representation on Wikipedia\n",
      "\tExploring History of artificial intelligence on Wikipedia\n",
      "\tExploring Generative artificial intelligence on Wikipedia\n",
      "\tExploring Game artificial intelligence on Wikipedia\n",
      "\tExploring Fuzzy logic on Wikipedia\n",
      "\tExploring Fiction about artificial intelligence on Wikipedia\n",
      "\tExploring Existential risk from artificial general intelligence on Wikipedia\n",
      "\tExploring Evolutionary computation on Wikipedia\n",
      "\tExploring Artificial intelligence entertainment on Wikipedia\n",
      "\tExploring Distributed artificial intelligence on Wikipedia\n",
      "\tExploring Signal processing conferences on Wikipedia\n",
      "\tExploring Artificial intelligence conferences on Wikipedia\n",
      "\tExploring Computer vision on Wikipedia\n",
      "\tExploring Artificial intelligence competitions on Wikipedia\n",
      "\tExploring AI companies on Wikipedia\n",
      "\tExploring Cognitive architecture on Wikipedia\n",
      "\tExploring Cloud robotics on Wikipedia\n",
      "\tExploring Chatbots on Wikipedia\n",
      "\tExploring Automated reasoning on Wikipedia\n",
      "\tExploring Artificial intelligence associations on Wikipedia\n",
      "\tExploring Artificial intelligence templates on Wikipedia\n",
      "\tExploring Artificial immune systems on Wikipedia\n",
      "\tExploring Artificial intelligence art on Wikipedia\n",
      "\tExploring Argument technology on Wikipedia\n",
      "\tExploring Applications of artificial intelligence on Wikipedia\n",
      "\tExploring Ambient intelligence on Wikipedia\n",
      "\tExploring AI accelerators on Wikipedia\n",
      "\tExploring Affective computing on Wikipedia\n",
      "\tExploring Text-to-image generation on Wikipedia\n",
      "\tExploring Google DeepMind on Wikipedia\n",
      "\tExploring Deepfakes on Wikipedia\n",
      "\tExploring Deep learning software on Wikipedia\n",
      "\tExploring Statistics stubs on Wikipedia\n",
      "\tExploring Statistical concepts on Wikipedia\n",
      "\tExploring Statistical software on Wikipedia\n",
      "\tExploring Statistical methods on Wikipedia\n",
      "\tExploring Statistical data on Wikipedia\n",
      "\tExploring Subfields of statistics on Wikipedia\n",
      "\tExploring Statistics profession and organizations on Wikipedia\n",
      "\tExploring Statistics-related lists on Wikipedia\n",
      "\tExploring Statisticians on Wikipedia\n",
      "\tExploring Data scientists on Wikipedia\n",
      "\tExploring Unsupervised learning on Wikipedia\n",
      "\tExploring Support vector machines on Wikipedia\n",
      "\tExploring Supervised learning on Wikipedia\n",
      "\tExploring Structured prediction on Wikipedia\n",
      "\tExploring Statistical natural language processing on Wikipedia\n",
      "\tExploring Semisupervised learning on Wikipedia\n",
      "\tExploring Natural language processing researchers on Wikipedia\n",
      "\tExploring Machine learning researchers on Wikipedia\n",
      "\tExploring Reinforcement learning on Wikipedia\n",
      "\tExploring Ontology learning (computer science) on Wikipedia\n",
      "\tExploring Markov models on Wikipedia\n",
      "\tExploring Machine learning task on Wikipedia\n",
      "\tExploring Machine learning algorithms on Wikipedia\n",
      "\tExploring Loss functions on Wikipedia\n",
      "\tExploring Log-linear models on Wikipedia\n",
      "\tExploring Learning in computer vision on Wikipedia\n",
      "\tExploring Latent variable models on Wikipedia\n",
      "\tExploring Kernel methods for machine learning on Wikipedia\n",
      "\tExploring Inductive logic programming on Wikipedia\n",
      "\tExploring Genetic programming on Wikipedia\n",
      "\tExploring Evolutionary algorithms on Wikipedia\n",
      "\tExploring Ensemble learning on Wikipedia\n",
      "\tExploring Dimension reduction on Wikipedia\n",
      "\tExploring Datasets in machine learning on Wikipedia\n",
      "\tExploring Data mining and machine learning software on Wikipedia\n",
      "\tExploring Signal processing conferences on Wikipedia\n",
      "\tExploring Artificial intelligence conferences on Wikipedia\n",
      "\tExploring Computational learning theory on Wikipedia\n",
      "\tExploring Cluster analysis on Wikipedia\n",
      "\tExploring Classification algorithms on Wikipedia\n",
      "\tExploring Blockmodeling on Wikipedia\n",
      "\tExploring Bayesian networks on Wikipedia\n",
      "\tExploring Artificial neural networks on Wikipedia\n",
      "\tExploring Applied machine learning on Wikipedia\n",
      "- Processing Wikipedia pages:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3448/3448 [22:26<00:00,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16232 Wikipedia pages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "categories = [\"Machine_learning\", \"Data_science\", \"Statistics\", \"Deep_learning\", \"Artificial_intelligence\"]\n",
    "extracted_texts = get_wikipedia_pages(categories)\n",
    "print(\"Found\", len(extracted_texts), \"Wikipedia pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_documents = [Document(text=extracted_text, doc_id=str(i)) for i, extracted_text in enumerate(extracted_texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.PersistentClient(path=\"../DB/wiki\")\n",
    "chroma_collection = chroma_client.get_or_create_collection(\"gemma_assistant_wiki\")\n",
    "\n",
    "\n",
    "# Create vector store\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16232/16232 [00:08<00:00, 1830.75it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:11<00:00, 175.13it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:10<00:00, 188.60it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:11<00:00, 175.40it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:11<00:00, 177.25it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:10<00:00, 202.03it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:09<00:00, 209.87it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:11<00:00, 185.12it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:11<00:00, 182.42it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 73/73 [00:00<00:00, 197.81it/s]\n"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "    wiki_documents, storage_context=storage_context, embed_model=embed_model, show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.2 Loading from vector store**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "import torch\n",
    "\n",
    "\n",
    "Settings.llm = None # Set this to none to make the index only do retrieval\n",
    "device_type = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\", cache_folder=\"../models\", device=device_type) # must be the same as the previous stage\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=\"../DB/wiki\")\n",
    "chroma_collection = chroma_client.get_or_create_collection(\"gemma_assistant_wiki\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "# load the vectorstore\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_vector_store(vector_store, storage_context=storage_context, embed_model=embed_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_science_query_engine = index.as_query_engine(\n",
    "    similarity_top_k=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "Outline of regression analysis : Regression analysis Linear regression\n",
      "\n",
      "Regression diagnostic : Regression diagnostics have often been developed or were initially proposed in the context of linear regression or, more particularly, ordinary least squares. This means that many formally defined diagnostics are only available for these contexts.\n",
      "\n",
      "Linear predictor function : In statistics and in machine learning, a linear predictor function is a linear function (linear combination) of a set of coefficients and explanatory variables (independent variables), whose value is used to predict the outcome of a dependent variable. This sort of function usually comes in linear regression, where the coefficients are called regression coefficients. However, they also occur in various types of linear classifiers (e.g. logistic regression, perceptrons, support vector machines, and linear discriminant analysis), as well as in various other models, such as principal component analysis and factor analysis. In many of these models, the coefficients are referred to as \"weights\".\n",
      "\n",
      "Outline of regression analysis : General linear model Ordinary least squares Generalized least squares Simple linear regression Trend estimation Ridge regression Polynomial regression Segmented regression Nonlinear regression\n",
      "\n",
      "Outline of regression analysis : Least squares Linear least squares (mathematics) Non-linear least squares Least absolute deviations Curve fitting Smoothing Cross-sectional study\n",
      "\n",
      "Partial least squares regression : Partial least squares regression (PLS regression) is a statistical method that bears some relation to principal components regression; instead of finding hyperplanes of maximum variance between the response and independent variables, it finds a linear regression model by projecting the predicted variables and the observable variables to a new space. Because both the X and Y data are projected to new spaces, the PLS family of methods are known as bilinear factor models. Partial least squares discriminant analysis (PLS-DA) is a variant used when the Y is categorical. PLS is used to find the fundamental relations between 2 matrices (X and Y), i.e. a latent variable approach to modeling the covariance structures in these two spaces. A PLS model will try to find the multidimensional direction in the X space that explains the maximum multidimensional variance direction in the Y space. PLS regression is particularly suited when the matrix of predictors has more variables than observations, and when there is multicollinearity among X values. By contrast, standard regression will fail in these cases (unless it is regularized). Partial least squares was introduced by the Swedish statistician Herman O. A. Wold, who then developed it with his son, Svante Wold. An alternative term for PLS is projection to latent structures, but the term partial least squares is still dominant in many areas. Although the original applications were in the social sciences, PLS regression is today most widely used in chemometrics and related areas. It is also used in bioinformatics, sensometrics, neuroscience, and anthropology.\n",
      "\n",
      "Linear predictor function : Linear model Linear regression == References ==\n",
      "\n",
      "Outline of regression analysis : The following outline is provided as an overview of and topical guide to regression analysis: Regression analysis â€“ use of statistical techniques for learning about the relationship between one or more dependent variables (Y) and one or more independent variables (X).\n",
      "\n",
      "Ecological regression : Ecological regression is a statistical technique which runs regression on aggregates, often used in political science and history to estimate group voting behavior from aggregate data.For example, if counties have a known Democratic vote (in percentage) D, and a known percentage of Catholics, C, then running a linear regression of dependent variable D against independent variable C will give D = a + bC. If the regression gives D = .22 + .45C for example, then the estimated Catholic vote (C = 1) is 67% Democratic and the non-Catholic vote (C = 0) is 22% Democratic. The technique has been often used in litigation brought under the Voting Rights Act of 1965 to see how blacks and whites voted.\n",
      "\n",
      "Proper linear model : In statistics, a proper linear model is a linear regression model in which the weights given to the predictor variables are chosen in such a way as to optimize the relationship between the prediction and the criterion. Simple regression analysis is the most common example of a proper linear model. Unit-weighted regression is the most common example of an improper linear model.\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: What is linear regression\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "print(data_science_query_engine.query(\"What is linear regression\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Python Code Assistant**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.2 Define a code intepreter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import regex\n",
    "import pickle\n",
    "import traceback\n",
    "import copy\n",
    "import datetime\n",
    "import dateutil.relativedelta\n",
    "import multiprocess\n",
    "from multiprocess import Pool\n",
    "from typing import Any, Dict, Optional\n",
    "from pebble import ProcessPool\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import TimeoutError\n",
    "from functools import partial\n",
    "from timeout_decorator import timeout\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "\n",
    "class GenericRuntime:\n",
    "    GLOBAL_DICT = {}\n",
    "    LOCAL_DICT = None\n",
    "    HEADERS = []\n",
    "    def __init__(self):\n",
    "        self._global_vars = copy.copy(self.GLOBAL_DICT)\n",
    "        self._local_vars = copy.copy(self.LOCAL_DICT) if self.LOCAL_DICT else None\n",
    "\n",
    "        for c in self.HEADERS:\n",
    "            self.exec_code(c)\n",
    "\n",
    "    def exec_code(self, code_piece: str) -> None:\n",
    "        if regex.search(r'(\\s|^)?input\\(', code_piece) or regex.search(r'(\\s|^)?os.system\\(', code_piece):\n",
    "            raise RuntimeError()\n",
    "        exec(code_piece, self._global_vars)\n",
    "        \n",
    "    def eval_code(self, expr: str) -> Any:\n",
    "        return eval(expr, self._global_vars)\n",
    "    \n",
    "    def inject(self, var_dict: Dict[str, Any]) -> None:\n",
    "        for k, v in var_dict.items():\n",
    "            self._global_vars[k] = v\n",
    "    \n",
    "    @property\n",
    "    def answer(self):\n",
    "        return self._global_vars['answer']\n",
    "\n",
    "class DateRuntime(GenericRuntime):\n",
    "    GLOBAL_DICT = {\n",
    "        'datetime': datetime.datetime, \n",
    "        'timedelta': dateutil.relativedelta.relativedelta,\n",
    "        'relativedelta': dateutil.relativedelta.relativedelta\n",
    "    }\n",
    "\n",
    "\n",
    "class CustomDict(dict):\n",
    "    def __iter__(self):\n",
    "        return list(super().__iter__()).__iter__()\n",
    "\n",
    "class ColorObjectRuntime(GenericRuntime):\n",
    "    GLOBAL_DICT = {'dict': CustomDict}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Hello world!', 'Done')\n"
     ]
    }
   ],
   "source": [
    "class PythonExecutor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        runtime: Optional[Any] = None,\n",
    "        get_answer_symbol: Optional[str] = None,\n",
    "        get_answer_expr: Optional[str] = None,\n",
    "        get_answer_from_stdout: bool = False,\n",
    "        timeout_length: int = 5,\n",
    "    ) -> None:\n",
    "        self.runtime = runtime if runtime else GenericRuntime()\n",
    "        self.answer_symbol = get_answer_symbol\n",
    "        self.answer_expr = get_answer_expr\n",
    "        self.get_answer_from_stdout = get_answer_from_stdout\n",
    "        self.pool = Pool(multiprocess.cpu_count())\n",
    "        self.timeout_length = timeout_length\n",
    "\n",
    "    def process_generation_to_code(self, gens: str):\n",
    "        return [g.split('\\n') for g in gens]\n",
    "\n",
    "    @staticmethod\n",
    "    def execute(\n",
    "        code,\n",
    "        get_answer_from_stdout = None,\n",
    "        runtime = None,\n",
    "        answer_symbol = None,\n",
    "        answer_expr = None,\n",
    "        timeout_length = 10,\n",
    "    ):\n",
    "        try:\n",
    "            if get_answer_from_stdout:\n",
    "                program_io = io.StringIO()\n",
    "                with redirect_stdout(program_io):\n",
    "                    timeout(timeout_length)(runtime.exec_code)('\\n'.join(code))\n",
    "                program_io.seek(0)\n",
    "                result = program_io.read()\n",
    "            elif answer_symbol:\n",
    "                timeout(timeout_length)(runtime.exec_code)('\\n'.join(code))\n",
    "                result = runtime._global_vars[answer_symbol]\n",
    "            elif answer_expr:\n",
    "                timeout(timeout_length)(runtime.exec_code)('\\n'.join(code))\n",
    "                result = timeout(timeout_length)(runtime.eval_code)(answer_expr)\n",
    "            else:\n",
    "                timeout(timeout_length)(runtime.exec_code)('\\n'.join(code[:-1]))\n",
    "                result = timeout(timeout_length)(runtime.eval_code)(code[-1])\n",
    "            report = \"Done\"\n",
    "            str(result)\n",
    "            pickle.dumps(result) # serialization check\n",
    "        except:\n",
    "            result = ''\n",
    "            report = traceback.format_exc().split('\\n')[-2]\n",
    "        return result, report\n",
    "\n",
    "    def apply(self, code):\n",
    "        return self.batch_apply([code])[0]\n",
    "\n",
    "    @staticmethod\n",
    "    def truncate(s, max_length=400):\n",
    "        half = max_length // 2\n",
    "        if len(s) > max_length:\n",
    "            s = s[:half] + \"...\" + s[-half:]\n",
    "        return s\n",
    "\n",
    "    def batch_apply(self, batch_code):\n",
    "        all_code_snippets = self.process_generation_to_code(batch_code)\n",
    "\n",
    "        timeout_cnt = 0\n",
    "        all_exec_results = []\n",
    "        with ProcessPool(max_workers=min(len(all_code_snippets), os.cpu_count())) as pool:\n",
    "            executor = partial(\n",
    "                self.execute,\n",
    "                get_answer_from_stdout=self.get_answer_from_stdout,\n",
    "                runtime=self.runtime,\n",
    "                answer_symbol=self.answer_symbol,\n",
    "                answer_expr=self.answer_expr,\n",
    "                timeout_length=self.timeout_length, # this timeout not work\n",
    "            )\n",
    "            future = pool.map(executor, all_code_snippets, timeout=self.timeout_length)\n",
    "            iterator = future.result()\n",
    "\n",
    "            if len(all_code_snippets) > 100:  \n",
    "                progress_bar = tqdm(total=len(all_code_snippets), desc=\"Execute\")  \n",
    "            else:  \n",
    "                progress_bar = None \n",
    "\n",
    "            while True:\n",
    "                try:\n",
    "                    result = next(iterator)\n",
    "                    all_exec_results.append(result)\n",
    "                except StopIteration:\n",
    "                    break\n",
    "                except TimeoutError as error:\n",
    "                    print(error)\n",
    "                    all_exec_results.append((\"\", \"Timeout Error\"))\n",
    "                    timeout_cnt += 1\n",
    "                except Exception as error:\n",
    "                    print(error)\n",
    "                    exit()\n",
    "                if progress_bar is not None:\n",
    "                    progress_bar.update(1) \n",
    "            \n",
    "            if progress_bar is not None:\n",
    "                progress_bar.close() \n",
    "\n",
    "        batch_results = []\n",
    "        for code, (res, report) in zip(all_code_snippets, all_exec_results):\n",
    "            # post processing\n",
    "            res, report = str(res).strip(), str(report).strip()\n",
    "            res, report = self.truncate(res), self.truncate(report)\n",
    "            batch_results.append((res, report))\n",
    "        return batch_results\n",
    "\n",
    "\n",
    "def test():\n",
    "    batch_code = [\n",
    "\"\"\"\n",
    "print(\"Hello world!\")\n",
    "\"\"\"\n",
    "    ]\n",
    "\n",
    "    executor = PythonExecutor(get_answer_from_stdout=True)\n",
    "    predictions = executor.apply(batch_code[0])\n",
    "    print(predictions)\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5. Combine all of them together**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.1 Define Router Engine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from unsloth import FastLanguageModel\n",
    "import os\n",
    "\n",
    "max_seq_length = 8192 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "\n",
    "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#     model_name = \"unsloth/gemma-7b-bnb-4bit\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "#     max_seq_length = max_seq_length,\n",
    "#     dtype = dtype,\n",
    "#     load_in_4bit = load_in_4bit,\n",
    "#     token = \"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\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    "#     cache_dir = \"../models\",\n",
    "# )\n",
    "# FastLanguageModel.for_inference(model) \n",
    "\n",
    "# llm = HuggingFaceLLM(model=model, tokenizer=tokenizer)\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo\", api_key=\"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\")\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert output to JSON: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 18\u001b[0m\n\u001b[1;32m      5\u001b[0m ds_tool \u001b[38;5;241m=\u001b[39m QueryEngineTool\u001b[38;5;241m.\u001b[39mfrom_defaults(\n\u001b[1;32m      6\u001b[0m     query_engine\u001b[38;5;241m=\u001b[39mdata_science_query_engine,\n\u001b[1;32m      7\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUseful for answering data science concepts\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m query_engine \u001b[38;5;241m=\u001b[39m RouterQueryEngine(\n\u001b[1;32m     11\u001b[0m     selector\u001b[38;5;241m=\u001b[39mLLMSingleSelector\u001b[38;5;241m.\u001b[39mfrom_defaults(),\n\u001b[1;32m     12\u001b[0m     query_engine_tools\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     17\u001b[0m )\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mquery_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is linear regression?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:211\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/llama_index/core/base/base_query_engine.py:53\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     52\u001b[0m         str_or_query_bundle \u001b[38;5;241m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[0;32m---> 53\u001b[0m     query_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstr_or_query_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m dispatch_event(QueryEndEvent())\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m query_result\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/llama_index/core/query_engine/router_query_engine.py:169\u001b[0m, in \u001b[0;36mRouterQueryEngine._query\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, query_bundle: QueryBundle) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RESPONSE_TYPE:\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    167\u001b[0m         CBEventType\u001b[38;5;241m.\u001b[39mQUERY, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query_bundle\u001b[38;5;241m.\u001b[39mquery_str}\n\u001b[1;32m    168\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m query_event:\n\u001b[0;32m--> 169\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_metadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result\u001b[38;5;241m.\u001b[39minds) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    172\u001b[0m             responses \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/llama_index/core/base/base_selector.py:87\u001b[0m, in \u001b[0;36mBaseSelector.select\u001b[0;34m(self, choices, query)\u001b[0m\n\u001b[1;32m     85\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [_wrap_choice(choice) \u001b[38;5;28;01mfor\u001b[39;00m choice \u001b[38;5;129;01min\u001b[39;00m choices]\n\u001b[1;32m     86\u001b[0m query_bundle \u001b[38;5;241m=\u001b[39m _wrap_query(query)\n\u001b[0;32m---> 87\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_select\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchoices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/llama_index/core/selectors/llm_selectors.py:118\u001b[0m, in \u001b[0;36mLLMSingleSelector._select\u001b[0;34m(self, choices, query)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# parse output\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prompt\u001b[38;5;241m.\u001b[39moutput_parser \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m parse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _structured_output_to_selector_result(parse)\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/llama_index/core/output_parsers/selection.py:97\u001b[0m, in \u001b[0;36mSelectionOutputParser.parse\u001b[0;34m(self, output)\u001b[0m\n\u001b[1;32m     94\u001b[0m     json_obj \u001b[38;5;241m=\u001b[39m [json_obj]\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m json_obj:\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to convert output to JSON: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     99\u001b[0m json_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_output(json_obj)\n\u001b[1;32m    100\u001b[0m answers \u001b[38;5;241m=\u001b[39m [Answer\u001b[38;5;241m.\u001b[39mfrom_dict(json_dict) \u001b[38;5;28;01mfor\u001b[39;00m json_dict \u001b[38;5;129;01min\u001b[39;00m json_output]\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert output to JSON: ''"
     ]
    }
   ],
   "source": [
    "paper_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=paper_query_engine,\n",
    "    description=\"Useful for search for papers\",\n",
    ")\n",
    "ds_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=data_science_query_engine,\n",
    "    description=\"Useful for answering data science concepts\",\n",
    ")\n",
    "\n",
    "query_engine = RouterQueryEngine(\n",
    "    selector=LLMSingleSelector.from_defaults(),\n",
    "    query_engine_tools=[\n",
    "        paper_tool,\n",
    "        ds_tool,\n",
    "    ],\n",
    "    verbose=True\n",
    ")\n",
    "print(query_engine.query(\"What is linear regression?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
