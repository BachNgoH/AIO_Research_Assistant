{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import feedparser\n",
    "\n",
    "def get_daily_arxiv_papers():\n",
    "    max_results = 25\n",
    "    categories = [\"cs.AI\", \"cs.CV\", \"cs.IR\", \"cs.LG\", \"cs.CL\"]\n",
    "    base_url = \"http://export.arxiv.org/api/query?\"\n",
    "    all_categories = [f\"cat:{category}\" for category in categories]\n",
    "    search_query = \"+OR+\".join(all_categories)\n",
    "\n",
    "    paper_list = []\n",
    "    start = 0\n",
    "    today = datetime.utcnow().date()\n",
    "    new_papers_found = True\n",
    "    wait_time = 3\n",
    "\n",
    "    while new_papers_found:\n",
    "        query = f\"search_query={search_query}&start={start}&max_results={max_results}&sortBy=submittedDate&sortOrder=descending\"\n",
    "        response = requests.get(base_url + query)\n",
    "        feed = feedparser.parse(response.content)\n",
    "\n",
    "        for r in feed.entries:\n",
    "            paper_date = datetime.strptime(r[\"published\"][:10], \"%Y-%m-%d\").date()\n",
    "            if paper_date >= today - timedelta(1):\n",
    "                new_papers_found = True\n",
    "                paper_list.append(\n",
    "                    Document(\n",
    "                        text=f\"\"\"\n",
    "Title: {r['title']}\n",
    "{r['summary']}\n",
    "                \"\"\",\n",
    "                        metadata={\n",
    "                            \"paper_id\": r[\"id\"].split(\"/\")[-1],\n",
    "                            \"paper_link\": r[\"id\"],\n",
    "                            \"title\": r[\"title\"],\n",
    "                            \"date\": r[\"published\"][:10],\n",
    "                        },\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                new_papers_found = False\n",
    "                break\n",
    "\n",
    "        start += max_results\n",
    "        time.sleep(wait_time)\n",
    "\n",
    "    return paper_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_list = get_daily_arxiv_papers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import MetadataMode\n",
    "daily_paper_content = \"\\n===============\\n\".join([paper.get_content(MetadataMode.LLM) for paper in paper_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionResponse(text='Hi! ðŸ‘‹  What can I do for you today? ðŸ˜Š \\n', additional_kwargs={}, raw={'content': {'parts': [{'text': 'Hi! ðŸ‘‹  What can I do for you today? ðŸ˜Š \\n'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [], 'token_count': 0, 'grounding_attributions': [], 'block_reason': 0}, logprobs=None, delta=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.llms.gemini import Gemini\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "gemini = Gemini(model_name=\"models/gemini-1.5-flash-latest\", api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "gemini.complete(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You are a professional researcher in the field of AI. You are given a list of paper abstract in one day.\n",
    "Your job is to summarize the trends and note out some most interesting papers in the list. \n",
    "Give the link to the full paper in the report.\n",
    "===============\n",
    "{daily_paper_content}\n",
    "\"\"\"\n",
    "\n",
    "response = gemini.complete(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## AI Research Paper Trends: May 14, 2024\n",
       "\n",
       "This report summarizes trends and highlights interesting papers from a list of AI research abstracts published on May 14, 2024.\n",
       "\n",
       "**Trends:**\n",
       "\n",
       "* **Multimodal Learning:** There is a strong focus on multimodal learning, particularly combining vision and language. This includes papers on vision-language pre-training, scientific figure interpretation, and multimodal large language models for medical applications.\n",
       "* **Robustness and Generalization:** Many papers address the challenges of robustness and generalization in AI models. This includes work on out-of-distribution detection, domain generalization, and improving the robustness of models to data variations.\n",
       "* **Explainability and Fairness:**  There is growing interest in making AI models more explainable and fair. Papers explore methods for explaining model decisions, mitigating bias in medical image classification, and ensuring fairness in channel pruning.\n",
       "* **Reinforcement Learning:**  Reinforcement learning continues to be a popular area of research, with papers focusing on improving efficiency, addressing challenges in offline RL, and applying RL to real-world problems like autonomous intersection management and option hedging.\n",
       "* **Large Language Models:**  LLMs are being applied to a wide range of tasks, including text-to-image generation, scientific figure interpretation, and enhancing the accessibility of legal documents.\n",
       "\n",
       "**Most Interesting Papers:**\n",
       "\n",
       "* **SciFIBench: Benchmarking Large Multimodal Models for Scientific Figure Interpretation** ([http://arxiv.org/abs/2405.08807v1](http://arxiv.org/abs/2405.08807v1)): This paper introduces a new benchmark for evaluating the ability of large multimodal models to understand and interpret scientific figures. This is a crucial area for AI to assist scientific research.\n",
       "* **CinePile: A Long Video Question Answering Dataset and Benchmark** ([http://arxiv.org/abs/2405.08813v1](http://arxiv.org/abs/2405.08813v1)): This paper presents a new dataset and benchmark for long-form video understanding, which is a challenging task for current AI models. The dataset is designed to test genuine long-form comprehension, not just analyzing a few frames.\n",
       "* **EfficientTrain++: Generalized Curriculum Learning for Efficient Visual Backbone Training** ([http://arxiv.org/abs/2405.08768v1](http://arxiv.org/abs/2405.08768v1)): This paper proposes a novel approach to curriculum learning that significantly reduces the training time of visual backbones without sacrificing accuracy. This is a valuable contribution to making AI models more efficient.\n",
       "* **Hunyuan-DiT: A Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding** ([http://arxiv.org/abs/2405.08748v1](http://arxiv.org/abs/2405.08748v1)): This paper presents a new text-to-image diffusion transformer that demonstrates fine-grained understanding of both English and Chinese. This is a significant step towards developing more multilingual and culturally sensitive AI models.\n",
       "* **Promoting AI Equity in Science: Generalized Domain Prompt Learning for Accessible VLM Research** ([http://arxiv.org/abs/2405.08668v1](http://arxiv.org/abs/2405.08668v1)): This paper proposes a framework for adapting vision-language models to specialized domains without requiring extensive data or resources. This is a crucial step towards making AI research more accessible and equitable.\n",
       "\n",
       "**Overall, this day's research highlights the rapid progress being made in AI, particularly in areas like multimodal learning, robustness, explainability, and fairness. These advancements are paving the way for AI to play an increasingly important role in various fields, from scientific research to healthcare and beyond.** \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../outputs/daily_report_may_14.md\", \"w\") as f:\n",
    "    f.write(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
