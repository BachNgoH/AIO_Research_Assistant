{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Building Gemma Research Assistant**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../figures/GemmaAIO-main-image.webp\" alt=\"main-image\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../figures/RAG%20-%20Scientific%20Assistant%20-%20Frame%201.jpg\" alt=\"pipeline\" width=800/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Scientific Research Assistant with Graph**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.1 Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2455227, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0704.0001</td>\n",
       "      <td>Calculation of prompt diphoton production cros...</td>\n",
       "      <td>A fully differential calculation in perturba...</td>\n",
       "      <td>hep-ph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0704.0002</td>\n",
       "      <td>Sparsity-certifying Graph Decompositions</td>\n",
       "      <td>We describe a new algorithm, the $(k,\\ell)$-...</td>\n",
       "      <td>math.CO cs.CG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0704.0003</td>\n",
       "      <td>The evolution of the Earth-Moon system based o...</td>\n",
       "      <td>The evolution of Earth-Moon system is descri...</td>\n",
       "      <td>physics.gen-ph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0704.0004</td>\n",
       "      <td>A determinant of Stirling cycle numbers counts...</td>\n",
       "      <td>We show that a determinant of Stirling cycle...</td>\n",
       "      <td>math.CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0704.0005</td>\n",
       "      <td>From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...</td>\n",
       "      <td>In this paper we show how to compute the $\\L...</td>\n",
       "      <td>math.CA math.FA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              title  \\\n",
       "0  0704.0001  Calculation of prompt diphoton production cros...   \n",
       "1  0704.0002           Sparsity-certifying Graph Decompositions   \n",
       "2  0704.0003  The evolution of the Earth-Moon system based o...   \n",
       "3  0704.0004  A determinant of Stirling cycle numbers counts...   \n",
       "4  0704.0005  From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...   \n",
       "\n",
       "                                            abstract       categories  \n",
       "0    A fully differential calculation in perturba...           hep-ph  \n",
       "1    We describe a new algorithm, the $(k,\\ell)$-...    math.CO cs.CG  \n",
       "2    The evolution of Earth-Moon system is descri...   physics.gen-ph  \n",
       "3    We show that a determinant of Stirling cycle...          math.CO  \n",
       "4    In this paper we show how to compute the $\\L...  math.CA math.FA  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.kaggle.com/code/matthewmaddock/nlp-arxiv-dataset-transformers-and-umap\n",
    "\n",
    "# This takes about 1 minute.\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "cols = ['id', 'title', 'abstract', 'categories']\n",
    "data = []\n",
    "file_name = '../data/arxiv-metadata-oai-snapshot.json'\n",
    "\n",
    "\n",
    "with open(file_name, encoding='latin-1') as f:\n",
    "    for line in f:\n",
    "        doc = json.loads(line)\n",
    "        lst = [doc['id'], doc['title'], doc['abstract'], doc['categories']]\n",
    "        data.append(lst)\n",
    "\n",
    "df_data = pd.DataFrame(data=data, columns=cols)\n",
    "\n",
    "print(df_data.shape)\n",
    "\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0704.0047</td>\n",
       "      <td>Intelligent location of simultaneously active ...</td>\n",
       "      <td>The intelligent acoustic emission locator is...</td>\n",
       "      <td>cs.NE cs.AI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0704.0050</td>\n",
       "      <td>Intelligent location of simultaneously active ...</td>\n",
       "      <td>Part I describes an intelligent acoustic emi...</td>\n",
       "      <td>cs.NE cs.AI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>0704.0304</td>\n",
       "      <td>The World as Evolving Information</td>\n",
       "      <td>This paper discusses the benefits of describ...</td>\n",
       "      <td>cs.IT cs.AI math.IT q-bio.PE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>0704.0671</td>\n",
       "      <td>Learning from compressed observations</td>\n",
       "      <td>The problem of statistical learning is to co...</td>\n",
       "      <td>cs.IT cs.LG math.IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>0704.0954</td>\n",
       "      <td>Sensor Networks with Random Links: Topology De...</td>\n",
       "      <td>In a sensor network, in practice, the commun...</td>\n",
       "      <td>cs.IT cs.LG math.IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2443613</th>\n",
       "      <td>quant-ph/0411140</td>\n",
       "      <td>Improved Bounds on Quantum Learning Algorithms</td>\n",
       "      <td>In this article we give several new results ...</td>\n",
       "      <td>quant-ph cs.LG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2445483</th>\n",
       "      <td>quant-ph/0507231</td>\n",
       "      <td>Algebras of Measurements: the logical structur...</td>\n",
       "      <td>In Quantum Physics, a measurement is represe...</td>\n",
       "      <td>quant-ph cs.AI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2448330</th>\n",
       "      <td>quant-ph/0607111</td>\n",
       "      <td>`Plausibilities of plausibilities': an approac...</td>\n",
       "      <td>Probability-like parameters appearing in som...</td>\n",
       "      <td>quant-ph cs.AI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2450042</th>\n",
       "      <td>quant-ph/0702072</td>\n",
       "      <td>Markovian Entanglement Networks</td>\n",
       "      <td>Graphical models of probabilistic dependenci...</td>\n",
       "      <td>quant-ph cs.AI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2452122</th>\n",
       "      <td>quant-ph/9802028</td>\n",
       "      <td>Analogue Quantum Computers for Data Analysis</td>\n",
       "      <td>Analogue computers use continuous properties...</td>\n",
       "      <td>quant-ph cs.CV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>336892 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id                                              title  \\\n",
       "46              0704.0047  Intelligent location of simultaneously active ...   \n",
       "49              0704.0050  Intelligent location of simultaneously active ...   \n",
       "303             0704.0304                  The World as Evolving Information   \n",
       "670             0704.0671              Learning from compressed observations   \n",
       "953             0704.0954  Sensor Networks with Random Links: Topology De...   \n",
       "...                   ...                                                ...   \n",
       "2443613  quant-ph/0411140     Improved Bounds on Quantum Learning Algorithms   \n",
       "2445483  quant-ph/0507231  Algebras of Measurements: the logical structur...   \n",
       "2448330  quant-ph/0607111  `Plausibilities of plausibilities': an approac...   \n",
       "2450042  quant-ph/0702072                    Markovian Entanglement Networks   \n",
       "2452122  quant-ph/9802028       Analogue Quantum Computers for Data Analysis   \n",
       "\n",
       "                                                  abstract  \\\n",
       "46         The intelligent acoustic emission locator is...   \n",
       "49         Part I describes an intelligent acoustic emi...   \n",
       "303        This paper discusses the benefits of describ...   \n",
       "670        The problem of statistical learning is to co...   \n",
       "953        In a sensor network, in practice, the commun...   \n",
       "...                                                    ...   \n",
       "2443613    In this article we give several new results ...   \n",
       "2445483    In Quantum Physics, a measurement is represe...   \n",
       "2448330    Probability-like parameters appearing in som...   \n",
       "2450042    Graphical models of probabilistic dependenci...   \n",
       "2452122    Analogue computers use continuous properties...   \n",
       "\n",
       "                           categories  \n",
       "46                        cs.NE cs.AI  \n",
       "49                        cs.NE cs.AI  \n",
       "303      cs.IT cs.AI math.IT q-bio.PE  \n",
       "670               cs.IT cs.LG math.IT  \n",
       "953               cs.IT cs.LG math.IT  \n",
       "...                               ...  \n",
       "2443613                quant-ph cs.LG  \n",
       "2445483                quant-ph cs.AI  \n",
       "2448330                quant-ph cs.AI  \n",
       "2450042                quant-ph cs.AI  \n",
       "2452122                quant-ph cs.CV  \n",
       "\n",
       "[336892 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df_data is your DataFrame and topics is your list of topics of interest\n",
    "topics = ['cs.AI', 'cs.CV', 'cs.IR', 'cs.LG', 'cs.CL']\n",
    "\n",
    "# Create a regular expression pattern that matches any of the topics\n",
    "# The pattern will look like 'cs.AI|cs.CV|cs.IR|cs.LG|cs.CL'\n",
    "pattern = '|'.join(topics)\n",
    "\n",
    "# Filter the DataFrame to include rows where the 'categories' column contains any of the topics\n",
    "# The na=False parameter makes sure that NaN values are treated as False\n",
    "df_filtered = df_data[df_data['categories'].str.contains(pattern, na=False)]\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                    0903.0465\n",
       "title                                   Breaking Value Symmetry\n",
       "abstract        Symmetry is an important factor in solving m...\n",
       "categories                                                cs.AI\n",
       "Name: 111584, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.iloc[1002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                    0707.0705\n",
       "title         Optimal Solutions for Sparse Principal Compone...\n",
       "abstract        Given a sample covariance matrix, we examine...\n",
       "categories                                          cs.AI cs.LG\n",
       "Name: 13875, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.iloc[110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336892\n"
     ]
    }
   ],
   "source": [
    "df_data = df_filtered\n",
    "print(len(df_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>categories</th>\n",
       "      <th>prepared_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0704.0047</td>\n",
       "      <td>Intelligent location of simultaneously active ...</td>\n",
       "      <td>The intelligent acoustic emission locator is d...</td>\n",
       "      <td>cs.NE cs.AI</td>\n",
       "      <td>Intelligent location of simultaneously active ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0704.0050</td>\n",
       "      <td>Intelligent location of simultaneously active ...</td>\n",
       "      <td>Part I describes an intelligent acoustic emiss...</td>\n",
       "      <td>cs.NE cs.AI</td>\n",
       "      <td>Intelligent location of simultaneously active ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>0704.0304</td>\n",
       "      <td>The World as Evolving Information</td>\n",
       "      <td>This paper discusses the benefits of describin...</td>\n",
       "      <td>cs.IT cs.AI math.IT q-bio.PE</td>\n",
       "      <td>The World as Evolving Information \\n This pape...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>0704.0671</td>\n",
       "      <td>Learning from compressed observations</td>\n",
       "      <td>The problem of statistical learning is to cons...</td>\n",
       "      <td>cs.IT cs.LG math.IT</td>\n",
       "      <td>Learning from compressed observations \\n The p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>0704.0954</td>\n",
       "      <td>Sensor Networks with Random Links: Topology De...</td>\n",
       "      <td>In a sensor network, in practice, the communic...</td>\n",
       "      <td>cs.IT cs.LG math.IT</td>\n",
       "      <td>Sensor Networks with Random Links: Topology De...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                              title  \\\n",
       "46   0704.0047  Intelligent location of simultaneously active ...   \n",
       "49   0704.0050  Intelligent location of simultaneously active ...   \n",
       "303  0704.0304                  The World as Evolving Information   \n",
       "670  0704.0671              Learning from compressed observations   \n",
       "953  0704.0954  Sensor Networks with Random Links: Topology De...   \n",
       "\n",
       "                                              abstract  \\\n",
       "46   The intelligent acoustic emission locator is d...   \n",
       "49   Part I describes an intelligent acoustic emiss...   \n",
       "303  This paper discusses the benefits of describin...   \n",
       "670  The problem of statistical learning is to cons...   \n",
       "953  In a sensor network, in practice, the communic...   \n",
       "\n",
       "                       categories  \\\n",
       "46                    cs.NE cs.AI   \n",
       "49                    cs.NE cs.AI   \n",
       "303  cs.IT cs.AI math.IT q-bio.PE   \n",
       "670           cs.IT cs.LG math.IT   \n",
       "953           cs.IT cs.LG math.IT   \n",
       "\n",
       "                                         prepared_text  \n",
       "46   Intelligent location of simultaneously active ...  \n",
       "49   Intelligent location of simultaneously active ...  \n",
       "303  The World as Evolving Information \\n This pape...  \n",
       "670  Learning from compressed observations \\n The p...  \n",
       "953  Sensor Networks with Random Links: Topology De...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(x):\n",
    "    \n",
    "    # Replace newline characters with a space\n",
    "    new_text = \" \".join([c.strip() for c in x.replace(\"\\n\", \"\").split()])\n",
    "    # Remove leading and trailing spaces\n",
    "    new_text = new_text.strip()\n",
    "    \n",
    "    return new_text\n",
    "\n",
    "df_data['title'] = df_data['title'].apply(clean_text)\n",
    "df_data['abstract'] = df_data['abstract'].apply(clean_text)\n",
    "\n",
    "df_data['prepared_text'] = df_data['title'] + ' \\n ' + df_data['abstract']\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gb2t/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "arxiv_documents = [Document(text=prepared_text, doc_id=id) for prepared_text,id in list(zip(df_data['prepared_text'], df_data['id']))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.2 Creating Index**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `VectorStoreIndex` is by far the most frequently used type of Index in llamaindex. This class takes your Documents and splits them up into Nodes. Then, it creates `vector_embeddings` of the text of every node. But what is `vector_embedding`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector embeddings are like turning the essence of your words into a mathematical sketch. Imagine every idea or concept in your text getting its unique numerical fingerprint. This is handy because even if two snippets of text use different words, if they're sharing the same idea, their numerical sketchesâ€”or embeddingsâ€”will be close neighbors in the numerical space. This magic is done using tools known as embedding models.\n",
    "\n",
    "Choosing the right embedding model is crucial. It's like picking the right artist to paint your portrait; you want the one who captures you best. A great place to start is the MTEB leaderboard, where the crÃ¨me de la crÃ¨me of embedding models are ranked. As we have quite a large dataset, the model size matters, we don't want to wait all day for the model to extract all the vector embeddings. When I last checked, the `BAAI/bge-small-en-v1.5` model was leading the pack, especially considering its size. It could be a solid choice if you're diving into the world of text embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "import chromadb\n",
    "import torch\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "Settings.llm = None\n",
    "# Create embed model\n",
    "device_type = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\", cache_folder=\"../models\", device=device_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we have to find somewhere to store all of the embeddings extracted by the model, and that's why we need a `vector store`. There are many to choose from, in this tutorial, I will choose the `chroma` vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.PersistentClient(path=\"../DB/papers\")\n",
    "chroma_collection = chroma_client.get_or_create_collection(\"gemma_assistant_arxiv\")\n",
    "\n",
    "\n",
    "# Create vector store\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 109131/109131 [01:07<00:00, 1620.57it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:07<00:00, 261.06it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:07<00:00, 273.76it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:08<00:00, 252.36it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:08<00:00, 250.44it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:08<00:00, 242.92it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:08<00:00, 236.97it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:08<00:00, 237.07it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:08<00:00, 241.30it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:08<00:00, 242.83it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:08<00:00, 235.62it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:08<00:00, 243.54it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:08<00:00, 242.94it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:08<00:00, 240.59it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:08<00:00, 233.25it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:08<00:00, 232.32it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:08<00:00, 237.72it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:08<00:00, 243.29it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:08<00:00, 238.90it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:08<00:00, 240.54it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:08<00:00, 233.28it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:08<00:00, 228.39it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:08<00:00, 236.91it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:08<00:00, 233.98it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:08<00:00, 228.93it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:08<00:00, 232.90it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:08<00:00, 232.84it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:08<00:00, 234.67it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:08<00:00, 227.91it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:08<00:00, 230.81it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:08<00:00, 230.62it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:09<00:00, 226.20it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:09<00:00, 224.43it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:09<00:00, 224.05it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:09<00:00, 224.95it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:09<00:00, 223.76it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:09<00:00, 223.92it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:09<00:00, 224.18it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:09<00:00, 218.88it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:09<00:00, 224.89it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:09<00:00, 222.41it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:09<00:00, 221.49it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:08<00:00, 229.63it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:09<00:00, 226.35it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:09<00:00, 223.87it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:09<00:00, 221.53it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:09<00:00, 222.96it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:09<00:00, 220.67it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:09<00:00, 226.73it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:09<00:00, 227.23it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:09<00:00, 221.40it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:09<00:00, 224.75it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:09<00:00, 227.43it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:08<00:00, 233.61it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 587/587 [00:02<00:00, 262.21it/s]\n"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "    arxiv_documents, storage_context=storage_context, embed_model=embed_model, show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.3 Loading from arxiv vector store**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "import torch\n",
    "\n",
    "\n",
    "Settings.llm = None # Set this to none to make the index only do retrieval\n",
    "device_type = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\", cache_folder=\"../models\", device=device_type) # must be the same as the previous stage\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=\"../DB/papers\")\n",
    "chroma_collection = chroma_client.get_or_create_collection(\"gemma_assistant_arxiv\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "# load the vectorstore\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_vector_store(vector_store, storage_context=storage_context, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_query_engine = index.as_query_engine(\n",
    "    similarity_top_k=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "Frame by Familiar Frame: Understanding Replication in Video Diffusion   Models \n",
      " Building on the momentum of image generation diffusion models, there is an increasing interest in video-based diffusion models. However, video generation poses greater challenges due to its higher-dimensional nature, the scarcity of training data, and the complex spatiotemporal relationships involved. Image generation models, due to their extensive data requirements, have already strained computational resources to their limits. There have been instances of these models reproducing elements from the training samples, leading to concerns and even legal disputes over sample replication. Video diffusion models, which operate with even more constrained datasets and are tasked with generating both spatial and temporal content, may be more prone to replicating samples from their training sets. Compounding the issue, these models are often evaluated using metrics that inadvertently reward replication. In our paper, we present a systematic investigation into the phenomenon of sample replication in video diffusion models. We scrutinize various recent diffusion models for video synthesis, assessing their tendency to replicate spatial and temporal content in both unconditional and conditional generation scenarios. Our study identifies strategies that are less likely to lead to replication. Furthermore, we propose new evaluation strategies that take replication into account, offering a more accurate measure of a model's ability to generate the original content.\n",
      "\n",
      "Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large   Datasets \n",
      " We present Stable Video Diffusion - a latent video diffusion model for high-resolution, state-of-the-art text-to-video and image-to-video generation. Recently, latent diffusion models trained for 2D image synthesis have been turned into generative video models by inserting temporal layers and finetuning them on small, high-quality video datasets. However, training methods in the literature vary widely, and the field has yet to agree on a unified strategy for curating video data. In this paper, we identify and evaluate three different stages for successful training of video LDMs: text-to-image pretraining, video pretraining, and high-quality video finetuning. Furthermore, we demonstrate the necessity of a well-curated pretraining dataset for generating high-quality videos and present a systematic curation process to train a strong base model, including captioning and filtering strategies. We then explore the impact of finetuning our base model on high-quality data and train a text-to-video model that is competitive with closed-source video generation. We also show that our base model provides a powerful motion representation for downstream tasks such as image-to-video generation and adaptability to camera motion-specific LoRA modules. Finally, we demonstrate that our model provides a strong multi-view 3D-prior and can serve as a base to finetune a multi-view diffusion model that jointly generates multiple views of objects in a feedforward fashion, outperforming image-based methods at a fraction of their compute budget. We release code and model weights at https://github.com/Stability-AI/generative-models .\n",
      "\n",
      "Diffusion Models for Time Series Applications: A Survey \n",
      " Diffusion models, a family of generative models based on deep learning, have become increasingly prominent in cutting-edge machine learning research. With a distinguished performance in generating samples that resemble the observed data, diffusion models are widely used in image, video, and text synthesis nowadays. In recent years, the concept of diffusion has been extended to time series applications, and many powerful models have been developed. Considering the deficiency of a methodical summary and discourse on these models, we provide this survey as an elementary resource for new researchers in this area and also an inspiration to motivate future research. For better understanding, we include an introduction about the basics of diffusion models. Except for this, we primarily focus on diffusion-based methods for time series forecasting, imputation, and generation, and present them respectively in three individual sections. We also compare different methods for the same application and highlight their connections if applicable. Lastly, we conclude the common limitation of diffusion-based methods and highlight potential future research directions.\n",
      "\n",
      "MeDM: Mediating Image Diffusion Models for Video-to-Video Translation   with Temporal Correspondence Guidance \n",
      " This study introduces an efficient and effective method, MeDM, that utilizes pre-trained image Diffusion Models for video-to-video translation with consistent temporal flow. The proposed framework can render videos from scene position information, such as a normal G-buffer, or perform text-guided editing on videos captured in real-world scenarios. We employ explicit optical flows to construct a practical coding that enforces physical constraints on generated frames and mediates independent frame-wise scores. By leveraging this coding, maintaining temporal consistency in the generated videos can be framed as an optimization problem with a closed-form solution. To ensure compatibility with Stable Diffusion, we also suggest a workaround for modifying observation-space scores in latent Diffusion Models. Notably, MeDM does not require fine-tuning or test-time optimization of the Diffusion Models. Through extensive qualitative, quantitative, and subjective experiments on various benchmarks, the study demonstrates the effectiveness and superiority of the proposed approach. Our project page can be found at https://medm2023.github.io\n",
      "\n",
      "VideoFusion: Decomposed Diffusion Models for High-Quality Video   Generation \n",
      " A diffusion probabilistic model (DPM), which constructs a forward diffusion process by gradually adding noise to data points and learns the reverse denoising process to generate new samples, has been shown to handle complex data distribution. Despite its recent success in image synthesis, applying DPMs to video generation is still challenging due to high-dimensional data spaces. Previous methods usually adopt a standard diffusion process, where frames in the same video clip are destroyed with independent noises, ignoring the content redundancy and temporal correlation. This work presents a decomposed diffusion process via resolving the per-frame noise into a base noise that is shared among all frames and a residual noise that varies along the time axis. The denoising pipeline employs two jointly-learned networks to match the noise decomposition accordingly. Experiments on various datasets confirm that our approach, termed as VideoFusion, surpasses both GAN-based and diffusion-based alternatives in high-quality video generation. We further show that our decomposed formulation can benefit from pre-trained image diffusion models and well-support text-conditioned video creation.\n",
      "\n",
      "Exploring Diffusion Models for Unsupervised Video Anomaly Detection \n",
      " This paper investigates the performance of diffusion models for video anomaly detection (VAD) within the most challenging but also the most operational scenario in which the data annotations are not used. As being sparse, diverse, contextual, and often ambiguous, detecting abnormal events precisely is a very ambitious task. To this end, we rely only on the information-rich spatio-temporal data, and the reconstruction power of the diffusion models such that a high reconstruction error is utilized to decide the abnormality. Experiments performed on two large-scale video anomaly detection datasets demonstrate the consistent improvement of the proposed method over the state-of-the-art generative models while in some cases our method achieves better scores than the more complex models. This is the first study using a diffusion model and examining its parameters' influence to present guidance for VAD in surveillance scenarios.\n",
      "\n",
      "VIDM: Video Implicit Diffusion Models \n",
      " Diffusion models have emerged as a powerful generative method for synthesizing high-quality and diverse set of images. In this paper, we propose a video generation method based on diffusion models, where the effects of motion are modeled in an implicit condition manner, i.e. one can sample plausible video motions according to the latent feature of frames. We improve the quality of the generated videos by proposing multiple strategies such as sampling space truncation, robustness penalty, and positional group normalization. Various experiments are conducted on datasets consisting of videos with different resolutions and different number of frames. Results show that the proposed method outperforms the state-of-the-art generative adversarial network-based methods by a significant margin in terms of FVD scores as well as perceptible visual quality.\n",
      "\n",
      "Diffusion$^2$: Dynamic 3D Content Generation via Score Composition of   Orthogonal Diffusion Models \n",
      " Recent advancements in 3D generation are predominantly propelled by improvements in 3D-aware image diffusion models which are pretrained on Internet-scale image data and fine-tuned on massive 3D data, offering the capability of producing highly consistent multi-view images. However, due to the scarcity of synchronized multi-view video data, it is impractical to adapt this paradigm to 4D generation directly. Despite that, the available video and 3D data are adequate for training video and multi-view diffusion models that can provide satisfactory dynamic and geometric priors respectively. In this paper, we present Diffusion$^2$, a novel framework for dynamic 3D content creation that leverages the knowledge about geometric consistency and temporal smoothness from these models to directly sample dense multi-view and multi-frame images which can be employed to optimize continuous 4D representation. Specifically, we design a simple yet effective denoising strategy via score composition of video and multi-view diffusion models based on the probability structure of the images to be generated. Owing to the high parallelism of the image generation and the efficiency of the modern 4D reconstruction pipeline, our framework can generate 4D content within few minutes. Furthermore, our method circumvents the reliance on 4D data, thereby having the potential to benefit from the scalability of the foundation video and multi-view diffusion models. Extensive experiments demonstrate the efficacy of our proposed framework and its capability to flexibly adapt to various types of prompts.\n",
      "\n",
      "Dreamix: Video Diffusion Models are General Video Editors \n",
      " Text-driven image and video diffusion models have recently achieved unprecedented generation realism. While diffusion models have been successfully applied for image editing, very few works have done so for video editing. We present the first diffusion-based method that is able to perform text-based motion and appearance editing of general videos. Our approach uses a video diffusion model to combine, at inference time, the low-resolution spatio-temporal information from the original video with new, high resolution information that it synthesized to align with the guiding text prompt. As obtaining high-fidelity to the original video requires retaining some of its high-resolution information, we add a preliminary stage of finetuning the model on the original video, significantly boosting fidelity. We propose to improve motion editability by a new, mixed objective that jointly finetunes with full temporal attention and with temporal attention masking. We further introduce a new framework for image animation. We first transform the image into a coarse video by simple image processing operations such as replication and perspective geometric projections, and then use our general video editor to animate it. As a further application, we can use our method for subject-driven video generation. Extensive qualitative and numerical experiments showcase the remarkable editing ability of our method and establish its superior performance compared to baseline methods.\n",
      "\n",
      "Grid Diffusion Models for Text-to-Video Generation \n",
      " Recent advances in the diffusion models have significantly improved text-to-image generation. However, generating videos from text is a more challenging task than generating images from text, due to the much larger dataset and higher computational cost required. Most existing video generation methods use either a 3D U-Net architecture that considers the temporal dimension or autoregressive generation. These methods require large datasets and are limited in terms of computational costs compared to text-to-image generation. To tackle these challenges, we propose a simple but effective novel grid diffusion for text-to-video generation without temporal dimension in architecture and a large text-video paired dataset. We can generate a high-quality video using a fixed amount of GPU memory regardless of the number of frames by representing the video as a grid image. Additionally, since our method reduces the dimensions of the video to the dimensions of the image, various image-based methods can be applied to videos, such as text-guided video manipulation from image manipulation. Our proposed method outperforms the existing methods in both quantitative and qualitative evaluations, demonstrating the suitability of our model for real-world video generation.\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: What are some papers related to diffusion models for video data?\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "print(paper_query_engine.query(\"What are some papers related to diffusion models for video data?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Graph-based paper relationship search**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.1 Download pre-extracted citation data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "parsed_article = load_dataset(\"BachNgoH/ParsedArxivPapers\")['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'authors', 'pub_date', 'abstract', 'sections', 'references', 'figures', 'formulas', 'doi', 'citation_data'],\n",
       "    num_rows: 19454\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Category': 'Extension or Continuation',\n",
       "  'Citation': '(Han et al., 2018)',\n",
       "  'Explanation': 'The cited work introduces the concept of few-shot RE, which the citing paper further explores and builds upon by delving into new relation types and directions in the field.'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': '(Doddington et al., 2004)',\n",
       "  'Explanation': 'The cited work, ACE dataset, serves as the origin of multi-lingual RE datasets, providing the foundational basis for subsequent research in exploring cross-lingual relations.'},\n",
       " {'Category': 'Extension or Continuation',\n",
       "  'Citation': '(Seganti et al., 2021)',\n",
       "  'Explanation': 'The cited work contributes a multi-lingual dataset that the citing paper extends by incorporating entity translations and Wikipedia alignments, thereby expanding the scope of multi-lingual RE research.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Elsahar et al., 2018)',\n",
       "  'Explanation': 'The cited work, TREx, provides the template for triplets in distantly supervised RE, which the citing paper adopts to investigate the knowledge present in pre-trained language models across multiple languages.'},\n",
       " {'Category': 'Extension or Continuation',\n",
       "  'Citation': '(KÃ¶ksal and Ã–zgÃ¼r, 2020)',\n",
       "  'Explanation': 'The cited work introduces new datasets for distantly supervised RE in multiple languages, which the citing paper continues by expanding the dataset coverage to include additional languages beyond English.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Bassignana and Plank, 2022a)',\n",
       "  'Explanation': 'The cited work, CrossRE, serves as the foundation for the MULTI-CROSSRE dataset proposed in the citing paper, where the latter builds upon the manually-annotated multi-domain RE corpus to create a more diverse and extensive dataset.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Bassignana and Plank, 2022a)',\n",
       "  'Explanation': 'The citing paper adopts the methodology of using machine-translated datasets in training and evaluation of NLP methods, as established by Bassignana and Plank (2022a).'},\n",
       " {'Category': 'Supporting Evidence',\n",
       "  'Citation': '(Liu et al., 2021)',\n",
       "  'Explanation': 'The cited work, CrossNER by Liu et al. (2021), serves as the basis for the manual annotation process of the CrossRE dataset, providing foundational Named Entity Recognition (NER) data.'},\n",
       " {'Category': 'Supporting Evidence',\n",
       "  'Citation': '(Conneau et al., 2018; Kassner et al., 2021)',\n",
       "  'Explanation': 'The cited works by Conneau et al. (2018) and Kassner et al. (2021) highlight the importance and standard practice of using machine-translated datasets in NLP methods, supporting the approach taken in the citing paper.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Chen et al., 2022)',\n",
       "  'Explanation': 'The citing paper leverages the methods developed by Chen et al. (2022) for transferring span information between source and target texts, which is crucial for the machine translation process discussed.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(DeepL,4)',\n",
       "  'Explanation': 'The citing paper utilizes the commercial machine translation service DeepL for translating document markup, which is essential for transferring named entities in the CrossRE dataset.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Han et al., 2018)',\n",
       "  'Explanation': 'The citing paper adopts the relation classification task setup from the cited work by Han et al., 2018, to assign correct relation types to entity pairs in the study.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Devlin et al., 2019)',\n",
       "  'Explanation': 'The citing paper adopts the use of the mono-lingual BERT language encoder from Devlin et al. (2019) as the basis for their baseline experiments.'},\n",
       " {'Category': 'Extension or Continuation',\n",
       "  'Citation': '(Conneau et al., 2020)',\n",
       "  'Explanation': 'The citing paper extends the research by using the multi-lingual XLM-R large language encoder from Conneau et al. (2020) to compare the original baseline results with the results on their MULTI-CROSSRE dataset.'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': '(Littell et al., 2017)',\n",
       "  'Explanation': 'The citing paper relies on lang2vec distance calculations from Littell et al. (2017) to sort languages by increasing distance to English, which is crucial for their analysis.'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_article[200]['citation_data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the number of annotated papers for now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_article = [x for x in parsed_article if x['citation_data'] is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotated Papers:  377\n"
     ]
    }
   ],
   "source": [
    "print(\"Annotated Papers: \", len(annotated_article))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2 Parsing generated data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From my observation, there are 2 main citation styles in AI papers, Author-year style and Numeric style:\n",
    "\n",
    "Example of Author-year style:\n",
    "- (Bassignana and Plank, 2022a) \n",
    "- (Liu et al., 2021)\n",
    "- (KÃ¶ksal and Ã–zgÃ¼r, 2020)\n",
    "\n",
    "Example of Numeric style:\n",
    "- [1], [2], [3]\n",
    "- [2, 56, 67]\n",
    "- [7 - 9]\n",
    "\n",
    "Therefore, we need different strategy to handle each style of citation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2.1 Handle Author-Year citation style**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling this citation style can be quite frustrating. Initially, we must separate combined citations like (Liu et al., 2021; Littell et al., 201) into individual entries. Then, we need to identify the first author and publication year. Subsequently, we have to locate the corresponding reference within our reference list based on the author's name and publication year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Heindorf et al. 2020',\n",
       " 'Heindorf et al. 2020',\n",
       " 'Tan, Zuo, and Ng 2023',\n",
       " '(Radinsky, Davidovich, and Markovitch 2012)',\n",
       " '(Sia, Dalmia, and Mielke 2020; Zhang et al. 2022)',\n",
       " '(Heindorf et al. 2020)',\n",
       " '(Culotta and Sorensen 2004; Bunescu and Mooney 2005; Ittoo and Bouma 2013)',\n",
       " '(Chen and Manning 2014; Schuster and Manning 2016)',\n",
       " '(Heindorf et al. 2020)',\n",
       " '(Tan, Zuo, and Ng 2023)',\n",
       " '(Sia, Dalmia, and Mielke 2020)',\n",
       " '(Zhang et al. 2022)',\n",
       " '(Finkel, Grenager, and Manning 2005)',\n",
       " '(Gao, Yao, and Chen 2021)',\n",
       " '(Zhang et al. 2022)']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse annotated articles\n",
    "import re\n",
    "\n",
    "# Function to normalize author names for comparison\n",
    "def normalize_author_name(name):\n",
    "    # Convert to lowercase and remove middle initials\n",
    "    name = name.lower()\n",
    "    name = re.sub(r\"\\s+[a-z]\\.\", \"\", name)  # Remove middle initials\n",
    "    return name\n",
    "\n",
    "\n",
    "citation_names = [c['Citation'] for c in annotated_article[0]['citation_data']]\n",
    "citation_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refined function to identify and normalize the first author from a citation\n",
    "def identify_and_normalize_first_author(citation_authors):\n",
    "    # Check for 'et al.' and 'and' to find the first author\n",
    "    if 'et al.' in citation_authors:\n",
    "        first_author = citation_authors.split('et al.')[0].strip()\n",
    "    elif ' and ' in citation_authors:\n",
    "        first_author = citation_authors.rsplit(' and ', 1)[0].split(',')[0].strip()\n",
    "    else:\n",
    "        first_author = citation_authors.split(',')[0].strip()\n",
    "    # Normalize the first author's name for comparison\n",
    "    return first_author.lower()\n",
    "\n",
    "\n",
    "# Function to split and parse citations in cases of citation \n",
    "# like (Culotta and Sorensen 2004; Bunescu and Mooney 2005; Ittoo and Bouma 2013)\n",
    "def split_and_parse_citation(citation):\n",
    "\n",
    "    # Remove outer parentheses\n",
    "    citation = citation.strip(\"()\")\n",
    "    # Split on semicolon if it's present, indicating multiple citations within one\n",
    "    if ';' in citation:\n",
    "        sub_citations = citation.split(';')\n",
    "    else:\n",
    "        sub_citations = [citation]\n",
    "    \n",
    "    # Parse each sub-citation for author names and year\n",
    "    for sub_citation in sub_citations:\n",
    "        # Splitting based on the last occurrence of space which is assumed to be before the year\n",
    "        *authors, year = sub_citation.rsplit(' ', 1)\n",
    "        authors = ' '.join(authors)  # Joining back the authors in case there are multiple names\n",
    "        parsed_citation = {'Author': identify_and_normalize_first_author(authors), 'Year': year}\n",
    "    \n",
    "    return parsed_citation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'authors': 'A Aziz; M A Hossain; A N Chy',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b0',\n",
       "  'title': 'CSECU-DSG @ Causal News Corpus 2022: Fusion of RoBERTa Transformers Variants for Causal Event Classification',\n",
       "  'year': '2022'},\n",
       " {'authors': 'Abu Dhabi',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b1',\n",
       "  'title': 'United Arab Emirates',\n",
       "  'year': ''},\n",
       " {'authors': 'R Bunescu; R Mooney',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b2',\n",
       "  'title': 'A Shortest Path Dependency Kernel for Relation Extraction',\n",
       "  'year': '2005'},\n",
       " {'authors': 'P Cao; X Zuo; Y Chen; K Liu; J Zhao; Y Chen; W Peng',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b3',\n",
       "  'title': 'Knowledge-Enriched Event Causality Identification via Latent Structure Induction Networks',\n",
       "  'year': '2021'},\n",
       " {'authors': 'T Caselli; P Vossen',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b4',\n",
       "  'title': 'The Event StoryLine Corpus: A New Benchmark for Causal and Temporal Relation Extraction',\n",
       "  'year': '2017'},\n",
       " {'authors': 'D Chen; C Manning',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b5',\n",
       "  'title': 'A Fast and Accurate Dependency Parser using Neural Networks',\n",
       "  'year': '2014'},\n",
       " {'authors': 'M Chen; Y Cao; K Deng; M Li; K Wang; J Shao; Y Zhang',\n",
       "  'journal': 'International Committee on Computational Linguistics',\n",
       "  'ref_id': 'b6',\n",
       "  'title': 'ERGO: Event Relational Graph Transformer for Document-level Event Causality Identification',\n",
       "  'year': '2022'},\n",
       " {'authors': 'X Chen; G Zhang; A Nik; M Li; J Fu',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b7',\n",
       "  'title': '1Cademy @ Causal News Corpus 2022: Enhance Causal Span Detection via Beam-Search-based Position Selector',\n",
       "  'year': '2022'},\n",
       " {'authors': 'Abu Dhabi',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b8',\n",
       "  'title': 'United Arab Emirates',\n",
       "  'year': ''},\n",
       " {'authors': 'A Culotta; J Sorensen',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b9',\n",
       "  'title': 'Dependency Tree Kernels for Relation Extraction',\n",
       "  'year': '2004'},\n",
       " {'authors': 'J Devlin; M.-W Chang; K Lee; K Toutanova',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b10',\n",
       "  'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding',\n",
       "  'year': '2019'},\n",
       " {'authors': 'J Dunietz; L Levin; J Carbonell',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b11',\n",
       "  'title': 'The BE-CauSE Corpus 2.0: Annotating Causality and Overlapping Relations',\n",
       "  'year': '2017'},\n",
       " {'authors': 'J R Finkel; T Grenager; C Manning',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b12',\n",
       "  'title': 'Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling',\n",
       "  'year': '2005'},\n",
       " {'authors': 'T Gao; X Yao; D Chen',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b13',\n",
       "  'title': 'SimCSE: Simple Contrastive Learning of Sentence Embeddings',\n",
       "  'year': '2021'},\n",
       " {'authors': 'O Hassanzadeh',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b14',\n",
       "  'title': 'Building a Knowledge Graph of Events and Consequences Using Wikidata',\n",
       "  'year': '2021-10-24'},\n",
       " {'authors': 'L He; S Zheng; T Yang; F Zhang',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b15',\n",
       "  'title': 'KLMo: Knowledge Graph Enhanced Pretrained Language Model with Fine-Grained Relationships',\n",
       "  'year': '2021'},\n",
       " {'authors': 'S Heindorf; Y Scholten; H Wachsmuth; A N Ngomo; M Potthast',\n",
       "  'journal': 'ACM',\n",
       "  'ref_id': 'b16',\n",
       "  'title': 'CauseNet: Towards a Causality Graph Extracted from the Web',\n",
       "  'year': '2020-10-19'},\n",
       " {'authors': 'I Hendrickx; S N Kim; Z Kozareva; P Nakov; D SÃ©aghdha; S PadÃ³; M Pennacchiotti; L Romano; S Szpakowicz',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b17',\n",
       "  'title': 'SemEval-2010 Task 8: Multi-Way Classification of Semantic Relations between Pairs of Nominals',\n",
       "  'year': '2010'},\n",
       " {'authors': 'C Hidey; K Mckeown',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b18',\n",
       "  'title': 'Identifying Causal Relations Using Parallel Wikipedia Articles',\n",
       "  'year': '2016'},\n",
       " {'authors': 'A Ittoo; G Bouma',\n",
       "  'journal': 'Data Knowl. Eng',\n",
       "  'ref_id': 'b19',\n",
       "  'title': 'Minimally-supervised learning of domain-specific causal relations using an opendomain corpus as knowledge base',\n",
       "  'year': '2013'},\n",
       " {'authors': 'K Izumi; H Sakaji',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b20',\n",
       "  'title': 'Economic Causal-Chain Search using Text Mining Technology',\n",
       "  'year': '2019'},\n",
       " {'authors': 'P Mirza; R Sprugnoli; S Tonelli; M Speranza',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b21',\n",
       "  'title': 'Annotating Causality in the TempEval-3 Corpus',\n",
       "  'year': '2014'},\n",
       " {'authors': 'P Mirza; S Tonelli',\n",
       "  'journal': 'Dublin City University and Association for Computational Linguistics',\n",
       "  'ref_id': 'b22',\n",
       "  'title': 'An Analysis of Causality between Events and its Relation to Temporal Information',\n",
       "  'year': '2014'},\n",
       " {'authors': 'A Nik; G Zhang; X Chen; M Li; J Fu',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b23',\n",
       "  'title': '1Cademy @ Causal News Corpus 2022: Leveraging Self-Training in Causality Classification of Socio-Political Event Data',\n",
       "  'year': '2022'},\n",
       " {'authors': 'K Radinsky; S Davidovich; S Markovitch',\n",
       "  'journal': 'ACM',\n",
       "  'ref_id': 'b24',\n",
       "  'title': 'Learning causality for news events prediction',\n",
       "  'year': '2012-04-16'},\n",
       " {'authors': 'S Schuster; C D Manning',\n",
       "  'journal': 'European Language Resources Association (ELRA)',\n",
       "  'ref_id': 'b25',\n",
       "  'title': 'Enhanced English Universal Dependencies: An Improved Representation for Natural Language Understanding Tasks',\n",
       "  'year': '2016'},\n",
       " {'authors': 'S Sia; A Dalmia; S J Mielke',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b26',\n",
       "  'title': 'Tired of Topic Models? Clusters of Pretrained Word Embeddings Make for Fast and Good Topics too!',\n",
       "  'year': '2020'},\n",
       " {'authors': 'F A Tan; X Zuo; S.-K Ng',\n",
       "  'journal': 'Springer International Publishing',\n",
       "  'ref_id': 'b27',\n",
       "  'title': 'UniCausal: Unified Benchmark and Repository for Causal Text Mining',\n",
       "  'year': '2023'},\n",
       " {'authors': 'B Webber; R Prasad; A Lee; A Joshi',\n",
       "  'journal': 'Philadelphia',\n",
       "  'ref_id': 'b28',\n",
       "  'title': 'The penn discourse treebank 3.0 annotation manual',\n",
       "  'year': '2019'},\n",
       " {'authors': 'Z Xu; Y Dang',\n",
       "  'journal': 'International Journal of Production Research',\n",
       "  'ref_id': 'b29',\n",
       "  'title': 'Data-driven causal knowledge graph construction for root cause analysis in quality problem solving',\n",
       "  'year': '2022'},\n",
       " {'authors': 'Z Zhang; M Fang; L Chen; M R Namazi Rad',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b30',\n",
       "  'title': 'Is Neural Topic Modelling Better than Clustering? An Empirical Study on Clustering with Contextual Embeddings for Topics',\n",
       "  'year': '2022'},\n",
       " {'authors': 'Z Zhang; H Wang; H Zhao; H Tong; H Ji',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b31',\n",
       "  'title': 'EventKE: Event-Enhanced Knowledge Graph Embedding',\n",
       "  'year': '2021'},\n",
       " {'authors': 'X Zuo; P Cao; Y Chen; K Liu; J Zhao; W Peng; Y Chen',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b32',\n",
       "  'title': 'Improving Event Causality Identification via Self-Supervised Representation Learning on External Causal Statement',\n",
       "  'year': '2021'},\n",
       " {'authors': 'X Zuo; P Cao; Y Chen; K Liu; J Zhao; W Peng; Y Chen',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b33',\n",
       "  'title': 'LearnDA: Learnable Knowledge-Guided Data Augmentation for Event Causality Identification',\n",
       "  'year': '2021'},\n",
       " {'authors': 'X Zuo; Y Chen; K Liu; J Zhao',\n",
       "  'journal': 'International Committee on Computational Linguistics',\n",
       "  'ref_id': 'b34',\n",
       "  'title': 'KnowDis: Knowledge Enhanced Data Augmentation for Event Causality Detection via Distant Supervision',\n",
       "  'year': '2020'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "references = annotated_article[0]['references']\n",
    "references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize and extract the first author's name\n",
    "def get_first_author(authors_str):\n",
    "    first_author = authors_str.split(';')[0].strip()\n",
    "    # Normalize the first author's name for comparison\n",
    "    return first_author.lower()\n",
    "\n",
    "# Generalized regular expression for detecting years in various date formats and standalone years\n",
    "\n",
    "# Function to detect various year patterns and extract the year\n",
    "def extract_years(string):\n",
    "    general_year_pattern = re.compile(r'(?:\\b|\\D)(\\d{4})(?:\\b|\\D)')\n",
    "    # Find all matches for the general year pattern\n",
    "\n",
    "    matches = general_year_pattern.findall(string)\n",
    "    # Add all unique years found in this string\n",
    "    year = matches[0] if matches else None\n",
    "    return year\n",
    "\n",
    "# Function to match citations with references\n",
    "def match_citations_with_references(citation, references):\n",
    "    match = None\n",
    "    citation_first_author = citation['Author']\n",
    "    citation_year = citation['Year'].strip()\n",
    "    for ref in references:\n",
    "        ref_first_author = get_first_author(ref['authors'])\n",
    "        ref_year = extract_years(ref['year']) if ref['year'] is not None else None\n",
    "        # Check for match by first author and year\n",
    "        if citation_first_author in ref_first_author: #and (citation_year == ref_year or ref_year is None):\n",
    "            match = {\n",
    "                'ref_id': ref['ref_id']\n",
    "            }\n",
    "    return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with the first sample\n",
    "for citation in annotated_article[0]['citation_data']:\n",
    "    parsed_name = split_and_parse_citation(citation['Citation'])\n",
    "    match = match_citations_with_references(parsed_name, references)\n",
    "    citation['ref_id'] = match['ref_id'] if match else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Category': 'Extension or Continuation',\n",
       "  'Citation': 'Heindorf et al. 2020',\n",
       "  'Explanation': 'The citing paper builds upon the research conducted by Heindorf et al. (2020) in the field of constructing causal KGs by exploring the differences in quality and quantity of extracted causal relations using both pattern-based and neural network-based methodologies.',\n",
       "  'ref_id': 'b16'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': 'Heindorf et al. 2020',\n",
       "  'Explanation': 'The cited work serves as a foundational source for the construction of causal KGs, where Cause and Effect arguments are directly extracted and used as nodes in the graph.',\n",
       "  'ref_id': 'b16'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': 'Tan, Zuo, and Ng 2023',\n",
       "  'Explanation': 'The citing paper employs the neural network-based methodologies proposed by Tan, Zuo, and Ng (2023) in the extraction of causal relations, alongside pattern-based approaches, to compare the effectiveness of both methods.',\n",
       "  'ref_id': 'b27'},\n",
       " {'Category': 'Theoretical Foundation',\n",
       "  'Citation': '(Radinsky, Davidovich, and Markovitch 2012)',\n",
       "  'Explanation': 'The cited work establishes the importance of generalizing over objects, actions, and events to make predictions, which forms the theoretical basis for the approach taken in the citing paper.',\n",
       "  'ref_id': 'b24'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Sia, Dalmia, and Mielke 2020; Zhang et al. 2022)',\n",
       "  'Explanation': 'The cited works introduce the method of condensing graphs by grouping nodes based on topic modeling solutions, which is adopted by the citing paper to structure its analysis.',\n",
       "  'ref_id': 'b31'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Heindorf et al. 2020)',\n",
       "  'Explanation': 'The citing paper replicates the methodology proposed by Heindorf et al. (2020) for detecting causal relations using linguistic patterns, indicating a direct adoption of the methods from the cited work.',\n",
       "  'ref_id': 'b16'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Culotta and Sorensen 2004; Bunescu and Mooney 2005; Ittoo and Bouma 2013)',\n",
       "  'Explanation': 'The citing paper leverages the dependency graph methodology proposed by Culotta and Sorensen (2004), Bunescu and Mooney (2005), and Ittoo and Bouma (2013) to identify the shortest path between Cause and Effect nouns, demonstrating a methodological basis for the research.',\n",
       "  'ref_id': 'b19'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Chen and Manning 2014; Schuster and Manning 2016)',\n",
       "  'Explanation': 'The citing paper utilizes the Stanford NLP Parser introduced by Chen and Manning (2014) and Schuster and Manning (2016) to enhance dependency graphs, indicating a methodological basis for the analysis conducted in the citing paper.',\n",
       "  'ref_id': 'b25'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': '(Heindorf et al. 2020)',\n",
       "  'Explanation': 'The citing paper acknowledges the use of the Wikipedia dataset from Heindorf et al. (2020) to obtain additional patterns for detecting causal relations, highlighting the reliance on external data for the research.',\n",
       "  'ref_id': 'b16'},\n",
       " {'Category': 'Extension or Continuation',\n",
       "  'Citation': '(Tan, Zuo, and Ng 2023)',\n",
       "  'Explanation': 'The citing paper builds upon the work of Tan, Zuo, and Ng by utilizing the causal text mining repository they developed, UniCausal, to further explore causal relations in text.',\n",
       "  'ref_id': 'b27'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Sia, Dalmia, and Mielke 2020)',\n",
       "  'Explanation': \"The cited work's approach of generating word embeddings from sequences and clustering them directly serves as the methodological basis for the clustering of arguments in the citing paper.\",\n",
       "  'ref_id': 'b26'},\n",
       " {'Category': 'Extension or Continuation',\n",
       "  'Citation': '(Zhang et al. 2022)',\n",
       "  'Explanation': \"The cited work's method of obtaining keywords per cluster through the TFIDF Ã— IDF method is extended and applied in the citing paper to extract relevant information from the clustered arguments.\",\n",
       "  'ref_id': 'b31'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': '(Finkel, Grenager, and Manning 2005)',\n",
       "  'Explanation': \"The cited work's use of the 7-class Stanford Named Entity Recognition (NER) Tagger provides the data source for extracting named-entities and filtering out irrelevant words in the arguments of the citing paper.\",\n",
       "  'ref_id': 'b12'},\n",
       " {'Category': 'Theoretical Foundation',\n",
       "  'Citation': '(Gao, Yao, and Chen 2021)',\n",
       "  'Explanation': \"The cited work's supervised pre-trained language model, SimCSE, forms the theoretical foundation for encoding arguments into embeddings and evaluating semantic textual similarity, which is adopted in the citing paper for clustering purposes.\",\n",
       "  'ref_id': 'b13'},\n",
       " {'Category': 'Supporting Evidence',\n",
       "  'Citation': '(Zhang et al. 2022)',\n",
       "  'Explanation': 'The cited work by Zhang et al. (2022) demonstrates the superiority of the method discussed in the citing paper over traditional TF or TFIDF methods when selecting topic words, providing empirical evidence for the effectiveness of the approach.',\n",
       "  'ref_id': 'b31'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated_article[0]['citation_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'authors': 'S Sia; A Dalmia; S J Mielke',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b26',\n",
       "  'title': 'Tired of Topic Models? Clusters of Pretrained Word Embeddings Make for Fast and Good Topics too!',\n",
       "  'year': '2020'},\n",
       " {'authors': 'F A Tan; X Zuo; S.-K Ng',\n",
       "  'journal': 'Springer International Publishing',\n",
       "  'ref_id': 'b27',\n",
       "  'title': 'UniCausal: Unified Benchmark and Repository for Causal Text Mining',\n",
       "  'year': '2023'},\n",
       " {'authors': 'B Webber; R Prasad; A Lee; A Joshi',\n",
       "  'journal': 'Philadelphia',\n",
       "  'ref_id': 'b28',\n",
       "  'title': 'The penn discourse treebank 3.0 annotation manual',\n",
       "  'year': '2019'},\n",
       " {'authors': 'Z Xu; Y Dang',\n",
       "  'journal': 'International Journal of Production Research',\n",
       "  'ref_id': 'b29',\n",
       "  'title': 'Data-driven causal knowledge graph construction for root cause analysis in quality problem solving',\n",
       "  'year': '2022'},\n",
       " {'authors': 'Z Zhang; M Fang; L Chen; M R Namazi Rad',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b30',\n",
       "  'title': 'Is Neural Topic Modelling Better than Clustering? An Empirical Study on Clustering with Contextual Embeddings for Topics',\n",
       "  'year': '2022'},\n",
       " {'authors': 'Z Zhang; H Wang; H Zhao; H Tong; H Ji',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b31',\n",
       "  'title': 'EventKE: Event-Enhanced Knowledge Graph Embedding',\n",
       "  'year': '2021'},\n",
       " {'authors': 'X Zuo; P Cao; Y Chen; K Liu; J Zhao; W Peng; Y Chen',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b32',\n",
       "  'title': 'Improving Event Causality Identification via Self-Supervised Representation Learning on External Causal Statement',\n",
       "  'year': '2021'},\n",
       " {'authors': 'X Zuo; P Cao; Y Chen; K Liu; J Zhao; W Peng; Y Chen',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b33',\n",
       "  'title': 'LearnDA: Learnable Knowledge-Guided Data Augmentation for Event Causality Identification',\n",
       "  'year': '2021'},\n",
       " {'authors': 'X Zuo; Y Chen; K Liu; J Zhao',\n",
       "  'journal': 'International Committee on Computational Linguistics',\n",
       "  'ref_id': 'b34',\n",
       "  'title': 'KnowDis: Knowledge Enhanced Data Augmentation for Event Causality Detection via Distant Supervision',\n",
       "  'year': '2020'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "references[26:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to group the citation data by ref_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'b16': [{'Category': 'Extension or Continuation', 'Citation': 'Heindorf et al. 2020', 'Explanation': 'The citing paper builds upon the research conducted by Heindorf et al. (2020) in the field of constructing causal KGs by exploring the differences in quality and quantity of extracted causal relations using both pattern-based and neural network-based methodologies.'}, {'Category': 'Data Source', 'Citation': 'Heindorf et al. 2020', 'Explanation': 'The cited work serves as a foundational source for the construction of causal KGs, where Cause and Effect arguments are directly extracted and used as nodes in the graph.'}, {'Category': 'Methodological Basis', 'Citation': '(Heindorf et al. 2020)', 'Explanation': 'The citing paper replicates the methodology proposed by Heindorf et al. (2020) for detecting causal relations using linguistic patterns, indicating a direct adoption of the methods from the cited work.'}, {'Category': 'Data Source', 'Citation': '(Heindorf et al. 2020)', 'Explanation': 'The citing paper acknowledges the use of the Wikipedia dataset from Heindorf et al. (2020) to obtain additional patterns for detecting causal relations, highlighting the reliance on external data for the research.'}], 'b27': [{'Category': 'Methodological Basis', 'Citation': 'Tan, Zuo, and Ng 2023', 'Explanation': 'The citing paper employs the neural network-based methodologies proposed by Tan, Zuo, and Ng (2023) in the extraction of causal relations, alongside pattern-based approaches, to compare the effectiveness of both methods.'}, {'Category': 'Extension or Continuation', 'Citation': '(Tan, Zuo, and Ng 2023)', 'Explanation': 'The citing paper builds upon the work of Tan, Zuo, and Ng by utilizing the causal text mining repository they developed, UniCausal, to further explore causal relations in text.'}], 'b24': [{'Category': 'Theoretical Foundation', 'Citation': '(Radinsky, Davidovich, and Markovitch 2012)', 'Explanation': 'The cited work establishes the importance of generalizing over objects, actions, and events to make predictions, which forms the theoretical basis for the approach taken in the citing paper.'}], 'b31': [{'Category': 'Methodological Basis', 'Citation': '(Sia, Dalmia, and Mielke 2020; Zhang et al. 2022)', 'Explanation': 'The cited works introduce the method of condensing graphs by grouping nodes based on topic modeling solutions, which is adopted by the citing paper to structure its analysis.'}, {'Category': 'Extension or Continuation', 'Citation': '(Zhang et al. 2022)', 'Explanation': \"The cited work's method of obtaining keywords per cluster through the TFIDF Ã— IDF method is extended and applied in the citing paper to extract relevant information from the clustered arguments.\"}, {'Category': 'Supporting Evidence', 'Citation': '(Zhang et al. 2022)', 'Explanation': 'The cited work by Zhang et al. (2022) demonstrates the superiority of the method discussed in the citing paper over traditional TF or TFIDF methods when selecting topic words, providing empirical evidence for the effectiveness of the approach.'}], 'b19': [{'Category': 'Methodological Basis', 'Citation': '(Culotta and Sorensen 2004; Bunescu and Mooney 2005; Ittoo and Bouma 2013)', 'Explanation': 'The citing paper leverages the dependency graph methodology proposed by Culotta and Sorensen (2004), Bunescu and Mooney (2005), and Ittoo and Bouma (2013) to identify the shortest path between Cause and Effect nouns, demonstrating a methodological basis for the research.'}], 'b25': [{'Category': 'Methodological Basis', 'Citation': '(Chen and Manning 2014; Schuster and Manning 2016)', 'Explanation': 'The citing paper utilizes the Stanford NLP Parser introduced by Chen and Manning (2014) and Schuster and Manning (2016) to enhance dependency graphs, indicating a methodological basis for the analysis conducted in the citing paper.'}], 'b26': [{'Category': 'Methodological Basis', 'Citation': '(Sia, Dalmia, and Mielke 2020)', 'Explanation': \"The cited work's approach of generating word embeddings from sequences and clustering them directly serves as the methodological basis for the clustering of arguments in the citing paper.\"}], 'b12': [{'Category': 'Data Source', 'Citation': '(Finkel, Grenager, and Manning 2005)', 'Explanation': \"The cited work's use of the 7-class Stanford Named Entity Recognition (NER) Tagger provides the data source for extracting named-entities and filtering out irrelevant words in the arguments of the citing paper.\"}], 'b13': [{'Category': 'Theoretical Foundation', 'Citation': '(Gao, Yao, and Chen 2021)', 'Explanation': \"The cited work's supervised pre-trained language model, SimCSE, forms the theoretical foundation for encoding arguments into embeddings and evaluating semantic textual similarity, which is adopted in the citing paper for clustering purposes.\"}]}\n"
     ]
    }
   ],
   "source": [
    "# Function to regroup citations by ref_id\n",
    "def regroup_citations_by_ref_id(citations):\n",
    "    grouped_citations = {}\n",
    "    for citation in citations:\n",
    "        if 'ref_id' in citation.keys():\n",
    "            ref_id = citation['ref_id']\n",
    "            # Create a copy of the citation without the ref_id\n",
    "            citation_copy = {k: v for k, v in citation.items() if k != 'ref_id'}\n",
    "            # Append the citation to the list associated with its ref_id\n",
    "            if ref_id in grouped_citations:\n",
    "                grouped_citations[ref_id].append(citation_copy)\n",
    "            else:\n",
    "                grouped_citations[ref_id] = [citation_copy]\n",
    "    return grouped_citations\n",
    "\n",
    "\n",
    "# Regroup the citationb list by ref_id\n",
    "grouped_citations = regroup_citations_by_ref_id(annotated_article[0]['citation_data'])\n",
    "print(grouped_citations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine all the steps together into one function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_citation_author_year(article):\n",
    "    for citation in article['citation_data']:\n",
    "        parsed_name = split_and_parse_citation(citation['Citation'])\n",
    "        match = match_citations_with_references(parsed_name, article['references'])\n",
    "        citation['ref_id'] = match['ref_id'] if match else None\n",
    "    return article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a grouped citation data for author-year citation style, let's start solving cases with numeric-style. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2.2 Handle Numeric Citation Style**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This style of citation seems simple at first, but there are many edge cases that we have to deal with. From my observation, there are 3 main types:\n",
    "\n",
    "- Singular citations such as [1] or [4]: These are processed conventionally, where the reference ID equals the citation number minus one.\n",
    "- Lists, for instance [1, 4, 6]: In this scenario, the citations are split into individual entries: [1], [4], and [6].\n",
    "- Ranges, like [1 - 5]: Here, the citation is divided into separate entries: [1], [2], [3], [4], [5].\n",
    "- Mixed ranges, such as [1] - [5]: These are split into distinct citations: [1] and [5]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_numeric_citations(citations):\n",
    "    # Helper function to parse ranges and individual numbers\n",
    "    def parse_part(part):\n",
    "        if '-' in part:  # Handle ranges\n",
    "            start, end = map(int, part.split('-'))\n",
    "            return list(range(start, end + 1))\n",
    "        else:  # Handle individual numbers\n",
    "            return [int(part)]\n",
    "\n",
    "    # Initialize the result list\n",
    "    result = []\n",
    "\n",
    "    # Find all parts of the input that match the patterns\n",
    "    parts = re.findall(r'\\[([^]]+)]', citations)\n",
    "    \n",
    "    for part in parts:\n",
    "        # For each part, remove spaces, split by commas and extend the result list\n",
    "        for subpart in part.replace(' ', '').split(','):\n",
    "            try:\n",
    "                result.extend(parse_part(subpart))\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    return [f\"[{num}]\" for num in result]\n",
    "\n",
    "# Function to apply citation splitting to a list of citation entries\n",
    "def split_citations_in_entries(citation_entries):\n",
    "    expanded_citation_entries = []\n",
    "    for entry in citation_entries:\n",
    "        # Use the split_citations function to get a list of individual citations from the Citation field\n",
    "        split_citations_list = split_numeric_citations(entry['Citation'])\n",
    "        for citation in split_citations_list:\n",
    "            # Create a new citation entry for each split citation, keeping other fields the same\n",
    "            \n",
    "            new_entry = {\n",
    "                'Citation': citation,\n",
    "                'Category': entry['Category'],\n",
    "                'Explanation': entry['Explanation']\n",
    "            }\n",
    "            expanded_citation_entries.append(new_entry)\n",
    "    return expanded_citation_entries\n",
    "\n",
    "\n",
    "def match_numeric_citation(citations):\n",
    "    for citation in citations:\n",
    "        # Regular expression to find single numbers inside square brackets\n",
    "        pattern = re.compile(r'\\[\\(?(?P<number>\\d+)\\)?\\]')\n",
    "        try:\n",
    "            #Find all matches in the text and convert them to integers\n",
    "            reference_num = [int(match.group('number')) for match in pattern.finditer(citation['Citation'])][0]\n",
    "            citation['ref_id'] = f\"b{reference_num -1}\"\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "    return citations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Category': 'Supporting Evidence',\n",
       "  'Citation': '[1]',\n",
       "  'Explanation': 'The cited work provides foundational data on structural health monitoring (SHM) systems, which is essential for understanding the context and importance of using SHM for damage detection and condition monitoring in the citing paper.'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': '[2]',\n",
       "  'Explanation': \"The cited work highlights the challenges of data availability and completeness in structural health monitoring, emphasizing the need for reliable data sets to train models, which is crucial for the citing paper's discussion on the importance of accurate predictions in SHM systems.\"},\n",
       " {'Category': 'Theoretical Foundation',\n",
       "  'Citation': '[3]',\n",
       "  'Explanation': \"The cited work by Caruana establishes the theoretical foundation of multi-task learning (MTL) algorithms, specifically neural networks with back propagation, which is essential for the citing paper's exploration of how MTL can improve generalization and accuracy in SHM systems.\"},\n",
       " {'Category': 'Extension or Continuation',\n",
       "  'Citation': '[4]',\n",
       "  'Explanation': \"The cited work demonstrates the application of multi-task learning to support vector machines, expanding the scope of MTL beyond neural networks and providing insights that can be relevant for the citing paper's discussion on the potential benefits of MTL for SHM systems.\"},\n",
       " {'Category': 'Extension or Continuation',\n",
       "  'Citation': '[5,6]',\n",
       "  'Explanation': \"The cited works extend the application of multi-task learning to decision trees, further showcasing the versatility of MTL across different machine-learning algorithms, which enriches the citing paper's exploration of MTL in the context of neural networks for SHM systems.\"}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated_article[7]['citation_data'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_article[7]['citation_data'] = split_citations_in_entries(annotated_article[7]['citation_data'])\n",
    "annotated_article[7]['citation_data'] =  match_numeric_citation(annotated_article[7]['citation_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Citation': '[1]',\n",
       "  'Category': 'Supporting Evidence',\n",
       "  'Explanation': 'The cited work provides foundational data on structural health monitoring (SHM) systems, which is essential for understanding the context and importance of using SHM for damage detection and condition monitoring in the citing paper.',\n",
       "  'ref_id': 'b0'},\n",
       " {'Citation': '[2]',\n",
       "  'Category': 'Data Source',\n",
       "  'Explanation': \"The cited work highlights the challenges of data availability and completeness in structural health monitoring, emphasizing the need for reliable data sets to train models, which is crucial for the citing paper's discussion on the importance of accurate predictions in SHM systems.\",\n",
       "  'ref_id': 'b1'},\n",
       " {'Citation': '[3]',\n",
       "  'Category': 'Theoretical Foundation',\n",
       "  'Explanation': \"The cited work by Caruana establishes the theoretical foundation of multi-task learning (MTL) algorithms, specifically neural networks with back propagation, which is essential for the citing paper's exploration of how MTL can improve generalization and accuracy in SHM systems.\",\n",
       "  'ref_id': 'b2'},\n",
       " {'Citation': '[4]',\n",
       "  'Category': 'Extension or Continuation',\n",
       "  'Explanation': \"The cited work demonstrates the application of multi-task learning to support vector machines, expanding the scope of MTL beyond neural networks and providing insights that can be relevant for the citing paper's discussion on the potential benefits of MTL for SHM systems.\",\n",
       "  'ref_id': 'b3'},\n",
       " {'Citation': '[5]',\n",
       "  'Category': 'Extension or Continuation',\n",
       "  'Explanation': \"The cited works extend the application of multi-task learning to decision trees, further showcasing the versatility of MTL across different machine-learning algorithms, which enriches the citing paper's exploration of MTL in the context of neural networks for SHM systems.\",\n",
       "  'ref_id': 'b4'},\n",
       " {'Citation': '[6]',\n",
       "  'Category': 'Extension or Continuation',\n",
       "  'Explanation': \"The cited works extend the application of multi-task learning to decision trees, further showcasing the versatility of MTL across different machine-learning algorithms, which enriches the citing paper's exploration of MTL in the context of neural networks for SHM systems.\",\n",
       "  'ref_id': 'b5'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated_article[7]['citation_data'][:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, single citation like [1], [6], [27] will be parsed normaly. But for citation like [5, 6], they will get split to 2 separated citations [5] and [6].\n",
    "\n",
    "Now, let's combine the steps together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proprocess_citation_numeric(article):\n",
    "    article['citation_data'] = split_citations_in_entries(article['citation_data'])\n",
    "    article['citation_data'] = match_numeric_citation(article['citation_data'])\n",
    "    return article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2.3 Process 2 citation style**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to detect the citation style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_citation_style(text):\n",
    "    # Pattern to match numeric citations like [1], [1, 2], [1-6], [1, 2-6], [1, 2, 3-6], etc.\n",
    "    numeric_pattern = re.compile(r'\\[\\d+(-\\d+)?(,\\s*\\d+(-\\d+)?)*\\]')\n",
    "    # Pattern for \"Author-Year\" citations like (Author, Year)\n",
    "    author_year_pattern = re.compile(r'\\([A-Za-z]+,\\s*\\d{4}\\)')\n",
    "\n",
    "    # Check for numeric citation style\n",
    "    if numeric_pattern.search(text):\n",
    "        return \"Numeric\"\n",
    "    # Check for author-year citation style\n",
    "    elif author_year_pattern.search(text):\n",
    "        return \"Author-Year\"\n",
    "    else:\n",
    "        return \"Author-Year\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Author-Year'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect_citation_style(\"(Amin et al., 2019)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Numeric'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect_citation_style(\"[1,6]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 377/377 [00:00<00:00, 554.63it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for article in tqdm(annotated_article, total=len(annotated_article)):\n",
    "    references = article['references']\n",
    "    citation_style = detect_citation_style(article['citation_data'][0][\"Citation\"])\n",
    "    try:\n",
    "        if citation_style == \"Author-Year\":\n",
    "            article = preprocess_citation_author_year(article)\n",
    "        elif citation_style == \"Numeric\":\n",
    "            article = proprocess_citation_numeric(article)\n",
    "        else:\n",
    "            print(f\"Uncertain citation style: {citation_style}\")\n",
    "            continue\n",
    "    except Exception as e:\n",
    "        print(article['citation_data'])\n",
    "        break\n",
    "\n",
    "    grouped_citations = regroup_citations_by_ref_id(article['citation_data'])\n",
    "    article['grouped_citations'] = grouped_citations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b5': [{'Category': 'Theoretical Foundation',\n",
       "   'Citation': '(Minsky, 1975; Fillmore, 1976)',\n",
       "   'Explanation': 'The cited works by Minsky (1975) and Fillmore (1976) establish the concept of frames and schemas, which serve as the theoretical foundation for the semantic information framing in the citing paper.'}],\n",
       " 'b8': [{'Category': 'Theoretical Foundation',\n",
       "   'Citation': '(Rumelhart, 1980)',\n",
       "   'Explanation': 'The work by Rumelhart (1980) on schemas contributes to the theoretical underpinning of the semantic information modeling approach adopted in the citing paper.'}],\n",
       " 'b1': [{'Category': 'Supporting Evidence',\n",
       "   'Citation': '(Baker et al., 1998)',\n",
       "   'Explanation': 'The work by Baker et al. (1998) on FrameNet provides foundational insights into semantic role labeling, supporting the approach taken in the citing paper.'}],\n",
       " 'b7': [{'Category': 'Extension or Continuation',\n",
       "   'Citation': '(Palmer et al., 2005)',\n",
       "   'Explanation': 'In contrast to the PropBank approach by Palmer et al. (2005), the citing paper extends the semantic modeling scheme by focusing on semantic units and diverse predicate sources.'}],\n",
       " 'b3': [{'Category': 'Theoretical Foundation',\n",
       "   'Citation': '(Beuls et al., 2021)',\n",
       "   'Explanation': 'The cited work on causal semantic frames in English serves as the theoretical foundation for the concept of semantic frames and frame elements discussed in the citing paper, providing a framework for understanding cause and effect relations in Italian sentences.'}],\n",
       " 'b2': [{'Category': 'Theoretical Foundation',\n",
       "   'Citation': '(Banarescu et al., 2013)',\n",
       "   'Explanation': 'The cited work establishes the structural components and relationships within AMR, providing the theoretical foundation for understanding how nodes and edges function in representing semantic concepts.'}],\n",
       " 'b12': [{'Category': 'Supporting Evidence',\n",
       "   'Citation': '(Xue et al., 2014)',\n",
       "   'Explanation': 'The cited work emphasizes the focus of AMR on English vocabulary, highlighting the language-specific nature of AMR and its applications.'}],\n",
       " 'b11': [{'Category': 'Extension or Continuation',\n",
       "   'Citation': '(Vanderwende et al., 2015)',\n",
       "   'Explanation': 'The cited work expands the application of AMR to other languages such as Chinese, French, German, Spanish, and Japanese, showcasing the versatility and adaptability of AMR beyond English.'}],\n",
       " 'b9': [{'Category': 'Data Source',\n",
       "   'Citation': '(Schneider et al., 2015)',\n",
       "   'Explanation': 'The cited work provides the example sentence used to demonstrate the conversion process from AMR to Meta SRL++, serving as the source of the textual input for the analysis.'}]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated_article[17]['grouped_citations']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.3 Building citation graph**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.3.1 Parsing annotated triplets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationships_dict = {\n",
    "    \"Supporting Evidence\": \"Is Evidence For\",\n",
    "    \"Methodological Basis\": \"Is Methodological Basis For\",\n",
    "    \"Theoretical Foundation\": \"Is Theoretical Foundation For\", \n",
    "    \"Data Source\": \"Is Data Source For\",\n",
    "    \"Extension or Continuation\": \"Is Extension or Continuation Of\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have grouped citation data; now we need to find the papers cited in the arXiv dataset by name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 377/377 [04:53<00:00,  1.29it/s]\n"
     ]
    }
   ],
   "source": [
    "df_data['title'] = df_data['title'].str.lower()\n",
    "titles = df_data['title'].tolist()\n",
    "\n",
    "\n",
    "def search_paper_by_name(name):\n",
    "    # matches = df_data['title'].str.contains(name, case=False, na=False, regex=False)\n",
    "    # filtered_df = df_data[matches]\n",
    "    # if len(filtered_df) == 0:\n",
    "    #     return None\n",
    "    # return filtered_df.iloc[0]['id']\n",
    "    titles = df_data['title'].tolist()\n",
    "    for idx, title in enumerate(titles):\n",
    "        if name in title:\n",
    "            return df_data.iloc[idx]['id']\n",
    "    return None\n",
    "\n",
    "for article_dict in tqdm(annotated_article, total=len(annotated_article)):\n",
    "\n",
    "    article_dict[\"arxiv_id\"] = search_paper_by_name(article_dict['title'].lower())\n",
    "\n",
    "    if \"grouped_citations\" in article_dict.keys():\n",
    "        article_dict[\"mapped_citation\"] = {}\n",
    "        for key,val in article_dict['grouped_citations'].items():\n",
    "            for ref in article_dict[\"references\"]:\n",
    "                if ref[\"ref_id\"] == key:\n",
    "                    title = ref[\"title\"]\n",
    "\n",
    "            title = title.lower()\n",
    "            arxiv_id = search_paper_by_name(title)\n",
    "            article_dict['mapped_citation'][key] = {\"title\": title, 'arxiv_id': arxiv_id, 'citation': val}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b3': {'title': 'an image is worth 16x16 words: transformers for image recognition at scale',\n",
       "  'arxiv_id': '2010.11929',\n",
       "  'citation': [{'Citation': '[4]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': 'The cited works introduce the concept of using image transformers as a backbone for video descriptor extraction, which the citing paper adopts for its video copy detection model.'}]},\n",
       " 'b6': {'title': 'swin transformer v2: scaling up capacity and resolution',\n",
       "  'arxiv_id': '2111.09883',\n",
       "  'citation': [{'Citation': '[7]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': 'The cited works introduce the concept of using image transformers as a backbone for video descriptor extraction, which the citing paper adopts for its video copy detection model.'}]},\n",
       " 'b0': {'title': 'a simple framework for contrastive learning of visual representations',\n",
       "  'arxiv_id': '2002.05709',\n",
       "  'citation': [{'Citation': '[1]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': \"The cited work, SimCLR, provides the method of combining with entropy loss for self-supervised learning, which is utilized in the training process of the citing paper's basic model.\"}]},\n",
       " 'b7': {'title': 'a self-supervised descriptor for image copy detection',\n",
       "  'arxiv_id': '2202.10261',\n",
       "  'citation': [{'Citation': '[8]',\n",
       "    'Category': 'Extension or Continuation',\n",
       "    'Explanation': 'The citing paper builds upon the SSCD method to train its basic model in a self-supervised manner, indicating an extension of the research conducted in the cited work.'}]},\n",
       " 'b9': {'title': 'spreading vectors for similarity search',\n",
       "  'arxiv_id': '1806.03198',\n",
       "  'citation': [{'Citation': '[10]',\n",
       "    'Category': 'Supporting Evidence',\n",
       "    'Explanation': \"The cited work proposes the entropy loss used in the training process of the citing paper's basic model, providing a foundational element for the loss function formulation.\"}]},\n",
       " 'b8': {'title': 'learning transferable visual models from natural language supervision',\n",
       "  'arxiv_id': '2103.00020',\n",
       "  'citation': [{'Citation': '[9]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': 'The citing paper adopts the CLIP model to extract frame features without post-processing, which serves as the foundation for the edited video detection method proposed in the study.'}]},\n",
       " 'b5': {'title': 'roberta: a robustly optimized bert pretraining approach',\n",
       "  'arxiv_id': '1907.11692',\n",
       "  'citation': [{'Citation': '[6]',\n",
       "    'Category': 'Theoretical Foundation',\n",
       "    'Explanation': 'The citing paper leverages the RoBERTa model to process frame features extracted by CLIP, providing a theoretical framework for the binary classification approach used in identifying edited videos.'}]}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_dict['mapped_citation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save annotated articles\n",
    "import json\n",
    "with open('../outputs/annotated_articles.json', 'w') as f:\n",
    "    json.dump(annotated_article, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a class for a paper node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperNode:\n",
    "    title: str\n",
    "    arxiv_id: str\n",
    "    \n",
    "    def __init__(self, title, arxiv_id):\n",
    "        self.title = title\n",
    "        self.arxiv_id = arxiv_id\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"Title: {self.title},\\n Arxiv ID: {self.arxiv_id}\"\n",
    "\n",
    "class PaperEdge:\n",
    "    category: str\n",
    "    explanation: str\n",
    "    verbose = True\n",
    "\n",
    "    def __init__(self, category, explanation):\n",
    "        self.category = category\n",
    "        self.explanation = explanation\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        if self.verbose:\n",
    "            return f\"Category: {self.category},\\n Explanation: {self.explanation}\"\n",
    "        else:\n",
    "            return f\"Category: {self.category}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 377/377 [00:02<00:00, 169.88it/s]\n"
     ]
    }
   ],
   "source": [
    "paper_dict = {}\n",
    "\n",
    "for article_dict in tqdm(annotated_article, total=len(annotated_article)):\n",
    "    paper_dict[article_dict['title'].lower()] = PaperNode(title=article_dict['title'], arxiv_id=article_dict['arxiv_id'])\n",
    "\n",
    "    if \"mapped_citation\" in article_dict.keys():\n",
    "        for key,val in article_dict['mapped_citation'].items():\n",
    "            title = val['title']\n",
    "            if title not in paper_dict.keys():\n",
    "                paper_node = PaperNode(title=val['title'], arxiv_id=val['arxiv_id'])\n",
    "                paper_dict[title] = paper_node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b16': {'title': 'causenet: towards a causality graph extracted from the web',\n",
       "  'arxiv_id': None,\n",
       "  'citation': [{'Category': 'Extension or Continuation',\n",
       "    'Citation': 'Heindorf et al. 2020',\n",
       "    'Explanation': 'The citing paper builds upon the research conducted by Heindorf et al. (2020) in the field of constructing causal KGs by exploring the differences in quality and quantity of extracted causal relations using both pattern-based and neural network-based methodologies.'},\n",
       "   {'Category': 'Data Source',\n",
       "    'Citation': 'Heindorf et al. 2020',\n",
       "    'Explanation': 'The cited work serves as a foundational source for the construction of causal KGs, where Cause and Effect arguments are directly extracted and used as nodes in the graph.'},\n",
       "   {'Category': 'Methodological Basis',\n",
       "    'Citation': '(Heindorf et al. 2020)',\n",
       "    'Explanation': 'The citing paper replicates the methodology proposed by Heindorf et al. (2020) for detecting causal relations using linguistic patterns, indicating a direct adoption of the methods from the cited work.'},\n",
       "   {'Category': 'Data Source',\n",
       "    'Citation': '(Heindorf et al. 2020)',\n",
       "    'Explanation': 'The citing paper acknowledges the use of the Wikipedia dataset from Heindorf et al. (2020) to obtain additional patterns for detecting causal relations, highlighting the reliance on external data for the research.'}]},\n",
       " 'b27': {'title': 'unicausal: unified benchmark and repository for causal text mining',\n",
       "  'arxiv_id': '2208.09163',\n",
       "  'citation': [{'Category': 'Methodological Basis',\n",
       "    'Citation': 'Tan, Zuo, and Ng 2023',\n",
       "    'Explanation': 'The citing paper employs the neural network-based methodologies proposed by Tan, Zuo, and Ng (2023) in the extraction of causal relations, alongside pattern-based approaches, to compare the effectiveness of both methods.'},\n",
       "   {'Category': 'Extension or Continuation',\n",
       "    'Citation': '(Tan, Zuo, and Ng 2023)',\n",
       "    'Explanation': 'The citing paper builds upon the work of Tan, Zuo, and Ng by utilizing the causal text mining repository they developed, UniCausal, to further explore causal relations in text.'}]},\n",
       " 'b24': {'title': 'learning causality for news events prediction',\n",
       "  'arxiv_id': None,\n",
       "  'citation': [{'Category': 'Theoretical Foundation',\n",
       "    'Citation': '(Radinsky, Davidovich, and Markovitch 2012)',\n",
       "    'Explanation': 'The cited work establishes the importance of generalizing over objects, actions, and events to make predictions, which forms the theoretical basis for the approach taken in the citing paper.'}]},\n",
       " 'b31': {'title': 'eventke: event-enhanced knowledge graph embedding',\n",
       "  'arxiv_id': None,\n",
       "  'citation': [{'Category': 'Methodological Basis',\n",
       "    'Citation': '(Sia, Dalmia, and Mielke 2020; Zhang et al. 2022)',\n",
       "    'Explanation': 'The cited works introduce the method of condensing graphs by grouping nodes based on topic modeling solutions, which is adopted by the citing paper to structure its analysis.'},\n",
       "   {'Category': 'Extension or Continuation',\n",
       "    'Citation': '(Zhang et al. 2022)',\n",
       "    'Explanation': \"The cited work's method of obtaining keywords per cluster through the TFIDF Ã— IDF method is extended and applied in the citing paper to extract relevant information from the clustered arguments.\"},\n",
       "   {'Category': 'Supporting Evidence',\n",
       "    'Citation': '(Zhang et al. 2022)',\n",
       "    'Explanation': 'The cited work by Zhang et al. (2022) demonstrates the superiority of the method discussed in the citing paper over traditional TF or TFIDF methods when selecting topic words, providing empirical evidence for the effectiveness of the approach.'}]},\n",
       " 'b19': {'title': 'minimally-supervised learning of domain-specific causal relations using an opendomain corpus as knowledge base',\n",
       "  'arxiv_id': None,\n",
       "  'citation': [{'Category': 'Methodological Basis',\n",
       "    'Citation': '(Culotta and Sorensen 2004; Bunescu and Mooney 2005; Ittoo and Bouma 2013)',\n",
       "    'Explanation': 'The citing paper leverages the dependency graph methodology proposed by Culotta and Sorensen (2004), Bunescu and Mooney (2005), and Ittoo and Bouma (2013) to identify the shortest path between Cause and Effect nouns, demonstrating a methodological basis for the research.'}]},\n",
       " 'b25': {'title': 'enhanced english universal dependencies: an improved representation for natural language understanding tasks',\n",
       "  'arxiv_id': None,\n",
       "  'citation': [{'Category': 'Methodological Basis',\n",
       "    'Citation': '(Chen and Manning 2014; Schuster and Manning 2016)',\n",
       "    'Explanation': 'The citing paper utilizes the Stanford NLP Parser introduced by Chen and Manning (2014) and Schuster and Manning (2016) to enhance dependency graphs, indicating a methodological basis for the analysis conducted in the citing paper.'}]},\n",
       " 'b26': {'title': 'tired of topic models? clusters of pretrained word embeddings make for fast and good topics too!',\n",
       "  'arxiv_id': '2004.14914',\n",
       "  'citation': [{'Category': 'Methodological Basis',\n",
       "    'Citation': '(Sia, Dalmia, and Mielke 2020)',\n",
       "    'Explanation': \"The cited work's approach of generating word embeddings from sequences and clustering them directly serves as the methodological basis for the clustering of arguments in the citing paper.\"}]},\n",
       " 'b12': {'title': 'incorporating non-local information into information extraction systems by gibbs sampling',\n",
       "  'arxiv_id': None,\n",
       "  'citation': [{'Category': 'Data Source',\n",
       "    'Citation': '(Finkel, Grenager, and Manning 2005)',\n",
       "    'Explanation': \"The cited work's use of the 7-class Stanford Named Entity Recognition (NER) Tagger provides the data source for extracting named-entities and filtering out irrelevant words in the arguments of the citing paper.\"}]},\n",
       " 'b13': {'title': 'simcse: simple contrastive learning of sentence embeddings',\n",
       "  'arxiv_id': '2104.08821',\n",
       "  'citation': [{'Category': 'Theoretical Foundation',\n",
       "    'Citation': '(Gao, Yao, and Chen 2021)',\n",
       "    'Explanation': \"The cited work's supervised pre-trained language model, SimCSE, forms the theoretical foundation for encoding arguments into embeddings and evaluating semantic textual similarity, which is adopted in the citing paper for clustering purposes.\"}]}}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated_article[0]['mapped_citation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets = []\n",
    "\n",
    "for article_dict in annotated_article:\n",
    "    for key, val in article_dict['mapped_citation'].items():\n",
    "        title = val['title']\n",
    "        citation = val['citation']\n",
    "        \n",
    "        # Use a dictionary to group explanations by category\n",
    "        category_explanations = {}\n",
    "        for rel in citation:\n",
    "            category = rel['Category']\n",
    "            explanation = rel['Explanation']\n",
    "            if category not in category_explanations:\n",
    "                category_explanations[category] = []\n",
    "            category_explanations[category].append(explanation)\n",
    "\n",
    "        source_node = paper_dict[title]\n",
    "        target_node = paper_dict[article_dict['title'].lower()]\n",
    "\n",
    "        # Construct triplets with aggregated explanations for each category\n",
    "        for category, explanations in category_explanations.items():\n",
    "            if category not in relationships_dict.keys():\n",
    "                relationships_dict[category] = f\"Is {category} Of\"\n",
    "\n",
    "            aggregated_explanation = \"; \".join(set(explanations))  # Remove duplicates and join explanations\n",
    "            rel = PaperEdge(category=category, explanation=aggregated_explanation)\n",
    "            reverse_rel = PaperEdge(category=relationships_dict[category], explanation=aggregated_explanation)\n",
    "\n",
    "            # Add the relationship in both directions\n",
    "            triplets.append((source_node, rel, target_node))\n",
    "            triplets.append((target_node, reverse_rel, source_node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Assuming 'triplets' is your list of relationships, \n",
    "# and each PaperNode object in the triplets has an 'arxiv_id' attribute\n",
    "\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add nodes and edges\n",
    "for source_node, relationship, target_node in triplets:\n",
    "    # Add nodes if they are not already in the graph\n",
    "    if source_node.arxiv_id not in G:\n",
    "        G.add_node(source_node.title, title=str(source_node), arxiv_id=source_node.arxiv_id)\n",
    "    if target_node.arxiv_id not in G:\n",
    "        G.add_node(target_node.title, title=str(target_node), arxiv_id=target_node.arxiv_id)\n",
    "    \n",
    "    # Add edge with relationship details\n",
    "    G.add_edge(source_node.title, target_node.title, title=str(relationship), category=relationship.category, explanation=relationship.explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25706\n"
     ]
    }
   ],
   "source": [
    "print(len(triplets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.3.2 Visualizing citation graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics related to cogvideo: ['Towards Accurate Image Coding: Improved Autoregressive Image Generation with Dynamic Vector Quantization']\n",
      "{'title': 'Title: make-a-video: text-to-video generation without text-video data,\\n Arxiv ID: 2209.14792', 'arxiv_id': '2209.14792'}\n"
     ]
    }
   ],
   "source": [
    "def find_connected_nodes(graph, node, relationship=None):\n",
    "    \"\"\"\n",
    "    Find nodes connected to the given node with an optional filter on the type of relationship.\n",
    "    \"\"\"\n",
    "    connected_nodes = []\n",
    "    for n, nbrs in graph.adj.items():\n",
    "        if n == node:\n",
    "            for nbr, eattr in nbrs.items():\n",
    "                if relationship is None or eattr['label'] == relationship:\n",
    "                    connected_nodes.append(nbr)\n",
    "    return connected_nodes\n",
    "\n",
    "# Function to search for a node by arxiv_id and return its details\n",
    "def find_nodes_by_arxiv_id(graph, arxiv_id):\n",
    "    for node, data in graph.nodes(data=True):\n",
    "        if data.get('arxiv_id') == arxiv_id:\n",
    "            return data  # or return data['paper_node'] to return the PaperNode object itself\n",
    "    return \"Paper not found in the graph.\"\n",
    "\n",
    "\n",
    "def find_shortest_path(graph, source, target):\n",
    "    \"\"\"\n",
    "    Find the shortest path between two nodes.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        path = nx.shortest_path(graph, source=source, target=target)\n",
    "        return path\n",
    "    except nx.NetworkXNoPath:\n",
    "        return None\n",
    "\n",
    "# Example Usage\n",
    "phenaki_related_topics = find_connected_nodes(G, 'cogview: mastering text-to-image generation via transformers')\n",
    "print(\"Topics related to cogvideo:\", phenaki_related_topics)\n",
    "\n",
    "# Example search\n",
    "search_result = find_nodes_by_arxiv_id(G, \"2209.14792\")\n",
    "print(search_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node: generating diverse high-fidelity images with vq-vae-2\n",
      "  Connected to: Late-Constraint Diffusion Guidance for Controllable Image Synthesis via Category: Methodological Basis,\n",
      "  Connected to: Attributable and Scalable Opinion Summarization via Category: Extension or Continuation,\n"
     ]
    }
   ],
   "source": [
    "def find_nodes_by_keyword(graph, keyword):\n",
    "    \"\"\"\n",
    "    Find nodes that contain the given keyword in their name and retrieve their connected nodes and relationships.\n",
    "    \"\"\"\n",
    "    keyword = keyword.lower()  # Convert keyword to lowercase for case-insensitive matching\n",
    "    matching_nodes = [node for node in graph.nodes if keyword in node.lower()]\n",
    "\n",
    "    related_nodes = {}\n",
    "    for node in matching_nodes:\n",
    "        connections = []\n",
    "        for neighbor, details in graph[node].items():\n",
    "            connections.append((neighbor, details['title'].split('\\n')[0]))\n",
    "        related_nodes[node] = connections\n",
    "\n",
    "    return related_nodes\n",
    "\n",
    "# Example Usage\n",
    "keyword = \"vq-vae\"\n",
    "phenaki_related = find_nodes_by_keyword(G, keyword)\n",
    "for node, connections in phenaki_related.items():\n",
    "    print(f\"Node: {node}\")\n",
    "    for conn in connections:\n",
    "        print(f\"  Connected to: {conn[0]} via {conn[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local cdn resources have problems on chrome/safari when used in jupyter-notebook. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"600px\"\n",
       "            src=\"nx.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f1ab42f3f70>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "\n",
    "# Assuming G is your original graph\n",
    "# Step 1: Create the subgraph for \"Node1\" and its neighbors\n",
    "subgraph = nx.ego_graph(G, 'make-a-video: text-to-video generation without text-video data', radius=3, center=True, undirected=False)\n",
    "# Nodes to be removed because they have a degree of 1 in the full graph\n",
    "nodes_to_remove = [node for node in subgraph if subgraph.degree(node) < 3]\n",
    "\n",
    "# Remove the nodes from the ego graph\n",
    "subgraph.remove_nodes_from(nodes_to_remove)\n",
    "\n",
    "nt = Network(notebook=True, font_color='#10000000')\n",
    "nt.from_nx(subgraph)\n",
    "nt.show(\"nx.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.4 Building Graph Query Engine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Basic Data Science Assistant**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.1 Download Wikipedia Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data science questions, I will use the source from wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U wikipedia-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Pre-compile the regular expression pattern for better performance\n",
    "BRACES_PATTERN = re.compile(r'\\{.*?\\}|\\}')\n",
    "\n",
    "def remove_braces_and_content(text):\n",
    "    \"\"\"Remove all occurrences of curly braces and their content from the given text\"\"\"\n",
    "    return BRACES_PATTERN.sub('', text)\n",
    "\n",
    "def clean_string(input_string):\n",
    "    \"\"\"Clean the input string.\"\"\"\n",
    "    \n",
    "    # Remove extra spaces by splitting the string by spaces and joining back together\n",
    "    cleaned_string = ' '.join(input_string.split())\n",
    "    \n",
    "    # Remove consecutive carriage return characters until there are no more consecutive occurrences\n",
    "    cleaned_string = re.sub(r'\\r+', '\\r', cleaned_string)\n",
    "    \n",
    "    # Remove all occurrences of curly braces and their content from the cleaned string\n",
    "    cleaned_string = remove_braces_and_content(cleaned_string)\n",
    "    \n",
    "    # Return the cleaned string\n",
    "    return cleaned_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_wikipedia_pages(wiki_wiki, category_name):\n",
    "    \"\"\"Extract all references from a category on Wikipedia\"\"\"\n",
    "    \n",
    "    # Get the Wikipedia page corresponding to the provided category name\n",
    "    category = wiki_wiki.page(\"Category:\" + category_name)\n",
    "    \n",
    "    # Initialize an empty list to store page titles\n",
    "    pages = []\n",
    "    \n",
    "    # Check if the category exists\n",
    "    if category.exists():\n",
    "        # Iterate through each article in the category and append its title to the list\n",
    "        for article in category.categorymembers.values():\n",
    "            pages.append(article.title)\n",
    "    \n",
    "    # Return the list of page titles\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_wikipedia_pages(categories):\n",
    "    \"\"\"Retrieve Wikipedia pages from a list of categories and extract their content\"\"\"\n",
    "    \n",
    "    # Create a Wikipedia object\n",
    "    wiki_wiki = wikipediaapi.Wikipedia('Kaggle Data Science Assistant with Gemma', 'en')\n",
    "    \n",
    "    # Initialize lists to store explored categories and Wikipedia pages\n",
    "    explored_categories = []\n",
    "    wikipedia_pages = []\n",
    "\n",
    "    # Iterate through each category\n",
    "    print(\"- Processing Wikipedia categories:\")\n",
    "    for category_name in categories:\n",
    "        print(f\"\\tExploring {category_name} on Wikipedia\")\n",
    "        \n",
    "        # Get the Wikipedia page corresponding to the category\n",
    "        category = wiki_wiki.page(\"Category:\" + category_name)\n",
    "        \n",
    "        # Extract Wikipedia pages from the category and extend the list\n",
    "        wikipedia_pages.extend(extract_wikipedia_pages(wiki_wiki, category_name))\n",
    "        \n",
    "        # Add the explored category to the list\n",
    "        explored_categories.append(category_name)\n",
    "\n",
    "    # Extract subcategories and remove duplicate categories\n",
    "    categories_to_explore = [item.replace(\"Category:\", \"\") for item in wikipedia_pages if \"Category:\" in item]\n",
    "    wikipedia_pages = list(set([item for item in wikipedia_pages if \"Category:\" not in item]))\n",
    "    \n",
    "    # Explore subcategories recursively\n",
    "    while categories_to_explore:\n",
    "        category_name = categories_to_explore.pop()\n",
    "        print(f\"\\tExploring {category_name} on Wikipedia\")\n",
    "        \n",
    "        # Extract more references from the subcategory\n",
    "        more_refs = extract_wikipedia_pages(wiki_wiki, category_name)\n",
    "\n",
    "        # Iterate through the references\n",
    "        for ref in more_refs:\n",
    "            # Check if the reference is a category\n",
    "            if \"Category:\" in ref:\n",
    "                new_category = ref.replace(\"Category:\", \"\")\n",
    "                # Add the new category to the explored categories list\n",
    "                if new_category not in explored_categories:\n",
    "                    explored_categories.append(new_category)\n",
    "            else:\n",
    "                # Add the reference to the Wikipedia pages list\n",
    "                if ref not in wikipedia_pages:\n",
    "                    wikipedia_pages.append(ref)\n",
    "\n",
    "    # Initialize a list to store extracted texts\n",
    "    extracted_texts = []\n",
    "    \n",
    "    # Iterate through each Wikipedia page\n",
    "    print(\"- Processing Wikipedia pages:\")\n",
    "    for page_title in tqdm(wikipedia_pages, total=len(wikipedia_pages)):\n",
    "        # Get the Wikipedia page\n",
    "        page = wiki_wiki.page(page_title)\n",
    "\n",
    "        # Append the page title and summary to the extracted texts list\n",
    "        if len(page.summary) > len(page.title):\n",
    "            extracted_texts.append(page.title + \" : \" + clean_string(page.summary))\n",
    "        \n",
    "        # Iterate through the sections in the page\n",
    "        for section in page.sections:\n",
    "            # Append the page title and section text to the extracted texts list\n",
    "            if len(section.text) > len(page.title):\n",
    "                extracted_texts.append(page.title + \" : \" + clean_string(section.text))\n",
    "                \n",
    "    # Return the extracted texts\n",
    "    return extracted_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Processing Wikipedia categories:\n",
      "\tExploring Machine_learning on Wikipedia\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tExploring Data_science on Wikipedia\n",
      "\tExploring Statistics on Wikipedia\n",
      "\tExploring Deep_learning on Wikipedia\n",
      "\tExploring Artificial_intelligence on Wikipedia\n",
      "\tExploring Artificial intelligence stubs on Wikipedia\n",
      "\tExploring Works created using artificial intelligence on Wikipedia\n",
      "\tExploring Virtual assistants on Wikipedia\n",
      "\tExploring Turing tests on Wikipedia\n",
      "\tExploring AI software on Wikipedia\n",
      "\tExploring Rule engines on Wikipedia\n",
      "\tExploring Artificial intelligence publications on Wikipedia\n",
      "\tExploring Philosophy of artificial intelligence on Wikipedia\n",
      "\tExploring Artificial intelligence people on Wikipedia\n",
      "\tExploring Open-source artificial intelligence on Wikipedia\n",
      "\tExploring Non-fiction books about Artificial intelligence on Wikipedia\n",
      "\tExploring Neural networks on Wikipedia\n",
      "\tExploring Multi-agent systems on Wikipedia\n",
      "\tExploring Mindâ€“body problem on Wikipedia\n",
      "\tExploring Machine learning on Wikipedia\n",
      "\tExploring Artificial intelligence laboratories on Wikipedia\n",
      "\tExploring Knowledge representation on Wikipedia\n",
      "\tExploring History of artificial intelligence on Wikipedia\n",
      "\tExploring Generative artificial intelligence on Wikipedia\n",
      "\tExploring Game artificial intelligence on Wikipedia\n",
      "\tExploring Fuzzy logic on Wikipedia\n",
      "\tExploring Fiction about artificial intelligence on Wikipedia\n",
      "\tExploring Existential risk from artificial general intelligence on Wikipedia\n",
      "\tExploring Evolutionary computation on Wikipedia\n",
      "\tExploring Artificial intelligence entertainment on Wikipedia\n",
      "\tExploring Distributed artificial intelligence on Wikipedia\n",
      "\tExploring Signal processing conferences on Wikipedia\n",
      "\tExploring Artificial intelligence conferences on Wikipedia\n",
      "\tExploring Computer vision on Wikipedia\n",
      "\tExploring Artificial intelligence competitions on Wikipedia\n",
      "\tExploring AI companies on Wikipedia\n",
      "\tExploring Cognitive architecture on Wikipedia\n",
      "\tExploring Cloud robotics on Wikipedia\n",
      "\tExploring Chatbots on Wikipedia\n",
      "\tExploring Automated reasoning on Wikipedia\n",
      "\tExploring Artificial intelligence associations on Wikipedia\n",
      "\tExploring Artificial intelligence templates on Wikipedia\n",
      "\tExploring Artificial immune systems on Wikipedia\n",
      "\tExploring Artificial intelligence art on Wikipedia\n",
      "\tExploring Argument technology on Wikipedia\n",
      "\tExploring Applications of artificial intelligence on Wikipedia\n",
      "\tExploring Ambient intelligence on Wikipedia\n",
      "\tExploring AI accelerators on Wikipedia\n",
      "\tExploring Affective computing on Wikipedia\n",
      "\tExploring Text-to-image generation on Wikipedia\n",
      "\tExploring Google DeepMind on Wikipedia\n",
      "\tExploring Deepfakes on Wikipedia\n",
      "\tExploring Deep learning software on Wikipedia\n",
      "\tExploring Statistics stubs on Wikipedia\n",
      "\tExploring Statistical concepts on Wikipedia\n",
      "\tExploring Statistical software on Wikipedia\n",
      "\tExploring Statistical methods on Wikipedia\n",
      "\tExploring Statistical data on Wikipedia\n",
      "\tExploring Subfields of statistics on Wikipedia\n",
      "\tExploring Statistics profession and organizations on Wikipedia\n",
      "\tExploring Statistics-related lists on Wikipedia\n",
      "\tExploring Statisticians on Wikipedia\n",
      "\tExploring Data scientists on Wikipedia\n",
      "\tExploring Unsupervised learning on Wikipedia\n",
      "\tExploring Support vector machines on Wikipedia\n",
      "\tExploring Supervised learning on Wikipedia\n",
      "\tExploring Structured prediction on Wikipedia\n",
      "\tExploring Statistical natural language processing on Wikipedia\n",
      "\tExploring Semisupervised learning on Wikipedia\n",
      "\tExploring Natural language processing researchers on Wikipedia\n",
      "\tExploring Machine learning researchers on Wikipedia\n",
      "\tExploring Reinforcement learning on Wikipedia\n",
      "\tExploring Ontology learning (computer science) on Wikipedia\n",
      "\tExploring Markov models on Wikipedia\n",
      "\tExploring Machine learning task on Wikipedia\n",
      "\tExploring Machine learning algorithms on Wikipedia\n",
      "\tExploring Loss functions on Wikipedia\n",
      "\tExploring Log-linear models on Wikipedia\n",
      "\tExploring Learning in computer vision on Wikipedia\n",
      "\tExploring Latent variable models on Wikipedia\n",
      "\tExploring Kernel methods for machine learning on Wikipedia\n",
      "\tExploring Inductive logic programming on Wikipedia\n",
      "\tExploring Genetic programming on Wikipedia\n",
      "\tExploring Evolutionary algorithms on Wikipedia\n",
      "\tExploring Ensemble learning on Wikipedia\n",
      "\tExploring Dimension reduction on Wikipedia\n",
      "\tExploring Datasets in machine learning on Wikipedia\n",
      "\tExploring Data mining and machine learning software on Wikipedia\n",
      "\tExploring Signal processing conferences on Wikipedia\n",
      "\tExploring Artificial intelligence conferences on Wikipedia\n",
      "\tExploring Computational learning theory on Wikipedia\n",
      "\tExploring Cluster analysis on Wikipedia\n",
      "\tExploring Classification algorithms on Wikipedia\n",
      "\tExploring Blockmodeling on Wikipedia\n",
      "\tExploring Bayesian networks on Wikipedia\n",
      "\tExploring Artificial neural networks on Wikipedia\n",
      "\tExploring Applied machine learning on Wikipedia\n",
      "- Processing Wikipedia pages:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3448/3448 [22:26<00:00,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16232 Wikipedia pages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "categories = [\"Machine_learning\", \"Data_science\", \"Statistics\", \"Deep_learning\", \"Artificial_intelligence\"]\n",
    "extracted_texts = get_wikipedia_pages(categories)\n",
    "print(\"Found\", len(extracted_texts), \"Wikipedia pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_documents = [Document(text=extracted_text, doc_id=str(i)) for i, extracted_text in enumerate(extracted_texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.PersistentClient(path=\"../DB/wiki\")\n",
    "chroma_collection = chroma_client.get_or_create_collection(\"gemma_assistant_wiki\")\n",
    "\n",
    "\n",
    "# Create vector store\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16232/16232 [00:08<00:00, 1830.75it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:11<00:00, 175.13it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:10<00:00, 188.60it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:11<00:00, 175.40it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:11<00:00, 177.25it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:10<00:00, 202.03it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:09<00:00, 209.87it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:11<00:00, 185.12it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:11<00:00, 182.42it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 73/73 [00:00<00:00, 197.81it/s]\n"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "    wiki_documents, storage_context=storage_context, embed_model=embed_model, show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.2 Loading from vector store**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "import torch\n",
    "\n",
    "\n",
    "Settings.llm = None # Set this to none to make the index only do retrieval\n",
    "device_type = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\", cache_folder=\"../models\", device=device_type) # must be the same as the previous stage\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=\"../DB/wiki\")\n",
    "chroma_collection = chroma_client.get_or_create_collection(\"gemma_assistant_wiki\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "# load the vectorstore\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_vector_store(vector_store, storage_context=storage_context, embed_model=embed_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_science_query_engine = index.as_query_engine(\n",
    "    similarity_top_k=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "Outline of regression analysis : Regression analysis Linear regression\n",
      "\n",
      "Regression diagnostic : Regression diagnostics have often been developed or were initially proposed in the context of linear regression or, more particularly, ordinary least squares. This means that many formally defined diagnostics are only available for these contexts.\n",
      "\n",
      "Linear predictor function : In statistics and in machine learning, a linear predictor function is a linear function (linear combination) of a set of coefficients and explanatory variables (independent variables), whose value is used to predict the outcome of a dependent variable. This sort of function usually comes in linear regression, where the coefficients are called regression coefficients. However, they also occur in various types of linear classifiers (e.g. logistic regression, perceptrons, support vector machines, and linear discriminant analysis), as well as in various other models, such as principal component analysis and factor analysis. In many of these models, the coefficients are referred to as \"weights\".\n",
      "\n",
      "Outline of regression analysis : General linear model Ordinary least squares Generalized least squares Simple linear regression Trend estimation Ridge regression Polynomial regression Segmented regression Nonlinear regression\n",
      "\n",
      "Outline of regression analysis : Least squares Linear least squares (mathematics) Non-linear least squares Least absolute deviations Curve fitting Smoothing Cross-sectional study\n",
      "\n",
      "Partial least squares regression : Partial least squares regression (PLS regression) is a statistical method that bears some relation to principal components regression; instead of finding hyperplanes of maximum variance between the response and independent variables, it finds a linear regression model by projecting the predicted variables and the observable variables to a new space. Because both the X and Y data are projected to new spaces, the PLS family of methods are known as bilinear factor models. Partial least squares discriminant analysis (PLS-DA) is a variant used when the Y is categorical. PLS is used to find the fundamental relations between 2 matrices (X and Y), i.e. a latent variable approach to modeling the covariance structures in these two spaces. A PLS model will try to find the multidimensional direction in the X space that explains the maximum multidimensional variance direction in the Y space. PLS regression is particularly suited when the matrix of predictors has more variables than observations, and when there is multicollinearity among X values. By contrast, standard regression will fail in these cases (unless it is regularized). Partial least squares was introduced by the Swedish statistician Herman O. A. Wold, who then developed it with his son, Svante Wold. An alternative term for PLS is projection to latent structures, but the term partial least squares is still dominant in many areas. Although the original applications were in the social sciences, PLS regression is today most widely used in chemometrics and related areas. It is also used in bioinformatics, sensometrics, neuroscience, and anthropology.\n",
      "\n",
      "Linear predictor function : Linear model Linear regression == References ==\n",
      "\n",
      "Outline of regression analysis : The following outline is provided as an overview of and topical guide to regression analysis: Regression analysis â€“ use of statistical techniques for learning about the relationship between one or more dependent variables (Y) and one or more independent variables (X).\n",
      "\n",
      "Ecological regression : Ecological regression is a statistical technique which runs regression on aggregates, often used in political science and history to estimate group voting behavior from aggregate data.For example, if counties have a known Democratic vote (in percentage) D, and a known percentage of Catholics, C, then running a linear regression of dependent variable D against independent variable C will give D = a + bC. If the regression gives D = .22 + .45C for example, then the estimated Catholic vote (C = 1) is 67% Democratic and the non-Catholic vote (C = 0) is 22% Democratic. The technique has been often used in litigation brought under the Voting Rights Act of 1965 to see how blacks and whites voted.\n",
      "\n",
      "Proper linear model : In statistics, a proper linear model is a linear regression model in which the weights given to the predictor variables are chosen in such a way as to optimize the relationship between the prediction and the criterion. Simple regression analysis is the most common example of a proper linear model. Unit-weighted regression is the most common example of an improper linear model.\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: What is linear regression\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "print(data_science_query_engine.query(\"What is linear regression\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Python Code Assistant**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.2 Define a code intepreter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import regex\n",
    "import pickle\n",
    "import traceback\n",
    "import copy\n",
    "import datetime\n",
    "import dateutil.relativedelta\n",
    "import multiprocess\n",
    "from multiprocess import Pool\n",
    "from typing import Any, Dict, Optional\n",
    "from pebble import ProcessPool\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import TimeoutError\n",
    "from functools import partial\n",
    "from timeout_decorator import timeout\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "\n",
    "class GenericRuntime:\n",
    "    GLOBAL_DICT = {}\n",
    "    LOCAL_DICT = None\n",
    "    HEADERS = []\n",
    "    def __init__(self):\n",
    "        self._global_vars = copy.copy(self.GLOBAL_DICT)\n",
    "        self._local_vars = copy.copy(self.LOCAL_DICT) if self.LOCAL_DICT else None\n",
    "\n",
    "        for c in self.HEADERS:\n",
    "            self.exec_code(c)\n",
    "\n",
    "    def exec_code(self, code_piece: str) -> None:\n",
    "        if regex.search(r'(\\s|^)?input\\(', code_piece) or regex.search(r'(\\s|^)?os.system\\(', code_piece):\n",
    "            raise RuntimeError()\n",
    "        exec(code_piece, self._global_vars)\n",
    "        \n",
    "    def eval_code(self, expr: str) -> Any:\n",
    "        return eval(expr, self._global_vars)\n",
    "    \n",
    "    def inject(self, var_dict: Dict[str, Any]) -> None:\n",
    "        for k, v in var_dict.items():\n",
    "            self._global_vars[k] = v\n",
    "    \n",
    "    @property\n",
    "    def answer(self):\n",
    "        return self._global_vars['answer']\n",
    "\n",
    "class DateRuntime(GenericRuntime):\n",
    "    GLOBAL_DICT = {\n",
    "        'datetime': datetime.datetime, \n",
    "        'timedelta': dateutil.relativedelta.relativedelta,\n",
    "        'relativedelta': dateutil.relativedelta.relativedelta\n",
    "    }\n",
    "\n",
    "\n",
    "class CustomDict(dict):\n",
    "    def __iter__(self):\n",
    "        return list(super().__iter__()).__iter__()\n",
    "\n",
    "class ColorObjectRuntime(GenericRuntime):\n",
    "    GLOBAL_DICT = {'dict': CustomDict}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Hello world!', 'Done')\n"
     ]
    }
   ],
   "source": [
    "class PythonExecutor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        runtime: Optional[Any] = None,\n",
    "        get_answer_symbol: Optional[str] = None,\n",
    "        get_answer_expr: Optional[str] = None,\n",
    "        get_answer_from_stdout: bool = False,\n",
    "        timeout_length: int = 5,\n",
    "    ) -> None:\n",
    "        self.runtime = runtime if runtime else GenericRuntime()\n",
    "        self.answer_symbol = get_answer_symbol\n",
    "        self.answer_expr = get_answer_expr\n",
    "        self.get_answer_from_stdout = get_answer_from_stdout\n",
    "        self.pool = Pool(multiprocess.cpu_count())\n",
    "        self.timeout_length = timeout_length\n",
    "\n",
    "    def process_generation_to_code(self, gens: str):\n",
    "        return [g.split('\\n') for g in gens]\n",
    "\n",
    "    @staticmethod\n",
    "    def execute(\n",
    "        code,\n",
    "        get_answer_from_stdout = None,\n",
    "        runtime = None,\n",
    "        answer_symbol = None,\n",
    "        answer_expr = None,\n",
    "        timeout_length = 10,\n",
    "    ):\n",
    "        try:\n",
    "            if get_answer_from_stdout:\n",
    "                program_io = io.StringIO()\n",
    "                with redirect_stdout(program_io):\n",
    "                    timeout(timeout_length)(runtime.exec_code)('\\n'.join(code))\n",
    "                program_io.seek(0)\n",
    "                result = program_io.read()\n",
    "            elif answer_symbol:\n",
    "                timeout(timeout_length)(runtime.exec_code)('\\n'.join(code))\n",
    "                result = runtime._global_vars[answer_symbol]\n",
    "            elif answer_expr:\n",
    "                timeout(timeout_length)(runtime.exec_code)('\\n'.join(code))\n",
    "                result = timeout(timeout_length)(runtime.eval_code)(answer_expr)\n",
    "            else:\n",
    "                timeout(timeout_length)(runtime.exec_code)('\\n'.join(code[:-1]))\n",
    "                result = timeout(timeout_length)(runtime.eval_code)(code[-1])\n",
    "            report = \"Done\"\n",
    "            str(result)\n",
    "            pickle.dumps(result) # serialization check\n",
    "        except:\n",
    "            result = ''\n",
    "            report = traceback.format_exc().split('\\n')[-2]\n",
    "        return result, report\n",
    "\n",
    "    def apply(self, code):\n",
    "        return self.batch_apply([code])[0]\n",
    "\n",
    "    @staticmethod\n",
    "    def truncate(s, max_length=400):\n",
    "        half = max_length // 2\n",
    "        if len(s) > max_length:\n",
    "            s = s[:half] + \"...\" + s[-half:]\n",
    "        return s\n",
    "\n",
    "    def batch_apply(self, batch_code):\n",
    "        all_code_snippets = self.process_generation_to_code(batch_code)\n",
    "\n",
    "        timeout_cnt = 0\n",
    "        all_exec_results = []\n",
    "        with ProcessPool(max_workers=min(len(all_code_snippets), os.cpu_count())) as pool:\n",
    "            executor = partial(\n",
    "                self.execute,\n",
    "                get_answer_from_stdout=self.get_answer_from_stdout,\n",
    "                runtime=self.runtime,\n",
    "                answer_symbol=self.answer_symbol,\n",
    "                answer_expr=self.answer_expr,\n",
    "                timeout_length=self.timeout_length, # this timeout not work\n",
    "            )\n",
    "            future = pool.map(executor, all_code_snippets, timeout=self.timeout_length)\n",
    "            iterator = future.result()\n",
    "\n",
    "            if len(all_code_snippets) > 100:  \n",
    "                progress_bar = tqdm(total=len(all_code_snippets), desc=\"Execute\")  \n",
    "            else:  \n",
    "                progress_bar = None \n",
    "\n",
    "            while True:\n",
    "                try:\n",
    "                    result = next(iterator)\n",
    "                    all_exec_results.append(result)\n",
    "                except StopIteration:\n",
    "                    break\n",
    "                except TimeoutError as error:\n",
    "                    print(error)\n",
    "                    all_exec_results.append((\"\", \"Timeout Error\"))\n",
    "                    timeout_cnt += 1\n",
    "                except Exception as error:\n",
    "                    print(error)\n",
    "                    exit()\n",
    "                if progress_bar is not None:\n",
    "                    progress_bar.update(1) \n",
    "            \n",
    "            if progress_bar is not None:\n",
    "                progress_bar.close() \n",
    "\n",
    "        batch_results = []\n",
    "        for code, (res, report) in zip(all_code_snippets, all_exec_results):\n",
    "            # post processing\n",
    "            res, report = str(res).strip(), str(report).strip()\n",
    "            res, report = self.truncate(res), self.truncate(report)\n",
    "            batch_results.append((res, report))\n",
    "        return batch_results\n",
    "\n",
    "\n",
    "def test():\n",
    "    batch_code = [\n",
    "\"\"\"\n",
    "print(\"Hello world!\")\n",
    "\"\"\"\n",
    "    ]\n",
    "\n",
    "    executor = PythonExecutor(get_answer_from_stdout=True)\n",
    "    predictions = executor.apply(batch_code[0])\n",
    "    print(predictions)\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5. Combine all of them together**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.1 Define Router Engine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from unsloth import FastLanguageModel\n",
    "import os\n",
    "\n",
    "max_seq_length = 8192 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "\n",
    "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#     model_name = \"unsloth/gemma-7b-bnb-4bit\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "#     max_seq_length = max_seq_length,\n",
    "#     dtype = dtype,\n",
    "#     load_in_4bit = load_in_4bit,\n",
    "#     token = \"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\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    "#     cache_dir = \"../models\",\n",
    "# )\n",
    "# FastLanguageModel.for_inference(model) \n",
    "\n",
    "# llm = HuggingFaceLLM(model=model, tokenizer=tokenizer)\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo\", api_key=\"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\")\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert output to JSON: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 18\u001b[0m\n\u001b[1;32m      5\u001b[0m ds_tool \u001b[38;5;241m=\u001b[39m QueryEngineTool\u001b[38;5;241m.\u001b[39mfrom_defaults(\n\u001b[1;32m      6\u001b[0m     query_engine\u001b[38;5;241m=\u001b[39mdata_science_query_engine,\n\u001b[1;32m      7\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUseful for answering data science concepts\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m query_engine \u001b[38;5;241m=\u001b[39m RouterQueryEngine(\n\u001b[1;32m     11\u001b[0m     selector\u001b[38;5;241m=\u001b[39mLLMSingleSelector\u001b[38;5;241m.\u001b[39mfrom_defaults(),\n\u001b[1;32m     12\u001b[0m     query_engine_tools\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     17\u001b[0m )\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mquery_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is linear regression?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:211\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/llama_index/core/base/base_query_engine.py:53\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     52\u001b[0m         str_or_query_bundle \u001b[38;5;241m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[0;32m---> 53\u001b[0m     query_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstr_or_query_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m dispatch_event(QueryEndEvent())\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m query_result\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/llama_index/core/query_engine/router_query_engine.py:169\u001b[0m, in \u001b[0;36mRouterQueryEngine._query\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, query_bundle: QueryBundle) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RESPONSE_TYPE:\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    167\u001b[0m         CBEventType\u001b[38;5;241m.\u001b[39mQUERY, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query_bundle\u001b[38;5;241m.\u001b[39mquery_str}\n\u001b[1;32m    168\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m query_event:\n\u001b[0;32m--> 169\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_metadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result\u001b[38;5;241m.\u001b[39minds) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    172\u001b[0m             responses \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/llama_index/core/base/base_selector.py:87\u001b[0m, in \u001b[0;36mBaseSelector.select\u001b[0;34m(self, choices, query)\u001b[0m\n\u001b[1;32m     85\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [_wrap_choice(choice) \u001b[38;5;28;01mfor\u001b[39;00m choice \u001b[38;5;129;01min\u001b[39;00m choices]\n\u001b[1;32m     86\u001b[0m query_bundle \u001b[38;5;241m=\u001b[39m _wrap_query(query)\n\u001b[0;32m---> 87\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_select\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchoices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/llama_index/core/selectors/llm_selectors.py:118\u001b[0m, in \u001b[0;36mLLMSingleSelector._select\u001b[0;34m(self, choices, query)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# parse output\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prompt\u001b[38;5;241m.\u001b[39moutput_parser \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m parse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _structured_output_to_selector_result(parse)\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/llama_index/core/output_parsers/selection.py:97\u001b[0m, in \u001b[0;36mSelectionOutputParser.parse\u001b[0;34m(self, output)\u001b[0m\n\u001b[1;32m     94\u001b[0m     json_obj \u001b[38;5;241m=\u001b[39m [json_obj]\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m json_obj:\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to convert output to JSON: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     99\u001b[0m json_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_output(json_obj)\n\u001b[1;32m    100\u001b[0m answers \u001b[38;5;241m=\u001b[39m [Answer\u001b[38;5;241m.\u001b[39mfrom_dict(json_dict) \u001b[38;5;28;01mfor\u001b[39;00m json_dict \u001b[38;5;129;01min\u001b[39;00m json_output]\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert output to JSON: ''"
     ]
    }
   ],
   "source": [
    "paper_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=paper_query_engine,\n",
    "    description=\"Useful for search for papers\",\n",
    ")\n",
    "ds_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=data_science_query_engine,\n",
    "    description=\"Useful for answering data science concepts\",\n",
    ")\n",
    "\n",
    "query_engine = RouterQueryEngine(\n",
    "    selector=LLMSingleSelector.from_defaults(),\n",
    "    query_engine_tools=[\n",
    "        paper_tool,\n",
    "        ds_tool,\n",
    "    ],\n",
    "    verbose=True\n",
    ")\n",
    "print(query_engine.query(\"What is linear regression?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
